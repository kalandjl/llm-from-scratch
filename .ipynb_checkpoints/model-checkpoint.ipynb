{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Each character has weights of a 32 long vector, defined by n_embed (embedding dimension)\n",
    "n_embd = 32\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "        \"\"\"\n",
    "        Calculates the gradients for gamma, beta, and the input x.\n",
    "        \"\"\"\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=32, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=16, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/548472516.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/548472516.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/548472516.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44517/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(129)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "# ---\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Clip gradients before optimizer\n",
    "    clip_gradients(model, max_norm=1.0)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x100)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        top = 20\n",
    "        if loss_initial < 3:\n",
    "            top = 3\n",
    "        ax.set_ylim(top=top) # cut off loses higher than 3\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 500\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
