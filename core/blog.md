# Day 1: Initialization, Embedding Matrices, Forward Pass

## Goal: Get output predictions from input char

# Day 2: Manual Backpropagation

## Goal: Calculate and update model weights through stochaic gradient descent

# Day 3: Transformer Block

## Build self attention block, wrap in transformer head

# Day 4: Assembling the Model

## Combine all the pieces into a working, trainable model

# Day 5: Building the interactive UI
    - Can adjust model temperature, input words and get output prediction
    - Attention heatmap

# Day 6: Modern Transformer Architecture:
    -   Rotary Positional Embeddings (RoPE)

