{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9be7617-81fe-4ddf-88e6-581ec50c4ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Turn off warnings\n",
    "np.seterr(all='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa26fce2-3790-4734-9e14-f41f9c49d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f9f693-67e4-44eb-b5d8-d5f781c7d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99eb36c-2032-4cdd-a7a6-442f21e1958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42f7020-2825-47a0-8b55-3dde8b508af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ecc58b-dc30-4888-aa87-8c0328835b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d36831e-2a7c-45b6-981f-c5cff26150fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5ce8b0-e3f6-4ce7-934b-aafc047d81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f91548-4df1-41cc-84ea-9bcdc91f67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d44f32-6646-4b01-8b25-19deb75d61c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix\n",
      "position_matrix\n",
      "transform.0.W1\n",
      "transform.0.W2\n",
      "transform.0.layer_norm1.gamma\n",
      "transform.0.layer_norm1.beta\n",
      "transform.0.layer_norm2.gamma\n",
      "transform.0.layer_norm2.beta\n",
      "transform.0.multi_head_attention_block.W_output\n",
      "transform.0.multi_head_attention_block.heads.0.key\n",
      "transform.0.multi_head_attention_block.heads.0.query\n",
      "transform.0.multi_head_attention_block.heads.0.value\n",
      "transform.0.multi_head_attention_block.heads.1.key\n",
      "transform.0.multi_head_attention_block.heads.1.query\n",
      "transform.0.multi_head_attention_block.heads.1.value\n",
      "transform.0.multi_head_attention_block.heads.2.key\n",
      "transform.0.multi_head_attention_block.heads.2.query\n",
      "transform.0.multi_head_attention_block.heads.2.value\n",
      "transform.0.multi_head_attention_block.heads.3.key\n",
      "transform.0.multi_head_attention_block.heads.3.query\n",
      "transform.0.multi_head_attention_block.heads.3.value\n",
      "transform.0.multi_head_attention_block.heads.4.key\n",
      "transform.0.multi_head_attention_block.heads.4.query\n",
      "transform.0.multi_head_attention_block.heads.4.value\n",
      "transform.0.multi_head_attention_block.heads.5.key\n",
      "transform.0.multi_head_attention_block.heads.5.query\n",
      "transform.0.multi_head_attention_block.heads.5.value\n",
      "transform.0.multi_head_attention_block.heads.6.key\n",
      "transform.0.multi_head_attention_block.heads.6.query\n",
      "transform.0.multi_head_attention_block.heads.6.value\n",
      "transform.0.multi_head_attention_block.heads.7.key\n",
      "transform.0.multi_head_attention_block.heads.7.query\n",
      "transform.0.multi_head_attention_block.heads.7.value\n",
      "transform.1.W1\n",
      "transform.1.W2\n",
      "transform.1.layer_norm1.gamma\n",
      "transform.1.layer_norm1.beta\n",
      "transform.1.layer_norm2.gamma\n",
      "transform.1.layer_norm2.beta\n",
      "transform.1.multi_head_attention_block.W_output\n",
      "transform.1.multi_head_attention_block.heads.0.key\n",
      "transform.1.multi_head_attention_block.heads.0.query\n",
      "transform.1.multi_head_attention_block.heads.0.value\n",
      "transform.1.multi_head_attention_block.heads.1.key\n",
      "transform.1.multi_head_attention_block.heads.1.query\n",
      "transform.1.multi_head_attention_block.heads.1.value\n",
      "transform.1.multi_head_attention_block.heads.2.key\n",
      "transform.1.multi_head_attention_block.heads.2.query\n",
      "transform.1.multi_head_attention_block.heads.2.value\n",
      "transform.1.multi_head_attention_block.heads.3.key\n",
      "transform.1.multi_head_attention_block.heads.3.query\n",
      "transform.1.multi_head_attention_block.heads.3.value\n",
      "transform.1.multi_head_attention_block.heads.4.key\n",
      "transform.1.multi_head_attention_block.heads.4.query\n",
      "transform.1.multi_head_attention_block.heads.4.value\n",
      "transform.1.multi_head_attention_block.heads.5.key\n",
      "transform.1.multi_head_attention_block.heads.5.query\n",
      "transform.1.multi_head_attention_block.heads.5.value\n",
      "transform.1.multi_head_attention_block.heads.6.key\n",
      "transform.1.multi_head_attention_block.heads.6.query\n",
      "transform.1.multi_head_attention_block.heads.6.value\n",
      "transform.1.multi_head_attention_block.heads.7.key\n",
      "transform.1.multi_head_attention_block.heads.7.query\n",
      "transform.1.multi_head_attention_block.heads.7.value\n",
      "transform.2.W1\n",
      "transform.2.W2\n",
      "transform.2.layer_norm1.gamma\n",
      "transform.2.layer_norm1.beta\n",
      "transform.2.layer_norm2.gamma\n",
      "transform.2.layer_norm2.beta\n",
      "transform.2.multi_head_attention_block.W_output\n",
      "transform.2.multi_head_attention_block.heads.0.key\n",
      "transform.2.multi_head_attention_block.heads.0.query\n",
      "transform.2.multi_head_attention_block.heads.0.value\n",
      "transform.2.multi_head_attention_block.heads.1.key\n",
      "transform.2.multi_head_attention_block.heads.1.query\n",
      "transform.2.multi_head_attention_block.heads.1.value\n",
      "transform.2.multi_head_attention_block.heads.2.key\n",
      "transform.2.multi_head_attention_block.heads.2.query\n",
      "transform.2.multi_head_attention_block.heads.2.value\n",
      "transform.2.multi_head_attention_block.heads.3.key\n",
      "transform.2.multi_head_attention_block.heads.3.query\n",
      "transform.2.multi_head_attention_block.heads.3.value\n",
      "transform.2.multi_head_attention_block.heads.4.key\n",
      "transform.2.multi_head_attention_block.heads.4.query\n",
      "transform.2.multi_head_attention_block.heads.4.value\n",
      "transform.2.multi_head_attention_block.heads.5.key\n",
      "transform.2.multi_head_attention_block.heads.5.query\n",
      "transform.2.multi_head_attention_block.heads.5.value\n",
      "transform.2.multi_head_attention_block.heads.6.key\n",
      "transform.2.multi_head_attention_block.heads.6.query\n",
      "transform.2.multi_head_attention_block.heads.6.value\n",
      "transform.2.multi_head_attention_block.heads.7.key\n",
      "transform.2.multi_head_attention_block.heads.7.query\n",
      "transform.2.multi_head_attention_block.heads.7.value\n",
      "transform.3.W1\n",
      "transform.3.W2\n",
      "transform.3.layer_norm1.gamma\n",
      "transform.3.layer_norm1.beta\n",
      "transform.3.layer_norm2.gamma\n",
      "transform.3.layer_norm2.beta\n",
      "transform.3.multi_head_attention_block.W_output\n",
      "transform.3.multi_head_attention_block.heads.0.key\n",
      "transform.3.multi_head_attention_block.heads.0.query\n",
      "transform.3.multi_head_attention_block.heads.0.value\n",
      "transform.3.multi_head_attention_block.heads.1.key\n",
      "transform.3.multi_head_attention_block.heads.1.query\n",
      "transform.3.multi_head_attention_block.heads.1.value\n",
      "transform.3.multi_head_attention_block.heads.2.key\n",
      "transform.3.multi_head_attention_block.heads.2.query\n",
      "transform.3.multi_head_attention_block.heads.2.value\n",
      "transform.3.multi_head_attention_block.heads.3.key\n",
      "transform.3.multi_head_attention_block.heads.3.query\n",
      "transform.3.multi_head_attention_block.heads.3.value\n",
      "transform.3.multi_head_attention_block.heads.4.key\n",
      "transform.3.multi_head_attention_block.heads.4.query\n",
      "transform.3.multi_head_attention_block.heads.4.value\n",
      "transform.3.multi_head_attention_block.heads.5.key\n",
      "transform.3.multi_head_attention_block.heads.5.query\n",
      "transform.3.multi_head_attention_block.heads.5.value\n",
      "transform.3.multi_head_attention_block.heads.6.key\n",
      "transform.3.multi_head_attention_block.heads.6.query\n",
      "transform.3.multi_head_attention_block.heads.6.value\n",
      "transform.3.multi_head_attention_block.heads.7.key\n",
      "transform.3.multi_head_attention_block.heads.7.query\n",
      "transform.3.multi_head_attention_block.heads.7.value\n",
      "transform.4.W1\n",
      "transform.4.W2\n",
      "transform.4.layer_norm1.gamma\n",
      "transform.4.layer_norm1.beta\n",
      "transform.4.layer_norm2.gamma\n",
      "transform.4.layer_norm2.beta\n",
      "transform.4.multi_head_attention_block.W_output\n",
      "transform.4.multi_head_attention_block.heads.0.key\n",
      "transform.4.multi_head_attention_block.heads.0.query\n",
      "transform.4.multi_head_attention_block.heads.0.value\n",
      "transform.4.multi_head_attention_block.heads.1.key\n",
      "transform.4.multi_head_attention_block.heads.1.query\n",
      "transform.4.multi_head_attention_block.heads.1.value\n",
      "transform.4.multi_head_attention_block.heads.2.key\n",
      "transform.4.multi_head_attention_block.heads.2.query\n",
      "transform.4.multi_head_attention_block.heads.2.value\n",
      "transform.4.multi_head_attention_block.heads.3.key\n",
      "transform.4.multi_head_attention_block.heads.3.query\n",
      "transform.4.multi_head_attention_block.heads.3.value\n",
      "transform.4.multi_head_attention_block.heads.4.key\n",
      "transform.4.multi_head_attention_block.heads.4.query\n",
      "transform.4.multi_head_attention_block.heads.4.value\n",
      "transform.4.multi_head_attention_block.heads.5.key\n",
      "transform.4.multi_head_attention_block.heads.5.query\n",
      "transform.4.multi_head_attention_block.heads.5.value\n",
      "transform.4.multi_head_attention_block.heads.6.key\n",
      "transform.4.multi_head_attention_block.heads.6.query\n",
      "transform.4.multi_head_attention_block.heads.6.value\n",
      "transform.4.multi_head_attention_block.heads.7.key\n",
      "transform.4.multi_head_attention_block.heads.7.query\n",
      "transform.4.multi_head_attention_block.heads.7.value\n",
      "transform.5.W1\n",
      "transform.5.W2\n",
      "transform.5.layer_norm1.gamma\n",
      "transform.5.layer_norm1.beta\n",
      "transform.5.layer_norm2.gamma\n",
      "transform.5.layer_norm2.beta\n",
      "transform.5.multi_head_attention_block.W_output\n",
      "transform.5.multi_head_attention_block.heads.0.key\n",
      "transform.5.multi_head_attention_block.heads.0.query\n",
      "transform.5.multi_head_attention_block.heads.0.value\n",
      "transform.5.multi_head_attention_block.heads.1.key\n",
      "transform.5.multi_head_attention_block.heads.1.query\n",
      "transform.5.multi_head_attention_block.heads.1.value\n",
      "transform.5.multi_head_attention_block.heads.2.key\n",
      "transform.5.multi_head_attention_block.heads.2.query\n",
      "transform.5.multi_head_attention_block.heads.2.value\n",
      "transform.5.multi_head_attention_block.heads.3.key\n",
      "transform.5.multi_head_attention_block.heads.3.query\n",
      "transform.5.multi_head_attention_block.heads.3.value\n",
      "transform.5.multi_head_attention_block.heads.4.key\n",
      "transform.5.multi_head_attention_block.heads.4.query\n",
      "transform.5.multi_head_attention_block.heads.4.value\n",
      "transform.5.multi_head_attention_block.heads.5.key\n",
      "transform.5.multi_head_attention_block.heads.5.query\n",
      "transform.5.multi_head_attention_block.heads.5.value\n",
      "transform.5.multi_head_attention_block.heads.6.key\n",
      "transform.5.multi_head_attention_block.heads.6.query\n",
      "transform.5.multi_head_attention_block.heads.6.value\n",
      "transform.5.multi_head_attention_block.heads.7.key\n",
      "transform.5.multi_head_attention_block.heads.7.query\n",
      "transform.5.multi_head_attention_block.heads.7.value\n",
      "Loading saved model weights...\n",
      "✓ Loaded main matrices\n",
      "Loading transformer 0...\n",
      "✓ Loaded transformer 0\n",
      "Loading transformer 1...\n",
      "✓ Loaded transformer 1\n",
      "Loading transformer 2...\n",
      "✓ Loaded transformer 2\n",
      "Loading transformer 3...\n",
      "✓ Loaded transformer 3\n",
      "Loading transformer 4...\n",
      "✓ Loaded transformer 4\n",
      "Loading transformer 5...\n",
      "✓ Loaded transformer 5\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])\n",
    "\n",
    "# Load the saved weights\n",
    "loaded_weights = np.load('my_model.npz')\n",
    "\n",
    "for key in loaded_weights.keys():\n",
    "    print(key)\n",
    "print(\"Loading saved model weights...\")\n",
    "\n",
    "# Set the main model weights\n",
    "model.embedding_matrix = loaded_weights[\"embedding_matrix\"]\n",
    "model.position_matrix = loaded_weights[\"position_matrix\"]\n",
    "print(\"✓ Loaded main matrices\")\n",
    "\n",
    "# Load transformer weights\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    print(f\"Loading transformer {idx}...\")\n",
    "    \n",
    "    # Load basic weights\n",
    "    transformer.W1 = loaded_weights[f\"transform.{idx}.W1\"]\n",
    "    transformer.W2 = loaded_weights[f\"transform.{idx}.W2\"]\n",
    "    transformer.layer_norm1.gamma = loaded_weights[f\"transform.{idx}.layer_norm1.gamma\"]\n",
    "    transformer.layer_norm1.beta = loaded_weights[f\"transform.{idx}.layer_norm1.beta\"]\n",
    "    transformer.layer_norm2.gamma = loaded_weights[f\"transform.{idx}.layer_norm2.gamma\"]\n",
    "    transformer.layer_norm2.beta = loaded_weights[f\"transform.{idx}.layer_norm2.beta\"]\n",
    "    transformer.multi_head_attention_block.W_output = loaded_weights[f\"transform.{idx}.multi_head_attention_block.W_output\"]\n",
    "    \n",
    "    # Load attention head weights (16 heads each)\n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        attention_head.W_key = loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"]\n",
    "        attention_head.W_query = loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"]\n",
    "        attention_head.W_value = loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"]\n",
    "    \n",
    "    print(f\"✓ Loaded transformer {idx}\")\n",
    "\n",
    "loaded_weights.close()\n",
    "print(\"Model weights loaded successfully!\")\n",
    "\n",
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_batch(data, 128, 34)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197b77b8-ec45-4f5e-a55e-bef5f6b9c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938662813685926\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss and probabilites\n",
    "logits = model.forward(x_batch)\n",
    "loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "print(loss_initial)\n",
    "\n",
    "def generate_text(prompt, temperature, heatMap, max_length):\n",
    "    print(f\"Generating with prompt: '{prompt}', temp: {temperature}, length: {max_length}\")\n",
    "    \n",
    "    if not prompt:\n",
    "        prompt = \"\\n\" # Default prompt\n",
    "\n",
    "    # Set the model's temperature for this specific generation\n",
    "    model.temperature = temperature\n",
    "    \n",
    "    # Encode the prompt and generate\n",
    "    char_indices = encode(prompt)\n",
    "    for _ in range(int(max_length)):\n",
    "        # Important: only feed the last block_size tokens as context\n",
    "        context = char_indices[-1000:]\n",
    "        next_char_index = model.pred(context)\n",
    "        char_indices.append(next_char_index)\n",
    "        \n",
    "    # Decode the final list of indices into a string\n",
    "    generated_text = decode(char_indices)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b4a92da-acb6-4c61-8944-23e81ab5ab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with prompt: 'def', temp: 0.1, length: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def test_request_header_values(self, httpbin):\\n             r = requests.get(httpbin(\"stream/4\"), allow'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"def\", 0.1, True, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
