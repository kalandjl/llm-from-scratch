{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccc1b09-1546-415d-ab68-ceffcf08feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset: 'python-algorithms' git repo\n",
    "base_dataset = open(\"../data/requests.txt\").read()\n",
    "\n",
    "# Get original character set and create encode/decode functions\n",
    "chars = sorted(list(set(base_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac1175c-36cf-4ec6-a2b9-dfaab6c95f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string-to-integer mapping\n",
    "stoi = {char: i for i, char in enumerate(chars)}\n",
    "# integer-to-string mapping  \n",
    "itos = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Filter new text to only include original characters\n",
    "def filter_text(text, allowed_chars):\n",
    "    return ''.join(char for char in text if char in allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3583e428-f657-43c5-a28c-2f90566061f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter new dataset\n",
    "text = open(\"../data/python_fundamentals.txt\").read()\n",
    "filtered_text = filter_text(text, set(chars))  # Convert to set for faster lookup\n",
    "\n",
    "# Use filtered text for training\n",
    "data = np.array(encode(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e35ffc0-aa10-4aad-9230-aa573b6b2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742d9899-4c28-4870-8db2-59dfccf20402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2167158-a556-41c1-aace-0a1c1f27cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class allows for the requires grad variable \n",
    "class Parameter:\n",
    "    def __init__(self, weights, requires_grad=True):\n",
    "        \n",
    "        self.data = weights\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = np.zeros_like(weights)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Parameter with data shape {self.data.shape}\"\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        self.data -= other \n",
    "\n",
    "    def zerograds(self):\n",
    "        if self.requires_grad:\n",
    "             self.grad = np.zeros_like(self.data)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a3ef82-0717-4202-ae38-468f1f21e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            queries = self.W_query.forward(x)\n",
    "        else:\n",
    "            queries = x @ self.W_query.data\n",
    "\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            keys = self.W_key.forward(x)\n",
    "        else:\n",
    "            keys = x @ self.W_key.data\n",
    "\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            values = self.W_value.forward(x)\n",
    "        else:\n",
    "            values = x @ self.W_value.data\n",
    "\n",
    "            \n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            self.W_query.lora_A.zerograds()\n",
    "            self.W_query.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_query.zerograds()\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            self.W_key.lora_A.zerograds()\n",
    "            self.W_key.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_key.zerograds()\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            self.W_value.lora_A.zerograds()\n",
    "            self.W_value.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_value.zerograds()\n",
    "\n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        if hasattr(self.W_query, 'backward'):\n",
    "            d_x_from_queries = self.W_query.backward(d_queries, self.cache['x'])\n",
    "        else:\n",
    "            W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query.data, self.cache['x'])\n",
    "\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.grad = W_query_grad\n",
    "        if hasattr(self.W_key, 'backward'):\n",
    "            d_x_from_keys = self.W_key.backward(d_keys, self.cache['x'])\n",
    "        else:\n",
    "            W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key.data, self.cache['x'])\n",
    "\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.grad = W_key_grad\n",
    "        if hasattr(self.W_value, 'backward'):\n",
    "            d_x_from_values = self.W_value.backward(d_values, self.cache['x'])\n",
    "        else:\n",
    "            W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value.data, self.cache['x'])\n",
    "            \n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.grad = W_value_grad\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        if hasattr(self.W_query, 'optimizer'):\n",
    "            self.W_query.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.data -= (self.W_query.grad * learning_rate)\n",
    "        if hasattr(self.W_key, 'optimizer'):\n",
    "            self.W_key.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.data -= (self.W_key.grad * learning_rate)\n",
    "        if hasattr(self.W_value, 'optimizer'):\n",
    "            self.W_value.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.data -= (self.W_value.grad * learning_rate)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2648dbe3-8102-4d1e-8b89-46485bc1835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_k = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_v = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = Parameter(np.random.randn(n_embd, n_embd) * 0.02)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output.data\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output.zerograds()\n",
    "        \n",
    "        W_output_grad, d_concat = self.linear_backward(d_output, self.W_output.data, self.cache['concat_output'])\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.grad = W_output_grad\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.data -= (self.W_output.grad * learning_rate)\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f90b01-cc96-4223-88d4-468b03d5c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = Parameter(np.ones((n_embd,)))\n",
    "        self.beta = Parameter(np.zeros((n_embd,)))\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma.data\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma.data + self.beta.data\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.beta.zerograds()\n",
    "        self.gamma.zerograds()\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.grad = np.sum(d_output, axis=(0,1))\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.data -= (self.gamma.grad * learning_rate)\n",
    "\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.data -= (self.beta.grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5b63b8-7988-4f57-9f3f-1ba4f59a8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1.data\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.W1.zerograds()\n",
    "        self.W2.zerograds()\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2.data, self.cache['hidden_activated'])\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1.data, self.cache['norm_output_1'])\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        \n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.data -= (self.W1.grad * learning_rate)\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.data -= (self.W2.grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255ef7dc-845c-4909-ae58-b90bd28fd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        vocab_size = len(chars)\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = Parameter(np.random.randn(vocab_size, n_embd))\n",
    "        self.position_matrix = Parameter(np.random.randn(max_sequence_length, n_embd))\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = Parameter(np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd))\n",
    "            W2 = Parameter(np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor)))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix.data[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix.data[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.data.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "        \n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix.zerograds()\n",
    "        self.position_matrix.zerograds()\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix.data  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "        # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            # Perform reverse lookup on embedding array\n",
    "            np.add.at(self.embedding_matrix.grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.data -= (self.embedding_matrix.grad * learning_rate)\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.data -= (self.position_matrix.grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "\n",
    "        parameters = {\n",
    "            \"embedding_matrix\": self.embedding_matrix,\n",
    "            \"position_matrix\": self.position_matrix,\n",
    "        }\n",
    "\n",
    "        for idx, transformer in enumerate(self.transformers):\n",
    "            parameters[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "            parameters[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "            parameters[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "            parameters[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "            parameters[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "            \n",
    "            for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def freezeParams(self): \n",
    "\n",
    "        # Freezes model weights in prereration for LoRA training\n",
    "        self.embedding_matrix.requires_grad=False\n",
    "        self.position_matrix.requires_grad=False\n",
    "        \n",
    "        for x in self.transformers:\n",
    "\n",
    "            x.W1.requires_grad=False\n",
    "            x.W2.requires_grad=False\n",
    "            x.layer_norm1.gamma.requires_grad=False\n",
    "            x.layer_norm1.beta.requires_grad=False\n",
    "            x.layer_norm2.gamma.requires_grad=False\n",
    "            x.layer_norm2.beta.requires_grad=False\n",
    "            x.multi_head_attention_block.W_output.requires_grad=False\n",
    "            \n",
    "            for i in x.multi_head_attention_block.heads:\n",
    "\n",
    "                i.W_key.requires_grad=False\n",
    "                i.W_query.requires_grad=False\n",
    "                i.W_value.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d20ed2-0ba8-44c0-8a24-05fbb8072f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a095dd-8bde-46e9-8f5b-26187189b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix\n",
      "position_matrix\n",
      "transform.0.W1\n",
      "transform.0.W2\n",
      "transform.0.layer_norm1.gamma\n",
      "transform.0.layer_norm1.beta\n",
      "transform.0.layer_norm2.gamma\n",
      "transform.0.layer_norm2.beta\n",
      "transform.0.multi_head_attention_block.W_output\n",
      "transform.0.multi_head_attention_block.heads.0.key\n",
      "transform.0.multi_head_attention_block.heads.0.query\n",
      "transform.0.multi_head_attention_block.heads.0.value\n",
      "transform.0.multi_head_attention_block.heads.1.key\n",
      "transform.0.multi_head_attention_block.heads.1.query\n",
      "transform.0.multi_head_attention_block.heads.1.value\n",
      "transform.0.multi_head_attention_block.heads.2.key\n",
      "transform.0.multi_head_attention_block.heads.2.query\n",
      "transform.0.multi_head_attention_block.heads.2.value\n",
      "transform.0.multi_head_attention_block.heads.3.key\n",
      "transform.0.multi_head_attention_block.heads.3.query\n",
      "transform.0.multi_head_attention_block.heads.3.value\n",
      "transform.0.multi_head_attention_block.heads.4.key\n",
      "transform.0.multi_head_attention_block.heads.4.query\n",
      "transform.0.multi_head_attention_block.heads.4.value\n",
      "transform.0.multi_head_attention_block.heads.5.key\n",
      "transform.0.multi_head_attention_block.heads.5.query\n",
      "transform.0.multi_head_attention_block.heads.5.value\n",
      "transform.0.multi_head_attention_block.heads.6.key\n",
      "transform.0.multi_head_attention_block.heads.6.query\n",
      "transform.0.multi_head_attention_block.heads.6.value\n",
      "transform.0.multi_head_attention_block.heads.7.key\n",
      "transform.0.multi_head_attention_block.heads.7.query\n",
      "transform.0.multi_head_attention_block.heads.7.value\n",
      "transform.1.W1\n",
      "transform.1.W2\n",
      "transform.1.layer_norm1.gamma\n",
      "transform.1.layer_norm1.beta\n",
      "transform.1.layer_norm2.gamma\n",
      "transform.1.layer_norm2.beta\n",
      "transform.1.multi_head_attention_block.W_output\n",
      "transform.1.multi_head_attention_block.heads.0.key\n",
      "transform.1.multi_head_attention_block.heads.0.query\n",
      "transform.1.multi_head_attention_block.heads.0.value\n",
      "transform.1.multi_head_attention_block.heads.1.key\n",
      "transform.1.multi_head_attention_block.heads.1.query\n",
      "transform.1.multi_head_attention_block.heads.1.value\n",
      "transform.1.multi_head_attention_block.heads.2.key\n",
      "transform.1.multi_head_attention_block.heads.2.query\n",
      "transform.1.multi_head_attention_block.heads.2.value\n",
      "transform.1.multi_head_attention_block.heads.3.key\n",
      "transform.1.multi_head_attention_block.heads.3.query\n",
      "transform.1.multi_head_attention_block.heads.3.value\n",
      "transform.1.multi_head_attention_block.heads.4.key\n",
      "transform.1.multi_head_attention_block.heads.4.query\n",
      "transform.1.multi_head_attention_block.heads.4.value\n",
      "transform.1.multi_head_attention_block.heads.5.key\n",
      "transform.1.multi_head_attention_block.heads.5.query\n",
      "transform.1.multi_head_attention_block.heads.5.value\n",
      "transform.1.multi_head_attention_block.heads.6.key\n",
      "transform.1.multi_head_attention_block.heads.6.query\n",
      "transform.1.multi_head_attention_block.heads.6.value\n",
      "transform.1.multi_head_attention_block.heads.7.key\n",
      "transform.1.multi_head_attention_block.heads.7.query\n",
      "transform.1.multi_head_attention_block.heads.7.value\n",
      "transform.2.W1\n",
      "transform.2.W2\n",
      "transform.2.layer_norm1.gamma\n",
      "transform.2.layer_norm1.beta\n",
      "transform.2.layer_norm2.gamma\n",
      "transform.2.layer_norm2.beta\n",
      "transform.2.multi_head_attention_block.W_output\n",
      "transform.2.multi_head_attention_block.heads.0.key\n",
      "transform.2.multi_head_attention_block.heads.0.query\n",
      "transform.2.multi_head_attention_block.heads.0.value\n",
      "transform.2.multi_head_attention_block.heads.1.key\n",
      "transform.2.multi_head_attention_block.heads.1.query\n",
      "transform.2.multi_head_attention_block.heads.1.value\n",
      "transform.2.multi_head_attention_block.heads.2.key\n",
      "transform.2.multi_head_attention_block.heads.2.query\n",
      "transform.2.multi_head_attention_block.heads.2.value\n",
      "transform.2.multi_head_attention_block.heads.3.key\n",
      "transform.2.multi_head_attention_block.heads.3.query\n",
      "transform.2.multi_head_attention_block.heads.3.value\n",
      "transform.2.multi_head_attention_block.heads.4.key\n",
      "transform.2.multi_head_attention_block.heads.4.query\n",
      "transform.2.multi_head_attention_block.heads.4.value\n",
      "transform.2.multi_head_attention_block.heads.5.key\n",
      "transform.2.multi_head_attention_block.heads.5.query\n",
      "transform.2.multi_head_attention_block.heads.5.value\n",
      "transform.2.multi_head_attention_block.heads.6.key\n",
      "transform.2.multi_head_attention_block.heads.6.query\n",
      "transform.2.multi_head_attention_block.heads.6.value\n",
      "transform.2.multi_head_attention_block.heads.7.key\n",
      "transform.2.multi_head_attention_block.heads.7.query\n",
      "transform.2.multi_head_attention_block.heads.7.value\n",
      "transform.3.W1\n",
      "transform.3.W2\n",
      "transform.3.layer_norm1.gamma\n",
      "transform.3.layer_norm1.beta\n",
      "transform.3.layer_norm2.gamma\n",
      "transform.3.layer_norm2.beta\n",
      "transform.3.multi_head_attention_block.W_output\n",
      "transform.3.multi_head_attention_block.heads.0.key\n",
      "transform.3.multi_head_attention_block.heads.0.query\n",
      "transform.3.multi_head_attention_block.heads.0.value\n",
      "transform.3.multi_head_attention_block.heads.1.key\n",
      "transform.3.multi_head_attention_block.heads.1.query\n",
      "transform.3.multi_head_attention_block.heads.1.value\n",
      "transform.3.multi_head_attention_block.heads.2.key\n",
      "transform.3.multi_head_attention_block.heads.2.query\n",
      "transform.3.multi_head_attention_block.heads.2.value\n",
      "transform.3.multi_head_attention_block.heads.3.key\n",
      "transform.3.multi_head_attention_block.heads.3.query\n",
      "transform.3.multi_head_attention_block.heads.3.value\n",
      "transform.3.multi_head_attention_block.heads.4.key\n",
      "transform.3.multi_head_attention_block.heads.4.query\n",
      "transform.3.multi_head_attention_block.heads.4.value\n",
      "transform.3.multi_head_attention_block.heads.5.key\n",
      "transform.3.multi_head_attention_block.heads.5.query\n",
      "transform.3.multi_head_attention_block.heads.5.value\n",
      "transform.3.multi_head_attention_block.heads.6.key\n",
      "transform.3.multi_head_attention_block.heads.6.query\n",
      "transform.3.multi_head_attention_block.heads.6.value\n",
      "transform.3.multi_head_attention_block.heads.7.key\n",
      "transform.3.multi_head_attention_block.heads.7.query\n",
      "transform.3.multi_head_attention_block.heads.7.value\n",
      "transform.4.W1\n",
      "transform.4.W2\n",
      "transform.4.layer_norm1.gamma\n",
      "transform.4.layer_norm1.beta\n",
      "transform.4.layer_norm2.gamma\n",
      "transform.4.layer_norm2.beta\n",
      "transform.4.multi_head_attention_block.W_output\n",
      "transform.4.multi_head_attention_block.heads.0.key\n",
      "transform.4.multi_head_attention_block.heads.0.query\n",
      "transform.4.multi_head_attention_block.heads.0.value\n",
      "transform.4.multi_head_attention_block.heads.1.key\n",
      "transform.4.multi_head_attention_block.heads.1.query\n",
      "transform.4.multi_head_attention_block.heads.1.value\n",
      "transform.4.multi_head_attention_block.heads.2.key\n",
      "transform.4.multi_head_attention_block.heads.2.query\n",
      "transform.4.multi_head_attention_block.heads.2.value\n",
      "transform.4.multi_head_attention_block.heads.3.key\n",
      "transform.4.multi_head_attention_block.heads.3.query\n",
      "transform.4.multi_head_attention_block.heads.3.value\n",
      "transform.4.multi_head_attention_block.heads.4.key\n",
      "transform.4.multi_head_attention_block.heads.4.query\n",
      "transform.4.multi_head_attention_block.heads.4.value\n",
      "transform.4.multi_head_attention_block.heads.5.key\n",
      "transform.4.multi_head_attention_block.heads.5.query\n",
      "transform.4.multi_head_attention_block.heads.5.value\n",
      "transform.4.multi_head_attention_block.heads.6.key\n",
      "transform.4.multi_head_attention_block.heads.6.query\n",
      "transform.4.multi_head_attention_block.heads.6.value\n",
      "transform.4.multi_head_attention_block.heads.7.key\n",
      "transform.4.multi_head_attention_block.heads.7.query\n",
      "transform.4.multi_head_attention_block.heads.7.value\n",
      "transform.5.W1\n",
      "transform.5.W2\n",
      "transform.5.layer_norm1.gamma\n",
      "transform.5.layer_norm1.beta\n",
      "transform.5.layer_norm2.gamma\n",
      "transform.5.layer_norm2.beta\n",
      "transform.5.multi_head_attention_block.W_output\n",
      "transform.5.multi_head_attention_block.heads.0.key\n",
      "transform.5.multi_head_attention_block.heads.0.query\n",
      "transform.5.multi_head_attention_block.heads.0.value\n",
      "transform.5.multi_head_attention_block.heads.1.key\n",
      "transform.5.multi_head_attention_block.heads.1.query\n",
      "transform.5.multi_head_attention_block.heads.1.value\n",
      "transform.5.multi_head_attention_block.heads.2.key\n",
      "transform.5.multi_head_attention_block.heads.2.query\n",
      "transform.5.multi_head_attention_block.heads.2.value\n",
      "transform.5.multi_head_attention_block.heads.3.key\n",
      "transform.5.multi_head_attention_block.heads.3.query\n",
      "transform.5.multi_head_attention_block.heads.3.value\n",
      "transform.5.multi_head_attention_block.heads.4.key\n",
      "transform.5.multi_head_attention_block.heads.4.query\n",
      "transform.5.multi_head_attention_block.heads.4.value\n",
      "transform.5.multi_head_attention_block.heads.5.key\n",
      "transform.5.multi_head_attention_block.heads.5.query\n",
      "transform.5.multi_head_attention_block.heads.5.value\n",
      "transform.5.multi_head_attention_block.heads.6.key\n",
      "transform.5.multi_head_attention_block.heads.6.query\n",
      "transform.5.multi_head_attention_block.heads.6.value\n",
      "transform.5.multi_head_attention_block.heads.7.key\n",
      "transform.5.multi_head_attention_block.heads.7.query\n",
      "transform.5.multi_head_attention_block.heads.7.value\n",
      "Loading saved model weights...\n",
      "Loaded main matrices\n",
      "Loading transformer 0...\n",
      "Loaded transformer 0\n",
      "Loading transformer 1...\n",
      "Loaded transformer 1\n",
      "Loading transformer 2...\n",
      "Loaded transformer 2\n",
      "Loading transformer 3...\n",
      "Loaded transformer 3\n",
      "Loading transformer 4...\n",
      "Loaded transformer 4\n",
      "Loading transformer 5...\n",
      "Loaded transformer 5\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved weights\n",
    "loaded_weights = np.load('../models/my_model.npz')\n",
    "\n",
    "for key in loaded_weights.keys():\n",
    "    print(key)\n",
    "print(\"Loading saved model weights...\")\n",
    "\n",
    "# Set the main model weights\n",
    "model.embedding_matrix = Parameter(loaded_weights[\"embedding_matrix\"])\n",
    "model.position_matrix = Parameter(loaded_weights[\"position_matrix\"])\n",
    "print(\"Loaded main matrices\")\n",
    "\n",
    "# Load transformer weights\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    print(f\"Loading transformer {idx}...\")\n",
    "    \n",
    "    # Load basic weights\n",
    "    transformer.W1 = Parameter(loaded_weights[f\"transform.{idx}.W1\"])\n",
    "    transformer.W2 = Parameter(loaded_weights[f\"transform.{idx}.W2\"])\n",
    "    transformer.layer_norm1.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.gamma\"])\n",
    "    transformer.layer_norm1.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.beta\"])\n",
    "    transformer.layer_norm2.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.gamma\"])\n",
    "    transformer.layer_norm2.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.beta\"])\n",
    "    transformer.multi_head_attention_block.W_output = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.W_output\"])\n",
    "    \n",
    "    # Load attention head weights (16 heads each)\n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        attention_head.W_key = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"])\n",
    "        attention_head.W_query = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"])\n",
    "        attention_head.W_value = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"])\n",
    "    \n",
    "    print(f\"Loaded transformer {idx}\")\n",
    "\n",
    "loaded_weights.close()\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95e997c1-5e97-4c33-b725-c1709ff67b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:21: RuntimeWarning: divide by zero encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:21: RuntimeWarning: overflow encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:21: RuntimeWarning: invalid value encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:26: RuntimeWarning: divide by zero encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:26: RuntimeWarning: overflow encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:26: RuntimeWarning: invalid value encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:31: RuntimeWarning: divide by zero encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:31: RuntimeWarning: overflow encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:31: RuntimeWarning: invalid value encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:39: RuntimeWarning: divide by zero encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:39: RuntimeWarning: overflow encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:39: RuntimeWarning: invalid value encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:57: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:57: RuntimeWarning: overflow encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3910858463.py:57: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3048600086.py:45: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3048600086.py:45: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3048600086.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3624853755.py:22: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3624853755.py:22: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3624853755.py:22: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3624853755.py:28: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3624853755.py:28: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/3624853755.py:28: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.963068944809678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/2046029974.py:62: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/2046029974.py:62: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_23528/2046029974.py:62: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_batch(data, 128, 34)\n",
    "\n",
    "\n",
    "# Calculate loss and probabilites\n",
    "logits = model.forward(x_batch)\n",
    "loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "print(loss_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1bb214-fbe0-4a80-a312-5cee61efd734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer frozen? True\n",
      "Attention layer 15 frozen? True\n"
     ]
    }
   ],
   "source": [
    "# This one loop will freeze every parameter in the entire model\n",
    "for key, param in model.parameters.items():\n",
    "    # \"Freeze\" the model's weight\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Embedding layer frozen?\", model.transformers[4].multi_head_attention_block.W_output.requires_grad== False)\n",
    "print(\"Attention layer 15 frozen?\", model.transformers[2].multi_head_attention_block.heads[0].W_key.requires_grad == False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979888d3-0703-4db6-a6db-ddd5c9b6c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "\n",
    "    def __init__(self, original_layer, rank, alpha):\n",
    "\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        # Get dimensions from the original layer\n",
    "        in_features, out_features = self.original_layer.data.shape\n",
    "\n",
    "\n",
    "        # Initialize LoRA A & B matrices\n",
    "        self.lora_A = Parameter(np.random.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = Parameter(np.random.randn(rank,, out_features) * 0.01)\n",
    "\n",
    "\n",
    "        # Scaling factor\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The origin linear layer calculation\n",
    "        original_output = x @ self.original_layer.data\n",
    "\n",
    "        # New update w/A & B matrices\n",
    "        lora_update = (x @ self.lora_A.data @ self.lora_B.data) * self.scaling\n",
    "\n",
    "        return original_output + lora_update\n",
    "    \n",
    "    def backward(self, d_output, x_input):\n",
    "\n",
    "        self.lora_A.zerograds()\n",
    "        self.lora_B.zerograds()\n",
    "        \n",
    "        # Gradient w.r.t input (what we return)\n",
    "        # d_x = d_output @ (W_original + A @ B)^T\n",
    "        # Since W_original is frozen, we only need gradients through A @ B\n",
    "        d_x_original = d_output @ self.original_layer.data.T\n",
    "        d_x_lora = d_output @ (self.lora_A.data @ self.lora_B.data).T * self.scaling\n",
    "        d_x = d_x_original + d_x_lora\n",
    "        \n",
    "        # Gradients for LoRA parameters\n",
    "        # Reshape inputs for matrix operations\n",
    "        x_reshaped = x_input.reshape(-1, x_input.shape[-1])  # (B*T, in_features)\n",
    "        d_output_reshaped = d_output.reshape(-1, d_output.shape[-1])  # (B*T, out_features)\n",
    "        \n",
    "        # Gradient w.r.t B: A^T @ x^T @ d_output * scaling\n",
    "        self.lora_B.grad = (self.lora_A.data.T @ x_reshaped.T) @ d_output_reshaped * self.scaling\n",
    "        \n",
    "        # Gradient w.r.t A: x^T @ d_output @ B^T * scaling  \n",
    "        self.lora_A.grad = x_reshaped.T @ (d_output_reshaped @ self.lora_B.data.T) * self.scaling\n",
    "        \n",
    "        # Don't compute gradients for original_layer since it's frozen\n",
    "        return d_x        \n",
    "    \n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.lora_A.requires_grad:\n",
    "            self.lora_A.data -= (self.lora_A.grad * learning_rate)\n",
    "\n",
    "        if self.lora_B.requires_grad:\n",
    "            self.lora_B.data -= (self.lora_B.grad * learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a627b5-da56-4ec0-afd7-c019791472af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LoRA layers in model's query and value attention matrices\n",
    "for transformer in model.transformers:\n",
    "    for head in transformer.multi_head_attention_block.heads:\n",
    "\n",
    "\n",
    "        # Rank and alpha variables for LoRA layers\n",
    "        r = 16\n",
    "        a = 16\n",
    "\n",
    "        # Wrap query and value weights with LoRA\n",
    "        head.W_query = LoRALayer(head.W_query, r, a)\n",
    "        head.W_value = LoRALayer(head.W_value, r, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746d5489-e771-47a9-a667-7b2c014f607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature, heatMap, max_length):\n",
    "    print(f\"Generating with prompt: '{prompt}', temp: {temperature}, length: {max_length}\")\n",
    "    \n",
    "    if not prompt:\n",
    "        prompt = \"\\n\" # Default prompt\n",
    "\n",
    "    # Set the model's temperature for this specific generation\n",
    "    model.temperature = temperature\n",
    "    \n",
    "    # Encode the prompt and generate\n",
    "    char_indices = encode(prompt)\n",
    "    for _ in range(int(max_length)):\n",
    "        # Important: only feed the last block_size tokens as context\n",
    "        context = char_indices[-1000:]\n",
    "        next_char_index = model.pred(context)\n",
    "        char_indices.append(next_char_index)\n",
    "        \n",
    "    # Decode the final list of indices into a string\n",
    "    generated_text = decode(char_indices)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f69d3-557a-41a0-bbb3-23918ea36043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAHHCAYAAABQhTneAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAASLFJREFUeJzt3Ql8VNX5//En62RPCBCWhIAQZA+bIsEFKyACRVxqFW1BRS2Irdb214ooVhCh8teKVhF3qiIqBbQqoqKguLBJMOwgS1gCYctO9vm/nhNmTCABhGTuZO7n/XrdznZncnKTkq/nPOccP6fT6RQAAACb8Le6AQAAAJ5E+AEAALZC+AEAALZC+AEAALZC+AEAALZC+AEAALZC+AEAALZC+AEAALZC+AEAALZC+AHqkVtvvVVatWp1Vu/9xz/+IX5+frXeJtQPl19+uTkAEH6AWqGh4kyOJUuWiF1DW0REhNQHuuPPG2+8IZdddpnExMRIWFiYdOnSRSZOnCj5+fniLXbu3HnGv3d6LoCf+bG3F3Du3nzzzSqP//Of/8hnn31m/ohWNmDAAGnSpMlZf52SkhIpLy8Xh8Pxi99bWlpqjpCQELEi/MydO1fy8vLEm5WVlcnNN98s7777rlx66aVy3XXXmfDz9ddfy+zZs6Vjx47y+eefn9PPsLZoEJs/f36V55588knZs2eP/Otf/6ry/LXXXitBQUHmfnBwsEfbCXgjwg9QB+655x557rnnTC/CqRQUFJg/rr6uvoSfKVOmyIMPPih//etfZdq0aVVe+9///ifXXHONXHnllbJw4UKPtutMf09+/etfy7p16+jpAU6DYS/AQ7TeonPnzrJ69WozpKJ/zPQPrXr//fdlyJAh0rx5c9Or06ZNG5k0aZLpiThVzY9r6OP//b//Jy+++KJ5n77/wgsvlJUrV5625kcfa1BbsGCBaZu+t1OnTvLJJ5+c1H4dsrvgggtMz5F+nZkzZ9Z6HdF7770nPXv2lNDQUGnUqJH87ne/k71791Y5Z//+/XLbbbdJQkKCaW+zZs1k2LBhVf7gr1q1SgYOHGg+Qz/rvPPOk9tvv/2UX/vYsWMm8Jx//vkmBJ1o6NChMnLkSHNtvv/+e3fYaN26dbWfl5KSYq7XiT2Eru8vNjZWbrrpJtm9e/cZ/57UZs2P/jz1Z6e9XI8++qjEx8dLZGSk/OY3v5Hs7GwpKiqS++67T+Li4syQpV5zfe5EZ/I9Ad4m0OoGAHZy+PBhGTRokPkDoX/YXcMnr7/+uvkDc//995vbL774QiZMmCA5OTkn9UBUR4dkcnNz5Q9/+IP5g/bEE0+YIZvt27e7hztqsmzZMpk3b57cfffd5o/fM888I9dff72kp6dLw4YNzTlr1qyRq666ygQN/UOpoUxrYBo3blxLV6biGugfWA1uGj4OHDgg06dPl2+++cZ8fa2/Udq29evXyx//+EcTBDMzM80Qo7bX9Vh7Z7RtDzzwgHmfBiP9Hk93HY4ePSr33nuvBAZW/0/jiBEj5LXXXpMPP/xQevfuLTfeeKN5ToOmtttl165dJiBV/tlNnjxZHn74Yfntb38rd9xxhxw8eFCeffZZE3Aqf3+n+j2pC3qtNbjotdq2bZtpk/7O+Pv7m+uhAVe/F/35aIjU38uz+Z4Ar6LDXgBq19ixY3W8q8pzffv2Nc+98MILJ51fUFBw0nN/+MMfnGFhYc7CwkL3cyNHjnS2bNnS/XjHjh3mMxs2bOg8cuSI+/n333/fPP+///3P/dwjjzxyUpv0cXBwsHPbtm3u59auXWuef/bZZ93PDR061LRl79697ue2bt3qDAwMPOkzq6PtDg8Pr/H14uJiZ1xcnLNz587OY8eOuZ//8MMPzedPmDDBPD569Kh5PG3atBo/a/78+eaclStXOn+Jp59+2rxP318TvcZ6znXXXWceZ2dnOx0Oh/Mvf/lLlfOeeOIJp5+fn3PXrl3m8c6dO50BAQHOyZMnVzkvLS3NXMPKz5/q9+R0hgwZUuX3ozL9XD1cvvzyS/N19Jrr9XcZPny4afugQYOqvD8lJaXKZ/+S7wnwNgx7AR6kwzTau3Ei/S9vF+3BOXTokCm41VqPTZs2nfZztQeiQYMG7sf6XqU9P6fTv39/M4zlkpycLFFRUe73ai+PFvlqvYsOy7kkJSWZ3onaoMNU2mOjvU+VC7J1KLB9+/by0Ucfua+TFuzqkI32SlTH1dugvTNaIH6m9Lor7f2qies17ZFTep30GujQUeX6rnfeecf0DCUmJprH2uukheraQ6I/W9fRtGlTadu2rXz55Zdn9HtSF7TnqnLv4EUXXWS+lxOHCfV5Hc7Sovmz+Z4Ab0L4ATxI6yqqm22jwzg6Iyc6Otr8QdUhGx3uUFp/cTquP7IuriBUU0A41Xtd73e9V0OJ1sNo2DlRdc+dDR0mUu3atTvpNQ0/rtc1FPzzn/80Bcc6FKTDKzrEp3VALn379jVDYzo8pzU/Wg+kQ1XV1atUF2xcIehMA5IGTw0F3333nXn8008/mXodfd5l69atJlBoKNCfbeVj48aN5hqfye9JXTjx56+/g6pFixYnPa9hx/X7+Eu/J8CbUPMDeFDlHh6XrKws8wdbQ4/W0WgvjPZ+/PDDD/L3v//d/ME5nYCAgGqfP5PJnOfyXitoEa4WH2uR9qJFi0zNidataJ1U9+7dTc2TzizTOhWdoaXnaC+GTgPX52pab6hDhw7m9scffzS9XNXR15ROeXfRtmhRsvb+9OnTx9xqvcwNN9zgPkd/htouDW3VXe8T21Td70ldqennf7rfi1/6PQHehPADWEyHcLTAVYcRtCfDZceOHeINdLaPhjEthj1Rdc+djZYtW5rbzZs3yxVXXFHlNX3O9bqLBsS//OUv5tAeiG7duplwU3m9JR120kOLcrUg/JZbbpE5c+aYwtzqXHLJJWbITM8dP358tX/Qdf0m1ywvl/DwcPNYZ6o99dRTZshLhx0rDxFqezU0aMGwzibzBb74PcE+GPYCLOb6I1u5p6W4uFief/558Zb2aV2Q9rTs27evSvCprfVudEq4hqwXXnihyvCUfr4OoWjtj9IaqMLCwpP+COswlOt9Olx3Yq+VhiN1qqEv7b3R9X00bGn4OZHWHemMJ51Cr6GqMh3i0mvz8ssvy9q1a6sMeSmdeafXUYfiTmybPtbwW9/44vcE+6DnB7CYDpVojY2uIfOnP/3JDCXoytDeNOyk050//fRTufjii2XMmDGmCPrf//63WY8mNTX1jD5Di48fe+yxk57XtWG00FlrebTIV4cAhw8f7p7qrtPX//znP5tzt2zZIv369TNFtjr0pFPSdZVjPVenhatZs2aZ4Kg1VBqMtE7npZdeMsOKgwcPPmUbdbq3TtHWtmgNj9YO6RCUToPXXiUdGtPPP5F+rgYwDU8aCPR9lWk79HsfN26cmXavw2p6vvbuafvvuusu8976xBe/J9gH4QewmK6lozOTdAjnoYceMkFIi531j7z2MngDXcROe2H0j5nW2GgxrNYnaa/MmcxGc/Vm6Xur+yOq4UcXcNTel6lTp5paJx1O0gCjQcQ1g0u/rgajxYsXm4Co4UcLorXOxhU4NDytWLHCDHFpKNJC3V69eslbb71lhmhORYOLfpYOb2kvjrZX261tfOSRR8zPSNt1Ih0WvPrqq83X0F4y7cWqLljp8JBuPaG9Ja7vR9ck0vfWR774PcEe2N4CwFnT/9rXmWpadwMA9QU1PwDOiE53r0wDz8cff1xlywQAqA/o+QFwRnRrCx2a0r2sdN2dGTNmmAJirZHRtV4AoL6g5gfAGdG9vd5++22zoKAuNqgbdz7++OMEHwD1jtcMe2mRo85y0QXMTkXX0tACRy0w7NKli+l2B1D3dJVkndWjU811lV/d3bxHjx5WNwsA6mf40R2RZ86cafYUOpVvv/3WzPQYNWqU6WrXYks91q1b57G2AgCA+s3ymp+8vDzzX4+6LoeuGaGLkT399NPVnqsLh+Xn55tpwS662Ji+RxdHAwAA8Pqan7Fjx5rVW3VtjOoWQKtMFx27//77qzyn66DoyrM10YLMyqu66n40R44cMWur6DAbAADwftpXo4uW6tYxun9evQ0/ugiZbt6ow15nQgstdSfnyvRx5R2dT6QbHroW3wIAAPXb7t27JSEhoX6GH238vffeK5999pkpXq4ruvR65d4iLdRMTEw0X1+Xu69ruw7ny5Bnlkm4I0CWP9i/zr8eAAC+KCcnx6wgrtuonCvLws/q1aslMzOzymwR3S/oq6++MnsG6VDVibsqN23a1CxXX5k+1udrolNy9TiRBh9PhJ9GziDxd4RJaYCfR74eAAC+zK8WSlYsm+2l+xalpaWZTRFdh+7sfMstt5j7JwYfpeuK6J4+lWnPkT7vrRyBFd9HSZlTyspZTxIAAKtZ1vOj3Va6I3RlumGgFiK7nh8xYoTEx8ebuh2lw2S6aeGTTz5piqS1ZmjVqlXy4osvirdyBP6cL4tLyyU0+ORQBwAAbLbOT03S09MlIyPD/bhPnz4ye/ZsE3a6du0qc+fONTO9TgxR3hp+ikrLLG0LAADwgnV+rCiYio6ONoXPnqrBafPgx2bIa/mD/aRJVN0VdwMA4KtyavHvt1f3/PgKV+9PUUm51U0BAMD2CD+eDD8MewEAYDnCjwdnfBWV0vMDAIDVCD8e4Aii5wcAAG9B+PEAan4AAPAehB8PYNgLAADvQfjxAAqeAQDwHoQfj9b80PMDAIDVCD8ewLAXAADeg/Dj0WEvwg8AAFYj/Hh0thc1PwAAWI3w4wGundwLigk/AABYjfDjAWHBgeaW8AMAgPUIPx4Qdrzn51hxqdVNAQDA9gg/Hhz2yqfnBwAAyxF+PCAsyNXzQ/gBAMBqhB+P1vww7AUAgNUIPx7AbC8AALwH4ceTBc+s8wMAgOUIPx5Azw8AAN6D8OPBmh8KngEAsB7hxwPC3T0/FDwDAGA1wo8HsM4PAADeg/DjwWGv4tJyKSt3Wt0cAABsjfDjwdleiqEvAACsRfjxAEegv/j5Vdyn6BkAAGsRfjzAz8/PvcUF090BALAW4cdDQt1bXBB+AACwEuHH46s8U/MDAICVCD8eDj/0/AAAYC3Cj4fDT34R4QcAACsRfjy9xQXDXgAAWIrw4yH0/AAA4B0IPx4S7nDN9qLnBwAAKxF+PCTcUdHzk0fPDwAAliL8eLjnJ7+Inh8AAKxE+PGQiOMFz4QfAACsRfjxcM9PHuEHAABLEX48JIJhLwAAvALhx9M1P6zwDACAfcPPjBkzJDk5WaKiosyRkpIiCxcurPH8kpISmThxorRp00ZCQkKka9eu8sknn0h9mu1Fzw8AADYOPwkJCTJ16lRZvXq1rFq1Sq644goZNmyYrF+/vtrzH3roIZk5c6Y8++yzsmHDBhk9erRce+21smbNGvF2zPYCAMA7+DmdTqd4kdjYWJk2bZqMGjXqpNeaN28u48ePl7Fjx7qfu/766yU0NFTefPPNM/r8nJwciY6OluzsbNPb5Ckb9uXI4Ge+lkYRDln1UH+PfV0AAHxBTi3+/a7ojvACZWVl8t5770l+fr4Z/qpOUVGRGe6qTIPPsmXLavxcfY8elS+eFSh4BgDAO1he8JyWliYRERHicDjMMNb8+fOlY8eO1Z47cOBAeeqpp2Tr1q1SXl4un332mcybN08yMjJq/PwpU6aYpOg6WrRoIVbW/BwrKZOycq/qbAMAwFYsDz/t2rWT1NRUWb58uYwZM0ZGjhxp6nmqM336dGnbtq20b99egoOD5Z577pHbbrtN/P1r/jbGjRtnushcx+7du8XKmh/F/l4AANg4/GiISUpKkp49e5peGp3BpSGnOo0bN5YFCxaYobFdu3bJpk2bTK9R69ata/x87VFyzSZzHVZwBPpLoL+fuc/O7gAA2Dj8nEiHsyrX6FRH637i4+OltLRU/vvf/5oZYt7Oz8+PVZ4BAPAClhY865DUoEGDJDExUXJzc2X27NmyZMkSWbRokXl9xIgRJuRoj5DSobG9e/dKt27dzO0//vEPE5b+9re/SX0QHhwg2cdKKHoGAMCu4SczM9MEHC1Y1mJkXfBQg8+AAQPM6+np6VXqeQoLC81aP9u3bzfDXYMHD5Y33nhDYmJipD5grR8AAGwefl555ZVTvq69QJX17du3xmLo+oBhLwAArOd1NT++LCo0yNzmFhJ+AACwCuHHg6JCKnp+cgpLrG4KAAC2RfixoOdHi54BAIA1CD8eFH08/OQcY9gLAACrEH48KCrkePhh2AsAAMsQfjwoKrSi5odhLwAArEP4sWTYi/ADAIBVCD8WDHvR8wMAgHUIPx7EOj8AAFiP8ONBDHsBAGA9wo8FixzmFpVKWbnT6uYAAGBLhB8Lhr1ULtPdAQCwBOHHg4IC/CUsOMDcZ6FDAACsQfjxMGZ8AQBgLcKPRQsdssozAADWIPx4GDO+AACwFuHHwxj2AgDAWoQfi2Z8MewFAIA1CD+WDXsx2wsAACsQfixa6JBhLwAArEH48TCGvQAAsBbhx6rwQ88PAACWIPx4GLO9AACwFuHHskUOKXgGAMAKhB8PY5FDAACsRfjxMIa9AACwFuHHooLnotJyKSwps7o5AADYDuHHwyIdgeLnV3E/l7ofAAA8jvDjYf7+fiYAKYa+AADwPMKPBWLCgs1tVkGx1U0BAMB2CD8WiA2vCD+H8wk/AAB4GuHHAo0iKsLPEcIPAAAeR/ixQMNwh7k9nFdkdVMAALAdwo8FYo/3/BzKo+cHAABPI/xYoOHxmh+GvQAA8DzCjwUaRRwf9spn2AsAAE8j/Fg524thLwAAPI7wY4GGx2t+mOoOAIDnEX4sHPbSmp/ycqfVzQEAwFYIPxZocHyF57Jyp+QUssUFAACeRPixQHCgv0SGVOzvxdAXAAA2Cj8zZsyQ5ORkiYqKMkdKSoosXLjwlO95+umnpV27dhIaGiotWrSQP//5z1JYWCj1TXRokLllc1MAADyrovvBIgkJCTJ16lRp27atOJ1OmTVrlgwbNkzWrFkjnTp1Oun82bNnywMPPCCvvvqq9OnTR7Zs2SK33nqr+Pn5yVNPPSX1SUxYkOw5eozwAwCAncLP0KFDqzyePHmy6Q36/vvvqw0/3377rVx88cVy8803m8etWrWS4cOHy/Lly6Xe9vwUEH4AALBlzU9ZWZnMmTNH8vPzzfBXdbS3Z/Xq1bJixQrzePv27fLxxx/L4MGDa/zcoqIiycnJqXJ4A4a9AACwYc+PSktLM2FH63YiIiJk/vz50rFjx2rP1R6fQ4cOySWXXGKGyUpLS2X06NHy4IMP1vj5U6ZMkUcffVS8DeEHAACb9vxo8XJqaqoZuhozZoyMHDlSNmzYUO25S5Yskccff1yef/55+eGHH2TevHny0UcfyaRJk2r8/HHjxkl2drb72L17t3iDKMIPAAD27PkJDg6WpKQkc79nz56ycuVKmT59usycOfOkcx9++GH5/e9/L3fccYd53KVLFzNMdtddd8n48ePF3//kLOdwOMzhbej5AQDApj0/JyovLzd1OtUpKCg4KeAEBASYWx0Gq09c4SeLgmcAAOzT86NDUoMGDZLExETJzc01U9l1aGvRokXm9REjRkh8fLyp23HNDtMp7d27d5eLLrpItm3bZnqD9HlXCKpv4SeHnh8AAOwTfjIzM03AycjIkOjoaLPgoQafAQMGmNfT09Or9PQ89NBDZk0fvd27d680btzYBB+dIl/fxIRWbHHBsBcAAJ7l56xv40XnSKe6a9DS4mddVdoqaXuyZei/l0nTqBD5/sF+lrUDAAC7/f32upofu3DX/Bxjby8AADyJ8GNx+CksKZei0jKrmwMAgG0Qfiyiu7oH+PuZ+4fz6P0BAMBTCD8W8ff3M/U+KiP7mNXNAQDANgg/FoqPCTW3+7IKrW4KAAC2QfixUPOYip6ffVn0/AAA4CmEHws1d/f8EH4AAPAUwo8XhJ+9DHsBAOAxhB+vqPmh5wcAAE8h/HjDsBezvQAA8BjCj4WaHS941p3d84tKrW4OAAC2QPixUFRIkEQ6KvaWZa0fAAA8g/BjMYqeAQDwLMKPxVjrBwAAzyL8WIy1fgAA8CzCj9cMexF+AADwBMKPl6z1k0HNDwAAHkH4sRhr/QAA4FmEHy8peNaen/Jyp9XNAQDA5xF+LNYkKkT8/ESKy8rlUH6R1c0BAMDnEX4sFhTgL3GRDnOfuh8AAOoe4ccLNIs+XvRM3Q8AAHWO8ONVCx3S8wMAQF0j/HgBen4AAPAcwo8XaBZ9vOcnm54fAADqGuHHi9b6yWCVZwAA6hzhx4t6fjLo+QEAoM4RfrxkrR91MLeIhQ4BAKhjhB8v0Pj4Oj+l5U45UlBsdXMAAPBphB8vWeiwYXiwuZ+ZwyrPAADUJcKPl/X+ZOZS9wMAQF0i/HiJuON1P/T8AABQtwg/XsK1vxc9PwAA1C3Cj5doEuUKP/T8AABQlwg/XiIukmEvAAA8gfDjJRj2AgDAMwg/XqLZ8S0udh9liwsAAOoS4cdLtG4c7l7lOaewxOrmAADgswg/XiIqJMi91s/2g/lWNwcAAJ9F+PEibY73/vyUmWd1UwAA8FmWhp8ZM2ZIcnKyREVFmSMlJUUWLlxY4/mXX365+Pn5nXQMGTJEfEGbxhHm9qeDhB8AAOpKoFgoISFBpk6dKm3bthWn0ymzZs2SYcOGyZo1a6RTp04nnT9v3jwpLv5548/Dhw9L165d5YYbbhBfQPgBAMDHw8/QoUOrPJ48ebLpDfr++++rDT+xsbFVHs+ZM0fCwsJ8Jvy0bxppbtekZ5kwqL1aAADAh8JPZWVlZfLee+9Jfn6+Gf46E6+88orcdNNNEh5eUStTnaKiInO45OTkiLfq0bKBhAYFmFWeN+3PlQ7NoqxuEgAAPsfygue0tDSJiIgQh8Mho0ePlvnz50vHjh1P+74VK1bIunXr5I477jjleVOmTJHo6Gj30aJFC/FWIUEBktKmobm/dMtBq5sDAIBPsjz8tGvXTlJTU2X58uUyZswYGTlypGzYsOGMen26dOkivXr1OuV548aNk+zsbPexe/du8WaXtW1kbr/96bDVTQEAwCdZPuwVHBwsSUlJ5n7Pnj1l5cqVMn36dJk5c2aN79GhMa33mThx4mk/X3uU9KgvuraIMbebMrx3eA4AgPrM8p6fE5WXl1ep0amO1gbpOb/73e/E15zfJFK0zlnrfg7nsckpAAA+FX50SOqrr76SnTt3mtoffbxkyRK55ZZbzOsjRowwz1U35HXNNddIw4YV9TG+JNwRKC1jw8x9LXoGAAA+NOyVmZlpAk5GRoYpRtYFDxctWiQDBgwwr6enp4u/f9V8tnnzZlm2bJl8+umn4qt0ltfOwwWyMSNHLk6qqAECAAA+EH60B+dUtBeougJpXQPHl7VvGiUL1+2n5wcAADvU/ECkfbOKxQ437afoGQCA2kb48UIdmlYsbrjlQJ6UlpVb3RwAAHwK4ccLJTQIlQhHoBSXlsuOQ/lWNwcAAJ9C+PFC/v5+0u74Pl8bqfsBAKBWEX68fJNTnfEFAABqD+HHS3WOjza3a3dnWd0UAAB8CuHHS/VIbGBuU3dnUfQMAEAtIvx4qbZxERLpCJSC4jLW+wEAoBYRfry46LlbYsUmpz+kH7W6OQAA+AzCjxfrfnyH9x/3ZFvdFAAAfAbhx4t1Ol70vH4fM74AAKgthB8v1ql5xUrPWw/kSlFpmdXNAQDAvuFn9+7dsmfPHvfjFStWyH333ScvvvhibbbN9uJjQiU6NEhKy52yZX+e1c0BAMC+4efmm2+WL7/80tzfv3+/DBgwwASg8ePHy8SJE2u7jbbl5+fn7v1Zt4+6HwAALAs/69atk169epn77777rnTu3Fm+/fZbeeutt+T111+vlYahQvfjM76+337Y6qYAAGDf8FNSUiIOh8Pc//zzz+Xqq68299u3by8ZGRm120Kbu6xtY3P71ZaDUlbutLo5AADYM/x06tRJXnjhBfn666/ls88+k6uuuso8v2/fPmnYsGFtt9HWerRsYBY7PFpQIml7GfoCAMCS8PPPf/5TZs6cKZdffrkMHz5cunbtap7/4IMP3MNhqB1BAf5ycVIjc3/p5oNWNwcAgHov8GzepKHn0KFDkpOTIw0aVOxBpe666y4JCwurzfZBRPq2ayyfrN8vS7dkyr3921rdHAAA7Nfzc+zYMSkqKnIHn127dsnTTz8tmzdvlri4uNpuo+31Pb+xe5PTrIJiq5sDAID9ws+wYcPkP//5j7mflZUlF110kTz55JNyzTXXyIwZM2q7jbbXPCZUzm8SIVrv/MLS7VY3BwAA+4WfH374QS699FJzf+7cudKkSRPT+6OB6JlnnqntNkJExv4qydy+sPQnM/MLAAB4MPwUFBRIZGSkuf/pp5/KddddJ/7+/tK7d28TglD7hnWLl5EpLc39xz7aIKVl5VY3CQAA+4SfpKQkWbBggdnmYtGiRXLllVea5zMzMyUqqmJFYtS++we0k5iwINlyIE+WbTtkdXMAALBP+JkwYYL89a9/lVatWpmp7SkpKe5eoO7du9d2G3FcdFiQDO7SzNxfvDHT6uYAAGCf8POb3/xG0tPTZdWqVabnx6Vfv37yr3/9qzbbhxP071Axm27xxgPidLLiMwAAHlnnRzVt2tQcrt3dExISWODQA/q0aSQhQf6yL7tQNu3PlQ7NGGYEAKDOe37Ky8vN7u3R0dHSsmVLc8TExMikSZPMa6g7IUEBcsnxFZ+19wcAAHgg/IwfP17+/e9/y9SpU2XNmjXmePzxx+XZZ5+Vhx9++Gw+Er/AFe2bmNvPqfsBAMAzw16zZs2Sl19+2b2bu0pOTpb4+Hi5++67ZfLkyWfzsThDV7SvqPtZuydLDucVScMIh9VNAgDAt3t+jhw5Iu3btz/peX1OX0PdahodIm3jIkTrnVftOmp1cwAA8P3wo7u467DXifQ57QFC3bvwvFhzu2onYRMAgDof9nriiSdkyJAh8vnnn7vX+Pnuu+/Moocff/zx2XwkfqELWzWQ2cvTZcVOen4AAKjznp++ffvKli1b5NprrzUbm+qhW1ysX79e3njjjbP5SPxCF7aq6PlZvzdb9mUds7o5AADUG37OWlwpb+3atdKjRw8pKysTb5WTk2Om6GdnZ9f7rThunPmdLN9xRK7u2lyeGc7K2gAA35VTi3+/z6rnB97h4V93FD8/kQ/W7pNtmblWNwcAgHqB8FOPdY6PlgEdKtb8mbl0u9XNAQCgXiD81HOjL29jbuet2Ss/HcyzujkAAPjWbC8taj4VLXyGZ/VIbGA2O9XVnp/4ZJPM/P0FVjcJAADfCT9aaHS610eMGHGubcIv9Ler2pvwo8ehvCJpxIrPAADUTvh57bXXpDbNmDHDHDt37jSPO3XqJBMmTJBBgwadsndJ9xabN2+eWU1aN1V9+umnZfDgwWJX5zeJlOSEaPlxT7YsTMuQ36e0srpJAAB4LUtrfhISEszmqKtXr5ZVq1bJFVdcIcOGDTPrBVWnuLhYBgwYYMLS3LlzZfPmzfLSSy+ZPcXsTqe7u2p/AACAh9b5qQ2xsbEybdo0GTVq1EmvvfDCC+a1TZs2SVBQkNh9nZ/KMnML5ZKpX0pxWbnMu7uPqQUCAMBX5PjiOj+6MOKcOXMkPz/fvWXGiT744APz2tixY6VJkybSuXNnefzxx716UUVPiYsMkau7VfT+PLN4q3hZpgUAwGtYHn7S0tIkIiJCHA6HjB49WubPny8dO3as9tzt27eb4S4NO7qH2MMPPyxPPvmkPPbYYzV+flFRkUmLlQ9fNbpvawkK8JMlmw/Ke6v3WN0cAAC8kuXhp127dpKamirLly+XMWPGyMiRI2XDhg3VnlteXi5xcXHy4osvSs+ePeXGG280xc86HFaTKVOmmG4y19GiRQvxVUlxkXL/gHbm/uMfb5Qj+cVWNwkAAK9jefgJDg6WpKQkE2Y0qHTt2lWmT59e7bnNmjWT888/XwICAtzPdejQQfbv32+Koaszbtw4Mz7oOnTneV9256XnSfumkZJVUCLPfbnN6uYAAOB1LA8/1fXu6FBVdS6++GLZtm2bOcdFd5fXUKQhqjo6nKaFUZUPXxYY4C9/vbKi90f3/Corp/YHAACvCT/aK/PVV1+Zqeta+6OPlyxZIrfccot5XRdM1OdcdFhM1/a59957Tej56KOPTMGzFkDjZ5ed31iiQ4PkYG6RrNhxxOrmAADgVX7RIoe1LTMz0wScjIwMU4+TnJwsixYtMmv5qPT0dPH3/zmfab2Ovv7nP//ZnKvr+2gQ+vvf/27hd+F9ggP9ZWCnJvLuqj0y74c9ktKmodVNAgDAa3jdOj91zVfX+TnR6l1H5PoZ34kj0F++H9dPGoRXPywIAEB94JPr/KB26SKHneOjpKi0XGavSLe6OQAAeA3Cj4/y8/OTUZecZ+6/smyHFBSXWt0kAAC8AuHHhw1Nbi4tG4aZ9X5mL6f3BwAARfjx8Wnvd1/extyf+dV2KSxhGxAAAAg/Pu7a7gkSHxNqpr3PZcsLAAAIP3aY9u6q/Xn9251seAoAsD3Cjw3ccEGCRDgCZVtmnixaf8Dq5gAAYCnCjw1EhgTJyD4tzf1JH26Q3MISq5sEAIBlCD82cc+v2pran71Zx+Tut35gzy8AgG0RfmwiNDhAZvyuh4QGBcjXWw/J+6l7rW4SAACWIPzYSHJCjPypX1tz/5nFW6W0rNzqJgEA4HGEH5sZkdJSYsODZefhAlmQus/q5gAA4HGEH5sJdwTKXZe1Nvef/WKrFJfS+wMAsBfCj017fxpFOGTX4QJ5edl2q5sDAIBHEX5sKCw4UB4c3N7cn7Zoszz6v/VSQv0PAMAmCD82dW33eLnlokTRBZ9f+2anjHx1hWQVFFvdLAAA6hzhx6b8/Pxk8rVd5KURF0h4cIB8+9NhGTVrlZSz/g8AwMcRfmxuQMcmMndMHxOAVu86Ku+vZf0fAIBvI/xAOjSLktF925j7f5v7oyxYQwACAPguwg+Mu/q2lkGdm0pJmdMEoDXpR61uEgAAdYLwA8MRGCDP39LDDIMVl5XLzS8tl/Hz0yT9cIHVTQMAoFYRflClCPqp33aVlNYN5VhJmby1PF2GPPu1pO7OsrppAADUGsIPqogMCZL/jOpleoG6toiR3MJS+dvctawDBADwGYQfnCQowF8Gd2kms2670OwDtuVAnryybIfVzQIAoFYQflCjmLBgGT+4g7n/9OdbZOehfKubBADAOSP84JSu6xEvvVvHSmFJudw+a6XkFJZY3SQAAM4J4QenLYJ+5qbu0jw6RLYfzJd/fbbF6iYBAHBOCD84rbioEJl6fbK5r/uA3TdnjRSVllndLAAAzgrhB2fksvMby+97tzT3F6Tuk+mfb7W6SQAAnBXCD87YpGs6y79u7GruP7/kJ5m6cBMboQIA6h3CD36Ra7snyN2XV+wD9sLSn+SPDIEBAOoZwg9+sb9d1V6evrGbBAX4yUc/ZsitrzILDABQfxB+cFau6R4vr93aS8KDA+S77Yflty98JwdyCq1uFgAAp0X4wVm7pG0jeecPKdIowiGb9ufKr59dJt9sO2R1swAAOCXCD85J5/homX93Hzm/SYQczC2S372yXCZ9uEHyikqtbhoAANUi/OCctYgNk/fHXiLDe7UQp1PMPmD9nlwiH/64T5z6BAAAXoTwg1oRGhwgU65Lltduu1BaNgyTAzlFcs/sNTL1k01WNw0AgCoIP6hVv2oXJ4vuu0z+dEWSeTxz6XZ56rMt9AABALwG4Qe1LiQoQO6/sp2MG9TePH5m8Va58z+r5VBekdVNAwCA8IO684e+bWTytZ0lOMBfPt94QPo/tdT0AhWXllvdNACAjRF+UKduuailvH/PxdK+aaRkFZSYXiCdEXYkv9jqpgEAbMrS8DNjxgxJTk6WqKgoc6SkpMjChQtrPP/1118XPz+/KkdISIhH24xfrkOzKPnwj5fI9Ju6SaQjUFbsOCKDp38tb36/i60xAAD2Cj8JCQkydepUWb16taxatUquuOIKGTZsmKxfv77G92hIysjIcB+7du3yaJtxdgID/GVYt3iZd3cfSYwNk/05hfLQgnVy5b++km9ZGBEA4EF+Ti+bhhMbGyvTpk2TUaNGVdvzc99990lWVtZZf35OTo5ER0dLdna2CVLwvMKSMpmzIt3sDJ+ZW1EEfdOFLeT/BraThhEOq5sHAPBCtfn322tqfsrKymTOnDmSn59vhr9qkpeXJy1btpQWLVqctpdIFRUVmQtW+YD1s8Fuvfg8WfyXvvK73onmuTkrd8uFkz+XB+ensTo0AKBOWR5+0tLSJCIiQhwOh4wePVrmz58vHTt2rPbcdu3ayauvvirvv/++vPnmm1JeXi59+vSRPXv21Pj5U6ZMMUnRdWhogneIDAmSx67pIu/c1VuSE6Kl3Ckye3m6pDy+WF7+erscK6YeCADgg8NexcXFkp6ebrqx5s6dKy+//LIsXbq0xgBUWUlJiXTo0EGGDx8ukyZNqrHnRw8X7fnRAMSwl/dZtvWQTHh/nWw/lG8eB/r7SffEGHlkaCezhxgAwL5yanHYy/Lwc6L+/ftLmzZtZObMmWd0/g033CCBgYHy9ttvn9H51Px4t/Jyp7y9Ml2eXbzNFEWrAH8/iY8JlcaRDkloECo390qUXufFmtl+AAB7yKnFv9+B4mV0KKtyT83p6oR02Gzw4MF13i54hr+/n1kbSAPOnqPHTE/Ql5sPSvqRAnOs3nVU3k/dJ52aR8kdl54nQ5Obm5lkAADUi/Azbtw4GTRokCQmJkpubq7Mnj1blixZIosWLTKvjxgxQuLj403djpo4caL07t1bkpKSzIwvnRWmU93vuOMOK78N1AHt1dHd4l+99ULZl10oe48ek4O5RbJs20GZ98NeWb8vR/78zlqZ9OFG6dOmoQzp0kwGdGxCEAIAeHf4yczMNAFH1+vRrixd8FCDz4ABA8zrWgvk7//zH7OjR4/KnXfeKfv375cGDRpIz5495dtvvz2j+iDU3xCkQ156qCHJzeT/BraX2ct3yavf7DQrRX/4Y4Y5mkaFyM0XJcrIPq0kOjTI6qYDALyU19X81DVqfnxHSVm5pO7OksUbM+W9Vbvl8PEtMzQoTRjaUQZ0aGKG0QAA9V+OLxc81zXCj2/SbTIWpu03G6dqbZBq1yRSftMzQa7rEc/iiQBQz+UQfs4e4ce35RaWyIwlP8l/vtvlXiwxONBfru8Rb4bLYsODrW4iAOAsEH7OAeHHHrILSuSDtXvl3VV7JG1vtnmuQViQ/PaCFnLDBQnSpnEEU+UBoB4h/JwDwo+96K/3yp1HZfz8NNmamed+vn3TSBn7qyQZ3KWZWUcIAODdCD/ngPBjT6Vl5Wa9oLdXpJuVpIvLys3zOgx2QcsGZtHEJlEhEhESKBedFythwV63BBYA2FoO4efsEX6QfaxEZn27U15ZtsPcP1GkI1C6JESbxRb7dYgzG7ECAKxF+DkHhB+4FJeWm3qglTuPyJr0oyYI7TpcIBnZFdtquIqlB3VuKjddmCi9W7OlBgBYhfBzDgg/OJWycqcJRIvW75e5q/eYVaVdWjUMk+t6JJiFFls3CicIAYAHEX7OAeEHZ0r/r/HjnmyZs3K3fJC6V/KLy9yvNQwPlpYNw6RrixgzPJYUF2FpWwHA1+UQfs4e4QdnI7+oVD5Zt1/mr9krK3YccRdMu4QGBcgFrRrIr5ObSY/EBiYM0TMEALWH8HMOCD+ojdWkdWPV3UcKzJ5iizcekPIT/l+kPUPxDULlmm7xZsNV3aQVAHD2CD/ngPCD2pZTWCIZWYXy+cYDJghtzMiVYyU/D5Gpni0bSHJCtHRoFiUdm0WZniFmkQHAmSP8nAPCD+paYUmZbN6fa2aQfZSWIat2HZUT/1+mCyu2jYuQlDYNzarTGooAADUj/JwDwg88bc/RAvn2p8OyKSNXNmbkyMb9OZJVUHV9Ie0J6poQI/07xEmfpEYSHRpkWXsBwBsRfs4B4QdW0//L7c8plNT0LPnfj/tk4br9VXqGtE5ad6Tv36GJ9G7dULolxkiEgxWnAdhbDuHn7BF+4G0O5RXJ2t1ZsnzHEfl8wwHZfii/yuu69ViXhBgZmtxMhnZtbrbhAAC7ySH8nD3CD7ydLqz49daDsnTLQVm186jszTrmfi3Q309+1T7OTKdvERsqvVrFShxhCIAN5BB+zh7hB/VNRvYx0yO0IHWfrN51tNpeId2c9cJWsWZT1gbhwZa1FQDqCuHnHBB+UJ9t2JcjX2w6INsy8+Sng/lmK44TxceEmllkOqX+6m7NpVGEw5K2AkBtIvycA8IPfIkOia3cccRszqorT2/NzKvyenCAv/RoGSMNwoKlWXSoRIYEmmn1uklrTBg9RADqD8LPOSD8wJfpzvSpu7Nk9c4j8tXWQ+Z+dXRGmfYMpbRuKH2SGpohs8gQptcD8F6En3NA+IGdpO3Jlk37c0woOpRXLEfyi+SH9CwzbHbiooud46OlT5uGpm5IgxGF1AC8CeHnHBB+AJHMnEL5bvth+X77YbMA467DBSed07pRuNm5Xvcoa90owmzR0bF5lAQF+FvSZgD2lkP4OXuEH6D62qHvfjpsjjW7j8rOQ/knbdaqQoL8Jfn47DINQzrlntllADyB8HMOCD/A6R3NLzYzyTQU6fYculmrTrPX4bMTtWkcboLQBa1iTe1Qq4Zh4qdFRQBQiwg/54DwA5yd8nKnbD+UZ0KQHrph6/aDVVejVo0igk0Y0iCkgUjrh4IDGSoDcG4IP+eA8APUniP5xWb3eg1Cq3celdQ9WVJcWn7SeQkNQuWi8xqaKfa6X1mL2DBL2gug/iL8nAPCD1B3ikrLZN3ebFm586is2qnrD1U/VNa6cbgM6NhEmkWFSPOYUGkTFyGJsWEUUwOoEeHnHBB+AM8OlR0pKJaNGTmmmFpnl/24J1tKq6mmDgrwkzaNI9w9Q82iQ6RpdIi0aBAmjSNZpRqwuxzCz9kj/ADWyi0skS83H5SvthyUguJS2X3kmPx0ME8KistqfE/D8GCzMrVOtdcaok7No6R14wizPhEAe8gh/Jw9wg/gnT1EGTmFkpqeZWqI9P6B7ELJMMexaqfdhwcHmIUZwx2B0jjCIeVOpzSJCpGuLWKkc3yUNAx3UGgN+JAcws/ZI/wA9cux4jLZciDXDJ1tyMiR9ftyzP1T9RRVDki6DpH2HMWawyENI4LdPUlae6SBiVojwF5/vwNrrVUAUAdCgwNMb44eLmXlTncgKiotl4O5RWYIbNfhfFmTnmWG0bS3KL+4TPKLda2iYzV+vi5J1CjCUVFjFFVRZ6RHo3CH+dq6Kez5TSLY7gPwIYQfAPWOBh3tudGjpmG03MJSU2yt0/ErjiI5kl9ibg/kFMmPe7LMIo4lZU4TnvT4UbJr/Jo6G03rjVo2qrht1TDcFGJrcGJ4DahfCD8AfI6/v59EhwWZ47xG4aedjbY/u9AcWl+0P0fva1AqMkNrB/OKZMehfEk/UmCO6jQIC5KGEQ5Tf6TT97XnSIfZ4iIdktAgTKJDg0wPU2CAn0SFBBGYAIsRfgDYOiRpENFDi6drklNYYoqxdThNg5BO1z+QUyiH8opMz9HRghJzqLVn+LU1EGn9kX5tLdjWlbFNW473JukQXIvY0OPBiVltQG2i4BkAzpL2HGUdK5HM3EIztJZXWGpmqGnv0ZG8YnOre6NpD5LWKemhiz5Wt85RTbRXyWwe6xRJioswU/w1ELmOmLCf70eFBkmkI9CEOsDX5FDwDADW05BRMYss+BcFJg1A2mukQ2qH8orlUK7euo5iU3+kIUofV+5V2n5I91I7cOo2+YnEhAWb3iStSdJDg5EWb8eEBpm2ao9TdGhwxVCcv5/ERYaYYTgNWvQywQ4IPwDg4cCkPTl6tG0Sedpp/jrUll9UanqN1u7JNr1MGp5yjpWY26yCils9dOabdiq5irw3H8j9RW3TVbaDA/xNENIjJChAQgIDJCQ4QEKD/M2yALpPm4YprW+KcARKeHCghDkCJCxY7wdImHkuwIQt/SzCFLwR4QcAvJQGiMq1SH2SGp3y/MKSMhOKDucXV/QsHZ/FpjPf8otLJbug4jUNRlnHisVP/Mx7tAdKCyC0fqmkTJcHOP0aSmdCe5XCggNMHVPz6FBxBPqbXifdz00LxEtKy81aSzprT4vDCUqwRfiZMWOGOXbu3Gked+rUSSZMmCCDBg067XvnzJkjw4cPl2HDhsmCBQs80FoA8G6mpyYo4KzWJNIQdLSgWIpLy81RdPzQ57UHSsOTzojT5QE0TOUVlZoeKT20pqni0JBVZt6vtLYpp7DUHNsP6pBdzbRWybV0QKPI48XfEQ4TnnQRSp0pp7e67pIpFD++YKW+TmhCvQo/CQkJMnXqVGnbtq1o3fWsWbNMmFmzZo0JQjXRsPTXv/5VLr30Uo+2FwB8lYamZtGhtfJZJWXlJgxpaNKQlJlTsVVJcVnFgpS6pIDWNmnP0NbMPNl+ME9yi0rNUVHXdOa0N0lDkmvlbu1RqrjVxxXP6+ta66TDdYQleOVsr9jYWJk2bZqMGjWq2tfLysrksssuk9tvv12+/vprycrK+kU9P8z2AgDvor1LOivuYG7FcJ37yC2WYyVlUlpebobktEcpq0DPKZbD+UVSWFLRw/RLF8iMDAmsOBxBpm4pKNDPBKOkxhHSJi7CDMuFBgW4g5z2NjWLCRFHYMVzsIZPzvbSUPPee+9Jfn6+pKSk1HjexIkTJS4uzoQjDT+nU1RUZI7KFw8A4F29TklxkZIU98vep8Nsh3W2XF6RudVapkP5FfcP63P5elsRlPRWh+G0cFyLxPUQqXnbk5oKwjUAaWDS8KS9SNp27VXSwBQS5G8KxB1BPxeL632H67nAikJws0TB8aUJNIzB8ywPP2lpaSbsFBYWSkREhMyfP186duxY7bnLli2TV155RVJTU8/486dMmSKPPvpoLbYYAOANdIZZWGygtIgNO+25OsihvUhar5RbqLPjKm7ziyp6lnQ4TmfWbT2QZ0KTBistCNdw4uplqigIr6h32l9L/x2tAUqDVEWgCnL3Srkeu4JWxVHxOCIkUKLMORXnM5RXD4e9iouLJT093XRjzZ07V15++WVZunTpSQEoNzdXkpOT5fnnn3cXRN96662nHfaqruenRYsWDHsBAM6I/pnMOVZqwlNFgCoxC1rqfa1tyswtMnVNFUXiZSYoVb4tKimXwuO3er7OyNP6ptqiuSc8OND0KoW4epoC/U0Pk1nXKTxYQoMDK3qmgnTZggD3fT3Xdb/i8D/+euWeK+9YsqA2h70sDz8n6t+/v7Rp00ZmzpxZ5Xnt7enevbsEBPw85lpeXjHe6+/vL5s3bzbvOx1qfgAAVtNaIg1B2otU0RtV0aNkglWV50oqbo8/zj3hHB3Gq2t+fhWF5QF+fhJkFsMMNgtixoY7JCpUQ1VF2HIFKb1tHhMiw7rF12o7fLLmp3KgqdxT49K+fXszRFbZQw89ZHqEpk+fbnpzAACoD3TavpmZFuE468/QvgvtXcotqhi+09l1rh4mvdV1nSpWCS+WY8UVz2lxeZG+frwXS2/1M9zvcz1XWu4OVtpF4i4uLy4z9VI7TtO27okxtR5+apOl4WfcuHFmCCsxMdGEmNmzZ8uSJUtk0aJF5vURI0ZIfHy8qdsJCQmRzp07V3l/TEyMuT3xeQAAfJ0ORelQlx5y6sXCz7p3qtAVjkrKpNxZMeNOt1txrSKuvVCu9aAq3yaeQR2WbcNPZmamCTgZGRmmK0trejT4DBgwwLyutUA6pAUAADzfOxUU4C+Rv3zNTK/ndTU/dY2aHwAA7P33m24VAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK4QfAABgK5aGnxkzZkhycrJERUWZIyUlRRYuXFjj+fPmzZMLLrhAYmJiJDw8XLp16yZvvPGGR9sMAADqt0Arv3hCQoJMnTpV2rZtK06nU2bNmiXDhg2TNWvWSKdOnU46PzY2VsaPHy/t27eX4OBg+fDDD+W2226TuLg4GThwoCXfAwAAqF/8nJo6vIgGnGnTpsmoUaPO6PwePXrIkCFDZNKkSWd0fk5OjkRHR0t2drbpbQIAAN6vNv9+e03NT1lZmcyZM0fy8/PN8NfpaGZbvHixbN68WS677DKPtBEAANR/lg57qbS0NBN2CgsLJSIiQubPny8dO3as8XxNfPHx8VJUVCQBAQHy/PPPy4ABA2o8X8/To3JyBAAA9mV5+GnXrp2kpqaaUDN37lwZOXKkLF26tMYAFBkZac7Py8szPT/333+/tG7dWi6//PJqz58yZYo8+uijdfxdAACA+sLran769+8vbdq0kZkzZ57R+XfccYfs3r1bFi1adMY9Py1atKDmBwAAm9b8WN7zc6Ly8vIqYeVcz3c4HOYAAACwPPyMGzdOBg0aJImJiZKbmyuzZ8+WJUuWuHtxRowYYep7dOhK6a2u86M9Qxp4Pv74Y7POj64XBAAA4PXhJzMz0wScjIwM05WlCx5q8HEVMKenp4u//88T0nQm2N133y179uyR0NBQs97Pm2++KTfeeKOF3wUAAKhPvK7mp66xzg8AAPWPT67zAwAA4AmEHwAAYCuEHwAAYCteN9W9rrlKnFjpGQCA+sP1d7s2SpVtF350Sr3ShQ4BAED9+zuuhc/nwnazvXRRxH379pltMvz8/Grtc10rR+tq08wi8xyuuzW47tbguluD6+4d113jigaf5s2bV1kG52zYrudHL1hCQkKdfb7+gPg/h+dx3a3BdbcG190aXHfrr/u59vi4UPAMAABshfADAABshfBTS3Tz1EceeYRNVD2M624Nrrs1uO7W4Lr73nW3XcEzAACwN3p+AACArRB+AACArRB+AACArRB+AACArRB+asFzzz0nrVq1kpCQELnoootkxYoVVjepXvvqq69k6NChZhVPXYV7wYIFVV7XGv0JEyZIs2bNJDQ0VPr37y9bt26tcs6RI0fklltuMQtjxcTEyKhRoyQvL8/D30n9MmXKFLnwwgvN6udxcXFyzTXXyObNm6ucU1hYKGPHjpWGDRtKRESEXH/99XLgwIEq56Snp8uQIUMkLCzMfM7//d//SWlpqYe/m/pjxowZkpyc7F7ILSUlRRYuXOh+nWte96ZOnWr+rbnvvvvcz3Hd68Y//vEPc60rH+3bt/f8ddfZXjh7c+bMcQYHBztfffVV5/r165133nmnMyYmxnngwAGrm1Zvffzxx87x48c7582bpzMRnfPnz6/y+tSpU53R0dHOBQsWONeuXeu8+uqrneedd57z2LFj7nOuuuoqZ9euXZ3ff/+98+uvv3YmJSU5hw8fbsF3U38MHDjQ+dprrznXrVvnTE1NdQ4ePNiZmJjozMvLc58zevRoZ4sWLZyLFy92rlq1ytm7d29nnz593K+XlpY6O3fu7Ozfv79zzZo15mfZqFEj57hx4yz6rrzfBx984Pzoo4+cW7ZscW7evNn54IMPOoOCgszPQXHN69aKFSucrVq1ciYnJzvvvfde9/Nc97rxyCOPODt16uTMyMhwHwcPHvT4dSf8nKNevXo5x44d635cVlbmbN68uXPKlCmWtstXnBh+ysvLnU2bNnVOmzbN/VxWVpbT4XA43377bfN4w4YN5n0rV650n7Nw4UKnn5+fc+/evR7+DuqvzMxMcx2XLl3qvs76R/m9995zn7Nx40ZzznfffWce6z9E/v7+zv3797vPmTFjhjMqKspZVFRkwXdRPzVo0MD58ssvc83rWG5urrNt27bOzz77zNm3b193+OG612340f8wrY4nrzvDXueguLhYVq9ebYZdKu8dpo+/++47S9vmq3bs2CH79++vcs11rxcdbnRdc73Voa4LLrjAfY6erz+b5cuXW9Lu+ig7O9vcxsbGmlv9XS8pKaly7bW7OjExscq179KlizRp0sR9zsCBA80GhevXr/f491DflJWVyZw5cyQ/P98Mf3HN65YOr+jwSeXrq7judUvLFLSsoXXr1qY8QYexPH3dbbexaW06dOiQ+ceq8g9B6eNNmzZZ1i5fpsFHVXfNXa/prY4DVxYYGGj+iLvOwamVl5eb+oeLL75YOnfubJ7TaxccHGyC5amufXU/G9drqF5aWpoJO1rvoHUO8+fPl44dO0pqairXvI5oyPzhhx9k5cqVJ73G73rd0f9Qff3116Vdu3aSkZEhjz76qFx66aWybt06j153wg+Aav+LWP8xWrZsmdVNsQX9Q6BBR3vb5s6dKyNHjpSlS5da3SyftXv3brn33nvls88+MxNV4DmDBg1y39dCfw1DLVu2lHfffddMYPEUhr3OQaNGjSQgIOCkSnR93LRpU8va5ctc1/VU11xvMzMzq7yuMwF0Bhg/l9O755575MMPP5Qvv/xSEhIS3M/rtdOh3qysrFNe++p+Nq7XUD39r92kpCTp2bOnmXXXtWtXmT59Ote8jujwiv4b0aNHD9MrrIeGzWeeecbc154ErrtnaC/P+eefL9u2bfPo7zvh5xz/wdJ/rBYvXlxluEAfaxc2at95551nfsErX3Md69VaHtc111v9P4/+A+fyxRdfmJ+N/lcGqqf15Rp8dMhFr5de68r0dz0oKKjKtdep8DpeX/na6xBO5fCp/3WtU7h1GAdnRn9Xi4qKuOZ1pF+/fuaaaW+b69AaQa0/cd3nunuGLkHy008/maVLPPr7fk5l2zBT3XWm0euvv25mGd11111mqnvlSnT88hkYOoVRD/0Vfeqpp8z9Xbt2uae66zV+//33nT/++KNz2LBh1U517969u3P58uXOZcuWmRkdTHU/tTFjxpglBJYsWVJlGmpBQUGVaag6/f2LL74w01BTUlLMceI01CuvvNJMl//kk0+cjRs3ZvrvKTzwwANmRt2OHTvM77M+1pmJn376qXmda+4ZlWd7Ka573fjLX/5i/o3R3/dvvvnGTFnXqeo6u9ST153wUwueffZZ88PS9X506ruuLYOz9+WXX5rQc+IxcuRI93T3hx9+2NmkSRMTPPv162fWR6ns8OHDJuxERESYKZC33XabCVWoWXXXXA9d+8dFA+bdd99tpmKHhYU5r732WhOQKtu5c6dz0KBBztDQUPOPmv5jV1JSYsF3VD/cfvvtzpYtW5p/P/Qfcf19dgUfxTW3Jvxw3evGjTfe6GzWrJn5fY+PjzePt23b5vHr7qf/U7udWAAAAN6Lmh8AAGArhB8AAGArhB8AAGArhB8AAGArhB8AAGArhB8AAGArhB8AAGArhB8AAGArhB8AXuPgwYMyZswYSUxMFIfDYfZxGzhwoHzzzTfmdT8/P1mwYIHVzQRQzwVa3QAAcLn++uvNrs6zZs2S1q1bm92adZPDw4cPW900AD6Enh8AXiErK0u+/vpr+ec//ym/+tWvpGXLltKrVy8ZN26cXH311dKqVStz3rXXXmt6gFyP1fvvvy89evSQkJAQE5oeffRRKS0tdb+u58+YMUMGDRokoaGh5py5c+e6X9fApTva687S+hn6tadMmeLhKwDAUwg/ALxCRESEOXRYq6io6KTXV65caW5fe+01ycjIcD/WwDRixAi59957ZcOGDTJz5kx5/fXXZfLkyVXe//DDD5uepbVr18ott9wiN910k2zcuNG89swzz8gHH3wg7777rmzevFneeuutKuEKgG9hY1MAXuO///2v3HnnnXLs2DHTk9O3b18TUpKTk909OPPnz5drrrnG/Z7+/ftLv379TA+Ry5tvvil/+9vfZN++fe73jR492vT+uPTu3dt8jeeff17+9Kc/yfr16+Xzzz835wLwbfT8APAa2jOjgUV7Ya666ipZsmSJCSjak1MT7cmZOHGiu+dIDw1Q2jtUUFDgPi8lJaXK+/Sxq+fn1ltvldTUVGnXrp0JQp9++mkdfpcArEb4AeBVtOZmwIABZpjq22+/NcHkkUceqfH8vLw8U+Oj4cV1pKWlydatW81nnQkNWDt27JBJkyaZXqff/va38pvf/KYWvysA3oTwA8CrdezYUfLz8839oKAgKSsrOym4aJ1OUlLSSYe//8//xH3//fdV3qePO3To4H4cFRUlN954o7z00kvyzjvvmCG4I0eO1Pn3B8DzmOoOwCvodPYbbrhBbr/9dlPjExkZKatWrZInnnhChg0bZs7RImSd+n7xxRebdYAaNGggEyZMkF//+tdmbSDtrdHAo0Nh69atk8cee8z9+e+9955ccMEFcskll5iC5hUrVsgrr7xiXnvqqafMTK/u3bub9+u5usZQTEyMZdcDQB3SgmcAsFphYaHzgQcecPbo0cMZHR3tDAsLc7Zr18750EMPOQsKCsw5H3zwgTMpKckZGBjobNmypfu9n3zyibNPnz7O0NBQZ1RUlLNXr17OF1980f26/lP33HPPOQcMGOB0OBzOVq1aOd955x3363put27dnOHh4eb9/fr1c/7www8evgIAPIXZXgB8XnWzxADYFzU/AADAVgg/AADAVih4BuDzGN0HUBk9PwAAwFYIPwAAwFYIPwAAwFYIPwAAwFYIPwAAwFYIPwAAwFYIPwAAwFYIPwAAwFYIPwAAQOzk/wNMmkBgAcWiVwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 25000\n",
    "learning_rate = 3e-6\n",
    "cosine_decay=True\n",
    "max_lr = 3e-3\n",
    "min_lr = 3e-4\n",
    "warmup_steps = 100\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# Plotting Initialization\n",
    "step_plot_losses = []\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "if cosine_decay:\n",
    "    current_lr = max_lr\n",
    "else: \n",
    "    current_lr=learning_rate\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "\n",
    "    \n",
    "    if cosine_decay:\n",
    "        # Learning rate decay schedule\n",
    "        if step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            current_lr = max_lr * (step + 1) / warmup_steps\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            decay_ratio = (step - warmup_steps) / (max_iters - warmup_steps)\n",
    "            cosine_output = 0.5 * (1 + np.cos(np.pi * decay_ratio))\n",
    "            current_lr = min_lr + (max_lr - min_lr) * cosine_output\n",
    "        \n",
    "    # Get a mini-batch of data\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(len(chars))[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(current_lr)\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        \n",
    "        step_plot_losses.append(np.mean(plot_losses[-1-step:-1]))\n",
    "        \n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # Graph plot\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(step_plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if step_plot_losses[-1] < 4:\n",
    "            if loss_initial < 2:\n",
    "                ax.set_ylim(top=2) # cut off loses higher than 2\n",
    "            else:\n",
    "                ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(step_plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0aaa2-f5bf-4712-b0b4-8334375eb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"def\", 1.0, True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f22749-5eab-4fdd-87a1-a84b8bc01c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training, collect LoRA weights\n",
    "lora_weights = {}\n",
    "\n",
    "for transformer_idx, transformer in enumerate(model.transformers):\n",
    "    for head_idx, head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        \n",
    "        # Check if it's a LoRA layer and collect A and B matrices\n",
    "        if hasattr(head.W_query, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_A\"] = head.W_query.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_B\"] = head.W_query.lora_B.data\n",
    "            \n",
    "        if hasattr(head.W_value, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_A\"] = head.W_value.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_B\"] = head.W_value.lora_B.data\n",
    "\n",
    "# Save the LoRA weights\n",
    "np.savez_compressed('../models/lora_weights.npz', **lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7976a-0db7-456e-a8dc-b1d9ef6b558c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
