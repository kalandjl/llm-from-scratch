{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccc1b09-1546-415d-ab68-ceffcf08feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset: 'python-algorithms' git repo\n",
    "base_dataset = open(\"../data/requests.txt\").read()\n",
    "\n",
    "# Get original character set and create encode/decode functions\n",
    "chars = sorted(list(set(base_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac1175c-36cf-4ec6-a2b9-dfaab6c95f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string-to-integer mapping\n",
    "stoi = {char: i for i, char in enumerate(chars)}\n",
    "# integer-to-string mapping  \n",
    "itos = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Filter new text to only include original characters\n",
    "def filter_text(text, allowed_chars):\n",
    "    return ''.join(char for char in text if char in allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3583e428-f657-43c5-a28c-2f90566061f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter new dataset\n",
    "text = open(\"../data/python_fundamentals.txt\").read()\n",
    "filtered_text = filter_text(text, set(chars))  # Convert to set for faster lookup\n",
    "\n",
    "# Use filtered text for training\n",
    "data = np.array(encode(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e35ffc0-aa10-4aad-9230-aa573b6b2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742d9899-4c28-4870-8db2-59dfccf20402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2167158-a556-41c1-aace-0a1c1f27cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class allows for the requires grad variable \n",
    "class Parameter:\n",
    "    def __init__(self, weights, requires_grad=True):\n",
    "        \n",
    "        self.data = weights\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = np.zeros_like(weights)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Parameter with data shape {self.data.shape}\"\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        self.data -= other \n",
    "\n",
    "    def zerograds(self):\n",
    "        if self.requires_grad:\n",
    "             self.grad = np.zeros_like(self.data)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a3ef82-0717-4202-ae38-468f1f21e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            queries = self.W_query.forward(x)\n",
    "        else:\n",
    "            queries = x @ self.W_query.data\n",
    "\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            keys = self.W_key.forward(x)\n",
    "        else:\n",
    "            keys = x @ self.W_key.data\n",
    "\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            values = self.W_value.forward(x)\n",
    "        else:\n",
    "            values = x @ self.W_value.data\n",
    "\n",
    "            \n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            self.W_query.lora_A.zerograds()\n",
    "            self.W_query.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_query.zerograds()\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            self.W_key.lora_A.zerograds()\n",
    "            self.W_key.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_key.zerograds()\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            self.W_value.lora_A.zerograds()\n",
    "            self.W_value.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_value.zerograds()\n",
    "\n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        if hasattr(self.W_query, 'backward'):\n",
    "            d_x_from_queries = self.W_query.backward(d_queries, self.cache['x'])\n",
    "        else:\n",
    "            W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query.data, self.cache['x'])\n",
    "\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.grad = W_query_grad\n",
    "        if hasattr(self.W_key, 'backward'):\n",
    "            d_x_from_keys = self.W_key.backward(d_keys, self.cache['x'])\n",
    "        else:\n",
    "            W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key.data, self.cache['x'])\n",
    "\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.grad = W_key_grad\n",
    "        if hasattr(self.W_value, 'backward'):\n",
    "            d_x_from_values = self.W_value.backward(d_values, self.cache['x'])\n",
    "        else:\n",
    "            W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value.data, self.cache['x'])\n",
    "            \n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.grad = W_value_grad\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        if hasattr(self.W_query, 'optimizer'):\n",
    "            self.W_query.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.data -= (self.W_query.grad * learning_rate)\n",
    "        if hasattr(self.W_key, 'optimizer'):\n",
    "            self.W_key.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.data -= (self.W_key.grad * learning_rate)\n",
    "        if hasattr(self.W_value, 'optimizer'):\n",
    "            self.W_value.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.data -= (self.W_value.grad * learning_rate)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2648dbe3-8102-4d1e-8b89-46485bc1835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_k = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_v = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = Parameter(np.random.randn(n_embd, n_embd) * 0.02)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output.data\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output.zerograds()\n",
    "        \n",
    "        W_output_grad, d_concat = self.linear_backward(d_output, self.W_output.data, self.cache['concat_output'])\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.grad = W_output_grad\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.data -= (self.W_output.grad * learning_rate)\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f90b01-cc96-4223-88d4-468b03d5c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = Parameter(np.ones((n_embd,)))\n",
    "        self.beta = Parameter(np.zeros((n_embd,)))\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma.data\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma.data + self.beta.data\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.beta.zerograds()\n",
    "        self.gamma.zerograds()\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.grad = np.sum(d_output, axis=(0,1))\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.data -= (self.gamma.grad * learning_rate)\n",
    "\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.data -= (self.beta.grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5b63b8-7988-4f57-9f3f-1ba4f59a8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1.data\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.W1.zerograds()\n",
    "        self.W2.zerograds()\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2.data, self.cache['hidden_activated'])\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1.data, self.cache['norm_output_1'])\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        \n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.data -= (self.W1.grad * learning_rate)\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.data -= (self.W2.grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255ef7dc-845c-4909-ae58-b90bd28fd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        vocab_size = len(chars)\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = Parameter(np.random.randn(vocab_size, n_embd))\n",
    "        self.position_matrix = Parameter(np.random.randn(max_sequence_length, n_embd))\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = Parameter(np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd))\n",
    "            W2 = Parameter(np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor)))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix.data[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix.data[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.data.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "        \n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix.zerograds()\n",
    "        self.position_matrix.zerograds()\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix.data  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "        # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            # Perform reverse lookup on embedding array\n",
    "            np.add.at(self.embedding_matrix.grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.data -= (self.embedding_matrix.grad * learning_rate)\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.data -= (self.position_matrix.grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "\n",
    "        parameters = {\n",
    "            \"embedding_matrix\": self.embedding_matrix,\n",
    "            \"position_matrix\": self.position_matrix,\n",
    "        }\n",
    "\n",
    "        for idx, transformer in enumerate(self.transformers):\n",
    "            parameters[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "            parameters[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "            parameters[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "            parameters[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "            parameters[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "            \n",
    "            for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def freezeParams(self): \n",
    "\n",
    "        # Freezes model weights in prereration for LoRA training\n",
    "        self.embedding_matrix.requires_grad=False\n",
    "        self.position_matrix.requires_grad=False\n",
    "        \n",
    "        for x in self.transformers:\n",
    "\n",
    "            x.W1.requires_grad=False\n",
    "            x.W2.requires_grad=False\n",
    "            x.layer_norm1.gamma.requires_grad=False\n",
    "            x.layer_norm1.beta.requires_grad=False\n",
    "            x.layer_norm2.gamma.requires_grad=False\n",
    "            x.layer_norm2.beta.requires_grad=False\n",
    "            x.multi_head_attention_block.W_output.requires_grad=False\n",
    "            \n",
    "            for i in x.multi_head_attention_block.heads:\n",
    "\n",
    "                i.W_key.requires_grad=False\n",
    "                i.W_query.requires_grad=False\n",
    "                i.W_value.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d20ed2-0ba8-44c0-8a24-05fbb8072f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a095dd-8bde-46e9-8f5b-26187189b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix\n",
      "position_matrix\n",
      "transform.0.W1\n",
      "transform.0.W2\n",
      "transform.0.layer_norm1.gamma\n",
      "transform.0.layer_norm1.beta\n",
      "transform.0.layer_norm2.gamma\n",
      "transform.0.layer_norm2.beta\n",
      "transform.0.multi_head_attention_block.W_output\n",
      "transform.0.multi_head_attention_block.heads.0.key\n",
      "transform.0.multi_head_attention_block.heads.0.query\n",
      "transform.0.multi_head_attention_block.heads.0.value\n",
      "transform.0.multi_head_attention_block.heads.1.key\n",
      "transform.0.multi_head_attention_block.heads.1.query\n",
      "transform.0.multi_head_attention_block.heads.1.value\n",
      "transform.0.multi_head_attention_block.heads.2.key\n",
      "transform.0.multi_head_attention_block.heads.2.query\n",
      "transform.0.multi_head_attention_block.heads.2.value\n",
      "transform.0.multi_head_attention_block.heads.3.key\n",
      "transform.0.multi_head_attention_block.heads.3.query\n",
      "transform.0.multi_head_attention_block.heads.3.value\n",
      "transform.0.multi_head_attention_block.heads.4.key\n",
      "transform.0.multi_head_attention_block.heads.4.query\n",
      "transform.0.multi_head_attention_block.heads.4.value\n",
      "transform.0.multi_head_attention_block.heads.5.key\n",
      "transform.0.multi_head_attention_block.heads.5.query\n",
      "transform.0.multi_head_attention_block.heads.5.value\n",
      "transform.0.multi_head_attention_block.heads.6.key\n",
      "transform.0.multi_head_attention_block.heads.6.query\n",
      "transform.0.multi_head_attention_block.heads.6.value\n",
      "transform.0.multi_head_attention_block.heads.7.key\n",
      "transform.0.multi_head_attention_block.heads.7.query\n",
      "transform.0.multi_head_attention_block.heads.7.value\n",
      "transform.1.W1\n",
      "transform.1.W2\n",
      "transform.1.layer_norm1.gamma\n",
      "transform.1.layer_norm1.beta\n",
      "transform.1.layer_norm2.gamma\n",
      "transform.1.layer_norm2.beta\n",
      "transform.1.multi_head_attention_block.W_output\n",
      "transform.1.multi_head_attention_block.heads.0.key\n",
      "transform.1.multi_head_attention_block.heads.0.query\n",
      "transform.1.multi_head_attention_block.heads.0.value\n",
      "transform.1.multi_head_attention_block.heads.1.key\n",
      "transform.1.multi_head_attention_block.heads.1.query\n",
      "transform.1.multi_head_attention_block.heads.1.value\n",
      "transform.1.multi_head_attention_block.heads.2.key\n",
      "transform.1.multi_head_attention_block.heads.2.query\n",
      "transform.1.multi_head_attention_block.heads.2.value\n",
      "transform.1.multi_head_attention_block.heads.3.key\n",
      "transform.1.multi_head_attention_block.heads.3.query\n",
      "transform.1.multi_head_attention_block.heads.3.value\n",
      "transform.1.multi_head_attention_block.heads.4.key\n",
      "transform.1.multi_head_attention_block.heads.4.query\n",
      "transform.1.multi_head_attention_block.heads.4.value\n",
      "transform.1.multi_head_attention_block.heads.5.key\n",
      "transform.1.multi_head_attention_block.heads.5.query\n",
      "transform.1.multi_head_attention_block.heads.5.value\n",
      "transform.1.multi_head_attention_block.heads.6.key\n",
      "transform.1.multi_head_attention_block.heads.6.query\n",
      "transform.1.multi_head_attention_block.heads.6.value\n",
      "transform.1.multi_head_attention_block.heads.7.key\n",
      "transform.1.multi_head_attention_block.heads.7.query\n",
      "transform.1.multi_head_attention_block.heads.7.value\n",
      "transform.2.W1\n",
      "transform.2.W2\n",
      "transform.2.layer_norm1.gamma\n",
      "transform.2.layer_norm1.beta\n",
      "transform.2.layer_norm2.gamma\n",
      "transform.2.layer_norm2.beta\n",
      "transform.2.multi_head_attention_block.W_output\n",
      "transform.2.multi_head_attention_block.heads.0.key\n",
      "transform.2.multi_head_attention_block.heads.0.query\n",
      "transform.2.multi_head_attention_block.heads.0.value\n",
      "transform.2.multi_head_attention_block.heads.1.key\n",
      "transform.2.multi_head_attention_block.heads.1.query\n",
      "transform.2.multi_head_attention_block.heads.1.value\n",
      "transform.2.multi_head_attention_block.heads.2.key\n",
      "transform.2.multi_head_attention_block.heads.2.query\n",
      "transform.2.multi_head_attention_block.heads.2.value\n",
      "transform.2.multi_head_attention_block.heads.3.key\n",
      "transform.2.multi_head_attention_block.heads.3.query\n",
      "transform.2.multi_head_attention_block.heads.3.value\n",
      "transform.2.multi_head_attention_block.heads.4.key\n",
      "transform.2.multi_head_attention_block.heads.4.query\n",
      "transform.2.multi_head_attention_block.heads.4.value\n",
      "transform.2.multi_head_attention_block.heads.5.key\n",
      "transform.2.multi_head_attention_block.heads.5.query\n",
      "transform.2.multi_head_attention_block.heads.5.value\n",
      "transform.2.multi_head_attention_block.heads.6.key\n",
      "transform.2.multi_head_attention_block.heads.6.query\n",
      "transform.2.multi_head_attention_block.heads.6.value\n",
      "transform.2.multi_head_attention_block.heads.7.key\n",
      "transform.2.multi_head_attention_block.heads.7.query\n",
      "transform.2.multi_head_attention_block.heads.7.value\n",
      "transform.3.W1\n",
      "transform.3.W2\n",
      "transform.3.layer_norm1.gamma\n",
      "transform.3.layer_norm1.beta\n",
      "transform.3.layer_norm2.gamma\n",
      "transform.3.layer_norm2.beta\n",
      "transform.3.multi_head_attention_block.W_output\n",
      "transform.3.multi_head_attention_block.heads.0.key\n",
      "transform.3.multi_head_attention_block.heads.0.query\n",
      "transform.3.multi_head_attention_block.heads.0.value\n",
      "transform.3.multi_head_attention_block.heads.1.key\n",
      "transform.3.multi_head_attention_block.heads.1.query\n",
      "transform.3.multi_head_attention_block.heads.1.value\n",
      "transform.3.multi_head_attention_block.heads.2.key\n",
      "transform.3.multi_head_attention_block.heads.2.query\n",
      "transform.3.multi_head_attention_block.heads.2.value\n",
      "transform.3.multi_head_attention_block.heads.3.key\n",
      "transform.3.multi_head_attention_block.heads.3.query\n",
      "transform.3.multi_head_attention_block.heads.3.value\n",
      "transform.3.multi_head_attention_block.heads.4.key\n",
      "transform.3.multi_head_attention_block.heads.4.query\n",
      "transform.3.multi_head_attention_block.heads.4.value\n",
      "transform.3.multi_head_attention_block.heads.5.key\n",
      "transform.3.multi_head_attention_block.heads.5.query\n",
      "transform.3.multi_head_attention_block.heads.5.value\n",
      "transform.3.multi_head_attention_block.heads.6.key\n",
      "transform.3.multi_head_attention_block.heads.6.query\n",
      "transform.3.multi_head_attention_block.heads.6.value\n",
      "transform.3.multi_head_attention_block.heads.7.key\n",
      "transform.3.multi_head_attention_block.heads.7.query\n",
      "transform.3.multi_head_attention_block.heads.7.value\n",
      "transform.4.W1\n",
      "transform.4.W2\n",
      "transform.4.layer_norm1.gamma\n",
      "transform.4.layer_norm1.beta\n",
      "transform.4.layer_norm2.gamma\n",
      "transform.4.layer_norm2.beta\n",
      "transform.4.multi_head_attention_block.W_output\n",
      "transform.4.multi_head_attention_block.heads.0.key\n",
      "transform.4.multi_head_attention_block.heads.0.query\n",
      "transform.4.multi_head_attention_block.heads.0.value\n",
      "transform.4.multi_head_attention_block.heads.1.key\n",
      "transform.4.multi_head_attention_block.heads.1.query\n",
      "transform.4.multi_head_attention_block.heads.1.value\n",
      "transform.4.multi_head_attention_block.heads.2.key\n",
      "transform.4.multi_head_attention_block.heads.2.query\n",
      "transform.4.multi_head_attention_block.heads.2.value\n",
      "transform.4.multi_head_attention_block.heads.3.key\n",
      "transform.4.multi_head_attention_block.heads.3.query\n",
      "transform.4.multi_head_attention_block.heads.3.value\n",
      "transform.4.multi_head_attention_block.heads.4.key\n",
      "transform.4.multi_head_attention_block.heads.4.query\n",
      "transform.4.multi_head_attention_block.heads.4.value\n",
      "transform.4.multi_head_attention_block.heads.5.key\n",
      "transform.4.multi_head_attention_block.heads.5.query\n",
      "transform.4.multi_head_attention_block.heads.5.value\n",
      "transform.4.multi_head_attention_block.heads.6.key\n",
      "transform.4.multi_head_attention_block.heads.6.query\n",
      "transform.4.multi_head_attention_block.heads.6.value\n",
      "transform.4.multi_head_attention_block.heads.7.key\n",
      "transform.4.multi_head_attention_block.heads.7.query\n",
      "transform.4.multi_head_attention_block.heads.7.value\n",
      "transform.5.W1\n",
      "transform.5.W2\n",
      "transform.5.layer_norm1.gamma\n",
      "transform.5.layer_norm1.beta\n",
      "transform.5.layer_norm2.gamma\n",
      "transform.5.layer_norm2.beta\n",
      "transform.5.multi_head_attention_block.W_output\n",
      "transform.5.multi_head_attention_block.heads.0.key\n",
      "transform.5.multi_head_attention_block.heads.0.query\n",
      "transform.5.multi_head_attention_block.heads.0.value\n",
      "transform.5.multi_head_attention_block.heads.1.key\n",
      "transform.5.multi_head_attention_block.heads.1.query\n",
      "transform.5.multi_head_attention_block.heads.1.value\n",
      "transform.5.multi_head_attention_block.heads.2.key\n",
      "transform.5.multi_head_attention_block.heads.2.query\n",
      "transform.5.multi_head_attention_block.heads.2.value\n",
      "transform.5.multi_head_attention_block.heads.3.key\n",
      "transform.5.multi_head_attention_block.heads.3.query\n",
      "transform.5.multi_head_attention_block.heads.3.value\n",
      "transform.5.multi_head_attention_block.heads.4.key\n",
      "transform.5.multi_head_attention_block.heads.4.query\n",
      "transform.5.multi_head_attention_block.heads.4.value\n",
      "transform.5.multi_head_attention_block.heads.5.key\n",
      "transform.5.multi_head_attention_block.heads.5.query\n",
      "transform.5.multi_head_attention_block.heads.5.value\n",
      "transform.5.multi_head_attention_block.heads.6.key\n",
      "transform.5.multi_head_attention_block.heads.6.query\n",
      "transform.5.multi_head_attention_block.heads.6.value\n",
      "transform.5.multi_head_attention_block.heads.7.key\n",
      "transform.5.multi_head_attention_block.heads.7.query\n",
      "transform.5.multi_head_attention_block.heads.7.value\n",
      "Loading saved model weights...\n",
      "Loaded main matrices\n",
      "Loading transformer 0...\n",
      "Loaded transformer 0\n",
      "Loading transformer 1...\n",
      "Loaded transformer 1\n",
      "Loading transformer 2...\n",
      "Loaded transformer 2\n",
      "Loading transformer 3...\n",
      "Loaded transformer 3\n",
      "Loading transformer 4...\n",
      "Loaded transformer 4\n",
      "Loading transformer 5...\n",
      "Loaded transformer 5\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved weights\n",
    "loaded_weights = np.load('../models/my_model.npz')\n",
    "\n",
    "for key in loaded_weights.keys():\n",
    "    print(key)\n",
    "print(\"Loading saved model weights...\")\n",
    "\n",
    "# Set the main model weights\n",
    "model.embedding_matrix = Parameter(loaded_weights[\"embedding_matrix\"])\n",
    "model.position_matrix = Parameter(loaded_weights[\"position_matrix\"])\n",
    "print(\"Loaded main matrices\")\n",
    "\n",
    "# Load transformer weights\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    print(f\"Loading transformer {idx}...\")\n",
    "    \n",
    "    # Load basic weights\n",
    "    transformer.W1 = Parameter(loaded_weights[f\"transform.{idx}.W1\"])\n",
    "    transformer.W2 = Parameter(loaded_weights[f\"transform.{idx}.W2\"])\n",
    "    transformer.layer_norm1.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.gamma\"])\n",
    "    transformer.layer_norm1.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.beta\"])\n",
    "    transformer.layer_norm2.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.gamma\"])\n",
    "    transformer.layer_norm2.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.beta\"])\n",
    "    transformer.multi_head_attention_block.W_output = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.W_output\"])\n",
    "    \n",
    "    # Load attention head weights (16 heads each)\n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        attention_head.W_key = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"])\n",
    "        attention_head.W_query = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"])\n",
    "        attention_head.W_value = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"])\n",
    "    \n",
    "    print(f\"Loaded transformer {idx}\")\n",
    "\n",
    "loaded_weights.close()\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95e997c1-5e97-4c33-b725-c1709ff67b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:21: RuntimeWarning: divide by zero encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:21: RuntimeWarning: overflow encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:21: RuntimeWarning: invalid value encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:26: RuntimeWarning: divide by zero encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:26: RuntimeWarning: overflow encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:26: RuntimeWarning: invalid value encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:31: RuntimeWarning: divide by zero encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:31: RuntimeWarning: overflow encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:31: RuntimeWarning: invalid value encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:39: RuntimeWarning: divide by zero encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:39: RuntimeWarning: overflow encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:39: RuntimeWarning: invalid value encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:57: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:57: RuntimeWarning: overflow encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:57: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3048600086.py:45: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3048600086.py:45: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3048600086.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:22: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:22: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:22: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:28: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:28: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:28: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7812277237019094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/2046029974.py:62: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/2046029974.py:62: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/2046029974.py:62: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_batch(data, 128, 34)\n",
    "\n",
    "\n",
    "# Calculate loss and probabilites\n",
    "logits = model.forward(x_batch)\n",
    "loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "print(loss_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1bb214-fbe0-4a80-a312-5cee61efd734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer frozen? True\n",
      "Attention layer 15 frozen? True\n"
     ]
    }
   ],
   "source": [
    "# This one loop will freeze every parameter in the entire model\n",
    "for key, param in model.parameters.items():\n",
    "    # \"Freeze\" the model's weight\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Embedding layer frozen?\", model.transformers[4].multi_head_attention_block.W_output.requires_grad== False)\n",
    "print(\"Attention layer 15 frozen?\", model.transformers[2].multi_head_attention_block.heads[0].W_key.requires_grad == False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979888d3-0703-4db6-a6db-ddd5c9b6c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "\n",
    "    def __init__(self, original_layer, rank, alpha):\n",
    "\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        # Get dimensions from the original layer\n",
    "        in_features, out_features = self.original_layer.data.shape\n",
    "\n",
    "\n",
    "        # Better initialization following the paper\n",
    "        self.lora_A = Parameter(np.random.randn(in_features, rank) * np.sqrt(1.0 / rank))\n",
    "        self.lora_B = Parameter(np.zeros((rank, out_features)))  # Start with zeros\n",
    "\n",
    "\n",
    "        # Scaling factor\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The origin linear layer calculation\n",
    "        original_output = x @ self.original_layer.data\n",
    "\n",
    "        # New update w/A & B matrices\n",
    "        lora_update = (x @ self.lora_A.data @ self.lora_B.data) * self.scaling\n",
    "\n",
    "        return original_output + lora_update\n",
    "    \n",
    "    def backward(self, d_output, x_input):\n",
    "\n",
    "        self.lora_A.zerograds()\n",
    "        self.lora_B.zerograds()\n",
    "        \n",
    "        # Gradient w.r.t input (what we return)\n",
    "        # d_x = d_output @ (W_original + A @ B)^T\n",
    "        # Since W_original is frozen, we only need gradients through A @ B\n",
    "        d_x_original = d_output @ self.original_layer.data.T\n",
    "        d_x_lora = d_output @ (self.lora_A.data @ self.lora_B.data).T * self.scaling\n",
    "        d_x = d_x_original + d_x_lora\n",
    "        \n",
    "        # Gradients for LoRA parameters\n",
    "        # Reshape inputs for matrix operations\n",
    "        x_reshaped = x_input.reshape(-1, x_input.shape[-1])  # (B*T, in_features)\n",
    "        d_output_reshaped = d_output.reshape(-1, d_output.shape[-1])  # (B*T, out_features)\n",
    "        \n",
    "        # Gradient w.r.t B: A^T @ x^T @ d_output * scaling\n",
    "        self.lora_B.grad = (self.lora_A.data.T @ x_reshaped.T) @ d_output_reshaped * self.scaling\n",
    "        \n",
    "        # Gradient w.r.t A: x^T @ d_output @ B^T * scaling  \n",
    "        self.lora_A.grad = x_reshaped.T @ (d_output_reshaped @ self.lora_B.data.T) * self.scaling\n",
    "        \n",
    "        # Don't compute gradients for original_layer since it's frozen\n",
    "        return d_x        \n",
    "    \n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.lora_A.requires_grad:\n",
    "            self.lora_A.data -= (self.lora_A.grad * learning_rate)\n",
    "\n",
    "        if self.lora_B.requires_grad:\n",
    "            self.lora_B.data -= (self.lora_B.grad * learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a627b5-da56-4ec0-afd7-c019791472af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LoRA layers in model's query and value attention matrices\n",
    "for transformer in model.transformers:\n",
    "    for head in transformer.multi_head_attention_block.heads:\n",
    "\n",
    "\n",
    "        # Rank and alpha variables for LoRA layers\n",
    "        r = 8\n",
    "        a = 1\n",
    "\n",
    "        # Wrap query and value weights with LoRA\n",
    "        head.W_query = LoRALayer(head.W_query, r, a)\n",
    "        head.W_value = LoRALayer(head.W_value, r, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746d5489-e771-47a9-a667-7b2c014f607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature, heatMap, max_length):\n",
    "    print(f\"Generating with prompt: '{prompt}', temp: {temperature}, length: {max_length}\")\n",
    "    \n",
    "    if not prompt:\n",
    "        prompt = \"\\n\" # Default prompt\n",
    "\n",
    "    # Set the model's temperature for this specific generation\n",
    "    model.temperature = temperature\n",
    "    \n",
    "    # Encode the prompt and generate\n",
    "    char_indices = encode(prompt)\n",
    "    for _ in range(int(max_length)):\n",
    "        # Important: only feed the last block_size tokens as context\n",
    "        context = char_indices[-1000:]\n",
    "        next_char_index = model.pred(context)\n",
    "        char_indices.append(next_char_index)\n",
    "        \n",
    "    # Decode the final list of indices into a string\n",
    "    generated_text = decode(char_indices)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a2dc584-ce11-4827-a268-c8e39cda6b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 5000\n",
    "base_lr = 3e-4\n",
    "max_lr = 3e-4\n",
    "min_lr = 3e-5\n",
    "use_linear=True\n",
    "use_cosine=False\n",
    "warmup_steps = 100\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# Plotting Initialization\n",
    "step_plot_losses = []\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f69d3-557a-41a0-bbb3-23918ea36043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training model\")\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup (same for all schedules)\n",
    "        current_lr = max_lr * (step + 1) / warmup_steps\n",
    "    else:\n",
    "        if use_cosine:\n",
    "            \n",
    "            # Cosine decay\n",
    "            decay_ratio = (step - warmup_steps) / (max_iters - warmup_steps)\n",
    "            cosine_output = 0.5 * (1 + np.cos(np.pi * decay_ratio))\n",
    "            current_lr = min_lr + (max_lr - min_lr) * cosine_output\n",
    "        elif use_linear:\n",
    "            \n",
    "            # Linear decay\n",
    "            decay_ratio = (step - warmup_steps) / (max_iters - warmup_steps)\n",
    "            current_lr = max_lr - (max_lr - min_lr) * decay_ratio\n",
    "        else:\n",
    "            # Constant learning rate\n",
    "            current_lr = base_lr\n",
    "        \n",
    "        decay_ratio = (step - warmup_steps) / (max_iters - warmup_steps)\n",
    "        current_lr = max_lr - (max_lr - min_lr) * decay_ratio        \n",
    "        \n",
    "    # Get a mini-batch of data\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(len(chars))[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(current_lr)\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        \n",
    "        step_plot_losses.append(np.mean(plot_losses[-1-step:-1]))\n",
    "        \n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # Graph plot\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(step_plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if step_plot_losses[-1] < 4:\n",
    "            if loss_initial < 2:\n",
    "                ax.set_ylim(top=2) # cut off loses higher than 2\n",
    "            else:\n",
    "                ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(step_plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0aaa2-f5bf-4712-b0b4-8334375eb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"def\", 1.0, True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f22749-5eab-4fdd-87a1-a84b8bc01c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training, collect LoRA weights\n",
    "lora_weights = {}\n",
    "\n",
    "for transformer_idx, transformer in enumerate(model.transformers):\n",
    "    for head_idx, head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        \n",
    "        # Check if it's a LoRA layer and collect A and B matrices\n",
    "        if hasattr(head.W_query, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_A\"] = head.W_query.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_B\"] = head.W_query.lora_B.data\n",
    "            \n",
    "        if hasattr(head.W_value, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_A\"] = head.W_value.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_B\"] = head.W_value.lora_B.data\n",
    "\n",
    "# Save the LoRA weights\n",
    "np.savez_compressed('../models/lora_weights.npz', **lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7976a-0db7-456e-a8dc-b1d9ef6b558c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
