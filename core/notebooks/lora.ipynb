{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccc1b09-1546-415d-ab68-ceffcf08feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset: 'python-algorithms' git repo\n",
    "base_dataset = open(\"../data/requests.txt\").read()\n",
    "\n",
    "# Get original character set and create encode/decode functions\n",
    "chars = sorted(list(set(base_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac1175c-36cf-4ec6-a2b9-dfaab6c95f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string-to-integer mapping\n",
    "stoi = {char: i for i, char in enumerate(chars)}\n",
    "# integer-to-string mapping  \n",
    "itos = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Filter new text to only include original characters\n",
    "def filter_text(text, allowed_chars):\n",
    "    return ''.join(char for char in text if char in allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3583e428-f657-43c5-a28c-2f90566061f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter new dataset\n",
    "text = open(\"../data/python_fundamentals.txt\").read()\n",
    "filtered_text = filter_text(text, set(chars))  # Convert to set for faster lookup\n",
    "\n",
    "# Use filtered text for training\n",
    "data = np.array(encode(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e35ffc0-aa10-4aad-9230-aa573b6b2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742d9899-4c28-4870-8db2-59dfccf20402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2167158-a556-41c1-aace-0a1c1f27cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class allows for the requires grad variable \n",
    "class Parameter:\n",
    "    def __init__(self, weights, requires_grad=True):\n",
    "        \n",
    "        self.data = weights\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = np.zeros_like(weights)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Parameter with data shape {self.data.shape}\"\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        self.data -= other \n",
    "\n",
    "    def zerograds(self):\n",
    "        if self.requires_grad:\n",
    "             self.grad = np.zeros_like(self.data)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a3ef82-0717-4202-ae38-468f1f21e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            queries = self.W_query.forward(x)\n",
    "        else:\n",
    "            queries = x @ self.W_query.data\n",
    "\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            keys = self.W_key.forward(x)\n",
    "        else:\n",
    "            keys = x @ self.W_key.data\n",
    "\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            values = self.W_value.forward(x)\n",
    "        else:\n",
    "            values = x @ self.W_value.data\n",
    "\n",
    "            \n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            self.W_query.lora_A.zerograds()\n",
    "            self.W_query.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_query.zerograds()\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            self.W_key.lora_A.zerograds()\n",
    "            self.W_key.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_key.zerograds()\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            self.W_value.lora_A.zerograds()\n",
    "            self.W_value.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_value.zerograds()\n",
    "\n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        if hasattr(self.W_query, 'backward'):\n",
    "            d_x_from_queries = self.W_query.backward(d_queries, self.cache['x'])\n",
    "        else:\n",
    "            W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query.data, self.cache['x'])\n",
    "\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.grad = W_query_grad\n",
    "        if hasattr(self.W_key, 'backward'):\n",
    "            d_x_from_keys = self.W_key.backward(d_keys, self.cache['x'])\n",
    "        else:\n",
    "            W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key.data, self.cache['x'])\n",
    "\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.grad = W_key_grad\n",
    "        if hasattr(self.W_value, 'backward'):\n",
    "            d_x_from_values = self.W_value.backward(d_values, self.cache['x'])\n",
    "        else:\n",
    "            W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value.data, self.cache['x'])\n",
    "            \n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.grad = W_value_grad\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        if hasattr(self.W_query, 'optimizer'):\n",
    "            self.W_query.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.data -= (self.W_query.grad * learning_rate)\n",
    "        if hasattr(self.W_key, 'optimizer'):\n",
    "            self.W_key.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.data -= (self.W_key.grad * learning_rate)\n",
    "        if hasattr(self.W_value, 'optimizer'):\n",
    "            self.W_value.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.data -= (self.W_value.grad * learning_rate)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2648dbe3-8102-4d1e-8b89-46485bc1835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_k = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_v = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = Parameter(np.random.randn(n_embd, n_embd) * 0.02)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output.data\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output.zerograds()\n",
    "        \n",
    "        W_output_grad, d_concat = self.linear_backward(d_output, self.W_output.data, self.cache['concat_output'])\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.grad = W_output_grad\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.data -= (self.W_output.grad * learning_rate)\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f90b01-cc96-4223-88d4-468b03d5c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = Parameter(np.ones((n_embd,)))\n",
    "        self.beta = Parameter(np.zeros((n_embd,)))\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma.data\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma.data + self.beta.data\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.beta.zerograds()\n",
    "        self.gamma.zerograds()\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.grad = np.sum(d_output, axis=(0,1))\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.data -= (self.gamma.grad * learning_rate)\n",
    "\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.data -= (self.beta.grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5b63b8-7988-4f57-9f3f-1ba4f59a8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1.data\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.W1.zerograds()\n",
    "        self.W2.zerograds()\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2.data, self.cache['hidden_activated'])\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1.data, self.cache['norm_output_1'])\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        \n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.data -= (self.W1.grad * learning_rate)\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.data -= (self.W2.grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255ef7dc-845c-4909-ae58-b90bd28fd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        vocab_size = len(chars)\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = Parameter(np.random.randn(vocab_size, n_embd))\n",
    "        self.position_matrix = Parameter(np.random.randn(max_sequence_length, n_embd))\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = Parameter(np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd))\n",
    "            W2 = Parameter(np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor)))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix.data[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix.data[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.data.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "        \n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix.zerograds()\n",
    "        self.position_matrix.zerograds()\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix.data  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "        # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            # Perform reverse lookup on embedding array\n",
    "            np.add.at(self.embedding_matrix.grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.data -= (self.embedding_matrix.grad * learning_rate)\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.data -= (self.position_matrix.grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "\n",
    "        parameters = {\n",
    "            \"embedding_matrix\": self.embedding_matrix,\n",
    "            \"position_matrix\": self.position_matrix,\n",
    "        }\n",
    "\n",
    "        for idx, transformer in enumerate(self.transformers):\n",
    "            parameters[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "            parameters[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "            parameters[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "            parameters[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "            parameters[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "            \n",
    "            for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def freezeParams(self): \n",
    "\n",
    "        # Freezes model weights in prereration for LoRA training\n",
    "        self.embedding_matrix.requires_grad=False\n",
    "        self.position_matrix.requires_grad=False\n",
    "        \n",
    "        for x in self.transformers:\n",
    "\n",
    "            x.W1.requires_grad=False\n",
    "            x.W2.requires_grad=False\n",
    "            x.layer_norm1.gamma.requires_grad=False\n",
    "            x.layer_norm1.beta.requires_grad=False\n",
    "            x.layer_norm2.gamma.requires_grad=False\n",
    "            x.layer_norm2.beta.requires_grad=False\n",
    "            x.multi_head_attention_block.W_output.requires_grad=False\n",
    "            \n",
    "            for i in x.multi_head_attention_block.heads:\n",
    "\n",
    "                i.W_key.requires_grad=False\n",
    "                i.W_query.requires_grad=False\n",
    "                i.W_value.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d20ed2-0ba8-44c0-8a24-05fbb8072f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a095dd-8bde-46e9-8f5b-26187189b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix\n",
      "position_matrix\n",
      "transform.0.W1\n",
      "transform.0.W2\n",
      "transform.0.layer_norm1.gamma\n",
      "transform.0.layer_norm1.beta\n",
      "transform.0.layer_norm2.gamma\n",
      "transform.0.layer_norm2.beta\n",
      "transform.0.multi_head_attention_block.W_output\n",
      "transform.0.multi_head_attention_block.heads.0.key\n",
      "transform.0.multi_head_attention_block.heads.0.query\n",
      "transform.0.multi_head_attention_block.heads.0.value\n",
      "transform.0.multi_head_attention_block.heads.1.key\n",
      "transform.0.multi_head_attention_block.heads.1.query\n",
      "transform.0.multi_head_attention_block.heads.1.value\n",
      "transform.0.multi_head_attention_block.heads.2.key\n",
      "transform.0.multi_head_attention_block.heads.2.query\n",
      "transform.0.multi_head_attention_block.heads.2.value\n",
      "transform.0.multi_head_attention_block.heads.3.key\n",
      "transform.0.multi_head_attention_block.heads.3.query\n",
      "transform.0.multi_head_attention_block.heads.3.value\n",
      "transform.0.multi_head_attention_block.heads.4.key\n",
      "transform.0.multi_head_attention_block.heads.4.query\n",
      "transform.0.multi_head_attention_block.heads.4.value\n",
      "transform.0.multi_head_attention_block.heads.5.key\n",
      "transform.0.multi_head_attention_block.heads.5.query\n",
      "transform.0.multi_head_attention_block.heads.5.value\n",
      "transform.0.multi_head_attention_block.heads.6.key\n",
      "transform.0.multi_head_attention_block.heads.6.query\n",
      "transform.0.multi_head_attention_block.heads.6.value\n",
      "transform.0.multi_head_attention_block.heads.7.key\n",
      "transform.0.multi_head_attention_block.heads.7.query\n",
      "transform.0.multi_head_attention_block.heads.7.value\n",
      "transform.1.W1\n",
      "transform.1.W2\n",
      "transform.1.layer_norm1.gamma\n",
      "transform.1.layer_norm1.beta\n",
      "transform.1.layer_norm2.gamma\n",
      "transform.1.layer_norm2.beta\n",
      "transform.1.multi_head_attention_block.W_output\n",
      "transform.1.multi_head_attention_block.heads.0.key\n",
      "transform.1.multi_head_attention_block.heads.0.query\n",
      "transform.1.multi_head_attention_block.heads.0.value\n",
      "transform.1.multi_head_attention_block.heads.1.key\n",
      "transform.1.multi_head_attention_block.heads.1.query\n",
      "transform.1.multi_head_attention_block.heads.1.value\n",
      "transform.1.multi_head_attention_block.heads.2.key\n",
      "transform.1.multi_head_attention_block.heads.2.query\n",
      "transform.1.multi_head_attention_block.heads.2.value\n",
      "transform.1.multi_head_attention_block.heads.3.key\n",
      "transform.1.multi_head_attention_block.heads.3.query\n",
      "transform.1.multi_head_attention_block.heads.3.value\n",
      "transform.1.multi_head_attention_block.heads.4.key\n",
      "transform.1.multi_head_attention_block.heads.4.query\n",
      "transform.1.multi_head_attention_block.heads.4.value\n",
      "transform.1.multi_head_attention_block.heads.5.key\n",
      "transform.1.multi_head_attention_block.heads.5.query\n",
      "transform.1.multi_head_attention_block.heads.5.value\n",
      "transform.1.multi_head_attention_block.heads.6.key\n",
      "transform.1.multi_head_attention_block.heads.6.query\n",
      "transform.1.multi_head_attention_block.heads.6.value\n",
      "transform.1.multi_head_attention_block.heads.7.key\n",
      "transform.1.multi_head_attention_block.heads.7.query\n",
      "transform.1.multi_head_attention_block.heads.7.value\n",
      "transform.2.W1\n",
      "transform.2.W2\n",
      "transform.2.layer_norm1.gamma\n",
      "transform.2.layer_norm1.beta\n",
      "transform.2.layer_norm2.gamma\n",
      "transform.2.layer_norm2.beta\n",
      "transform.2.multi_head_attention_block.W_output\n",
      "transform.2.multi_head_attention_block.heads.0.key\n",
      "transform.2.multi_head_attention_block.heads.0.query\n",
      "transform.2.multi_head_attention_block.heads.0.value\n",
      "transform.2.multi_head_attention_block.heads.1.key\n",
      "transform.2.multi_head_attention_block.heads.1.query\n",
      "transform.2.multi_head_attention_block.heads.1.value\n",
      "transform.2.multi_head_attention_block.heads.2.key\n",
      "transform.2.multi_head_attention_block.heads.2.query\n",
      "transform.2.multi_head_attention_block.heads.2.value\n",
      "transform.2.multi_head_attention_block.heads.3.key\n",
      "transform.2.multi_head_attention_block.heads.3.query\n",
      "transform.2.multi_head_attention_block.heads.3.value\n",
      "transform.2.multi_head_attention_block.heads.4.key\n",
      "transform.2.multi_head_attention_block.heads.4.query\n",
      "transform.2.multi_head_attention_block.heads.4.value\n",
      "transform.2.multi_head_attention_block.heads.5.key\n",
      "transform.2.multi_head_attention_block.heads.5.query\n",
      "transform.2.multi_head_attention_block.heads.5.value\n",
      "transform.2.multi_head_attention_block.heads.6.key\n",
      "transform.2.multi_head_attention_block.heads.6.query\n",
      "transform.2.multi_head_attention_block.heads.6.value\n",
      "transform.2.multi_head_attention_block.heads.7.key\n",
      "transform.2.multi_head_attention_block.heads.7.query\n",
      "transform.2.multi_head_attention_block.heads.7.value\n",
      "transform.3.W1\n",
      "transform.3.W2\n",
      "transform.3.layer_norm1.gamma\n",
      "transform.3.layer_norm1.beta\n",
      "transform.3.layer_norm2.gamma\n",
      "transform.3.layer_norm2.beta\n",
      "transform.3.multi_head_attention_block.W_output\n",
      "transform.3.multi_head_attention_block.heads.0.key\n",
      "transform.3.multi_head_attention_block.heads.0.query\n",
      "transform.3.multi_head_attention_block.heads.0.value\n",
      "transform.3.multi_head_attention_block.heads.1.key\n",
      "transform.3.multi_head_attention_block.heads.1.query\n",
      "transform.3.multi_head_attention_block.heads.1.value\n",
      "transform.3.multi_head_attention_block.heads.2.key\n",
      "transform.3.multi_head_attention_block.heads.2.query\n",
      "transform.3.multi_head_attention_block.heads.2.value\n",
      "transform.3.multi_head_attention_block.heads.3.key\n",
      "transform.3.multi_head_attention_block.heads.3.query\n",
      "transform.3.multi_head_attention_block.heads.3.value\n",
      "transform.3.multi_head_attention_block.heads.4.key\n",
      "transform.3.multi_head_attention_block.heads.4.query\n",
      "transform.3.multi_head_attention_block.heads.4.value\n",
      "transform.3.multi_head_attention_block.heads.5.key\n",
      "transform.3.multi_head_attention_block.heads.5.query\n",
      "transform.3.multi_head_attention_block.heads.5.value\n",
      "transform.3.multi_head_attention_block.heads.6.key\n",
      "transform.3.multi_head_attention_block.heads.6.query\n",
      "transform.3.multi_head_attention_block.heads.6.value\n",
      "transform.3.multi_head_attention_block.heads.7.key\n",
      "transform.3.multi_head_attention_block.heads.7.query\n",
      "transform.3.multi_head_attention_block.heads.7.value\n",
      "transform.4.W1\n",
      "transform.4.W2\n",
      "transform.4.layer_norm1.gamma\n",
      "transform.4.layer_norm1.beta\n",
      "transform.4.layer_norm2.gamma\n",
      "transform.4.layer_norm2.beta\n",
      "transform.4.multi_head_attention_block.W_output\n",
      "transform.4.multi_head_attention_block.heads.0.key\n",
      "transform.4.multi_head_attention_block.heads.0.query\n",
      "transform.4.multi_head_attention_block.heads.0.value\n",
      "transform.4.multi_head_attention_block.heads.1.key\n",
      "transform.4.multi_head_attention_block.heads.1.query\n",
      "transform.4.multi_head_attention_block.heads.1.value\n",
      "transform.4.multi_head_attention_block.heads.2.key\n",
      "transform.4.multi_head_attention_block.heads.2.query\n",
      "transform.4.multi_head_attention_block.heads.2.value\n",
      "transform.4.multi_head_attention_block.heads.3.key\n",
      "transform.4.multi_head_attention_block.heads.3.query\n",
      "transform.4.multi_head_attention_block.heads.3.value\n",
      "transform.4.multi_head_attention_block.heads.4.key\n",
      "transform.4.multi_head_attention_block.heads.4.query\n",
      "transform.4.multi_head_attention_block.heads.4.value\n",
      "transform.4.multi_head_attention_block.heads.5.key\n",
      "transform.4.multi_head_attention_block.heads.5.query\n",
      "transform.4.multi_head_attention_block.heads.5.value\n",
      "transform.4.multi_head_attention_block.heads.6.key\n",
      "transform.4.multi_head_attention_block.heads.6.query\n",
      "transform.4.multi_head_attention_block.heads.6.value\n",
      "transform.4.multi_head_attention_block.heads.7.key\n",
      "transform.4.multi_head_attention_block.heads.7.query\n",
      "transform.4.multi_head_attention_block.heads.7.value\n",
      "transform.5.W1\n",
      "transform.5.W2\n",
      "transform.5.layer_norm1.gamma\n",
      "transform.5.layer_norm1.beta\n",
      "transform.5.layer_norm2.gamma\n",
      "transform.5.layer_norm2.beta\n",
      "transform.5.multi_head_attention_block.W_output\n",
      "transform.5.multi_head_attention_block.heads.0.key\n",
      "transform.5.multi_head_attention_block.heads.0.query\n",
      "transform.5.multi_head_attention_block.heads.0.value\n",
      "transform.5.multi_head_attention_block.heads.1.key\n",
      "transform.5.multi_head_attention_block.heads.1.query\n",
      "transform.5.multi_head_attention_block.heads.1.value\n",
      "transform.5.multi_head_attention_block.heads.2.key\n",
      "transform.5.multi_head_attention_block.heads.2.query\n",
      "transform.5.multi_head_attention_block.heads.2.value\n",
      "transform.5.multi_head_attention_block.heads.3.key\n",
      "transform.5.multi_head_attention_block.heads.3.query\n",
      "transform.5.multi_head_attention_block.heads.3.value\n",
      "transform.5.multi_head_attention_block.heads.4.key\n",
      "transform.5.multi_head_attention_block.heads.4.query\n",
      "transform.5.multi_head_attention_block.heads.4.value\n",
      "transform.5.multi_head_attention_block.heads.5.key\n",
      "transform.5.multi_head_attention_block.heads.5.query\n",
      "transform.5.multi_head_attention_block.heads.5.value\n",
      "transform.5.multi_head_attention_block.heads.6.key\n",
      "transform.5.multi_head_attention_block.heads.6.query\n",
      "transform.5.multi_head_attention_block.heads.6.value\n",
      "transform.5.multi_head_attention_block.heads.7.key\n",
      "transform.5.multi_head_attention_block.heads.7.query\n",
      "transform.5.multi_head_attention_block.heads.7.value\n",
      "Loading saved model weights...\n",
      "Loaded main matrices\n",
      "Loading transformer 0...\n",
      "Loaded transformer 0\n",
      "Loading transformer 1...\n",
      "Loaded transformer 1\n",
      "Loading transformer 2...\n",
      "Loaded transformer 2\n",
      "Loading transformer 3...\n",
      "Loaded transformer 3\n",
      "Loading transformer 4...\n",
      "Loaded transformer 4\n",
      "Loading transformer 5...\n",
      "Loaded transformer 5\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved weights\n",
    "loaded_weights = np.load('../models/my_model.npz')\n",
    "\n",
    "for key in loaded_weights.keys():\n",
    "    print(key)\n",
    "print(\"Loading saved model weights...\")\n",
    "\n",
    "# Set the main model weights\n",
    "model.embedding_matrix = Parameter(loaded_weights[\"embedding_matrix\"])\n",
    "model.position_matrix = Parameter(loaded_weights[\"position_matrix\"])\n",
    "print(\"Loaded main matrices\")\n",
    "\n",
    "# Load transformer weights\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    print(f\"Loading transformer {idx}...\")\n",
    "    \n",
    "    # Load basic weights\n",
    "    transformer.W1 = Parameter(loaded_weights[f\"transform.{idx}.W1\"])\n",
    "    transformer.W2 = Parameter(loaded_weights[f\"transform.{idx}.W2\"])\n",
    "    transformer.layer_norm1.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.gamma\"])\n",
    "    transformer.layer_norm1.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.beta\"])\n",
    "    transformer.layer_norm2.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.gamma\"])\n",
    "    transformer.layer_norm2.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.beta\"])\n",
    "    transformer.multi_head_attention_block.W_output = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.W_output\"])\n",
    "    \n",
    "    # Load attention head weights (16 heads each)\n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        attention_head.W_key = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"])\n",
    "        attention_head.W_query = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"])\n",
    "        attention_head.W_value = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"])\n",
    "    \n",
    "    print(f\"Loaded transformer {idx}\")\n",
    "\n",
    "loaded_weights.close()\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95e997c1-5e97-4c33-b725-c1709ff67b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:21: RuntimeWarning: divide by zero encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:21: RuntimeWarning: overflow encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:21: RuntimeWarning: invalid value encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:26: RuntimeWarning: divide by zero encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:26: RuntimeWarning: overflow encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:26: RuntimeWarning: invalid value encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:31: RuntimeWarning: divide by zero encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:31: RuntimeWarning: overflow encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:31: RuntimeWarning: invalid value encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:39: RuntimeWarning: divide by zero encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:39: RuntimeWarning: overflow encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:39: RuntimeWarning: invalid value encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:57: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:57: RuntimeWarning: overflow encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3910858463.py:57: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3048600086.py:45: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3048600086.py:45: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3048600086.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:22: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:22: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:22: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:28: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:28: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/3624853755.py:28: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7812277237019094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/2046029974.py:62: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/2046029974.py:62: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_29402/2046029974.py:62: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_batch(data, 128, 34)\n",
    "\n",
    "\n",
    "# Calculate loss and probabilites\n",
    "logits = model.forward(x_batch)\n",
    "loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "print(loss_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1bb214-fbe0-4a80-a312-5cee61efd734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer frozen? True\n",
      "Attention layer 15 frozen? True\n"
     ]
    }
   ],
   "source": [
    "# This one loop will freeze every parameter in the entire model\n",
    "for key, param in model.parameters.items():\n",
    "    # \"Freeze\" the model's weight\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Embedding layer frozen?\", model.transformers[4].multi_head_attention_block.W_output.requires_grad== False)\n",
    "print(\"Attention layer 15 frozen?\", model.transformers[2].multi_head_attention_block.heads[0].W_key.requires_grad == False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979888d3-0703-4db6-a6db-ddd5c9b6c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "\n",
    "    def __init__(self, original_layer, rank, alpha):\n",
    "\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        # Get dimensions from the original layer\n",
    "        in_features, out_features = self.original_layer.data.shape\n",
    "\n",
    "\n",
    "        # Better initialization following the paper\n",
    "        self.lora_A = Parameter(np.random.randn(in_features, rank) * np.sqrt(1.0 / rank))\n",
    "        self.lora_B = Parameter(np.zeros((rank, out_features)))  # Start with zeros\n",
    "\n",
    "\n",
    "        # Scaling factor\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The origin linear layer calculation\n",
    "        original_output = x @ self.original_layer.data\n",
    "\n",
    "        # New update w/A & B matrices\n",
    "        lora_update = (x @ self.lora_A.data @ self.lora_B.data) * self.scaling\n",
    "\n",
    "        return original_output + lora_update\n",
    "    \n",
    "    def backward(self, d_output, x_input):\n",
    "\n",
    "        self.lora_A.zerograds()\n",
    "        self.lora_B.zerograds()\n",
    "        \n",
    "        # Gradient w.r.t input (what we return)\n",
    "        # d_x = d_output @ (W_original + A @ B)^T\n",
    "        # Since W_original is frozen, we only need gradients through A @ B\n",
    "        d_x_original = d_output @ self.original_layer.data.T\n",
    "        d_x_lora = d_output @ (self.lora_A.data @ self.lora_B.data).T * self.scaling\n",
    "        d_x = d_x_original + d_x_lora\n",
    "        \n",
    "        # Gradients for LoRA parameters\n",
    "        # Reshape inputs for matrix operations\n",
    "        x_reshaped = x_input.reshape(-1, x_input.shape[-1])  # (B*T, in_features)\n",
    "        d_output_reshaped = d_output.reshape(-1, d_output.shape[-1])  # (B*T, out_features)\n",
    "        \n",
    "        # Gradient w.r.t B: A^T @ x^T @ d_output * scaling\n",
    "        self.lora_B.grad = (self.lora_A.data.T @ x_reshaped.T) @ d_output_reshaped * self.scaling\n",
    "        \n",
    "        # Gradient w.r.t A: x^T @ d_output @ B^T * scaling  \n",
    "        self.lora_A.grad = x_reshaped.T @ (d_output_reshaped @ self.lora_B.data.T) * self.scaling\n",
    "        \n",
    "        # Don't compute gradients for original_layer since it's frozen\n",
    "        return d_x        \n",
    "    \n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.lora_A.requires_grad:\n",
    "            self.lora_A.data -= (self.lora_A.grad * learning_rate)\n",
    "\n",
    "        if self.lora_B.requires_grad:\n",
    "            self.lora_B.data -= (self.lora_B.grad * learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a627b5-da56-4ec0-afd7-c019791472af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LoRA layers in model's query and value attention matrices\n",
    "for transformer in model.transformers:\n",
    "    for head in transformer.multi_head_attention_block.heads:\n",
    "\n",
    "\n",
    "        # Rank and alpha variables for LoRA layers\n",
    "        r = 8\n",
    "        a = 1\n",
    "\n",
    "        # Wrap query and value weights with LoRA\n",
    "        head.W_query = LoRALayer(head.W_query, r, a)\n",
    "        head.W_value = LoRALayer(head.W_value, r, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746d5489-e771-47a9-a667-7b2c014f607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature, heatMap, max_length):\n",
    "    print(f\"Generating with prompt: '{prompt}', temp: {temperature}, length: {max_length}\")\n",
    "    \n",
    "    if not prompt:\n",
    "        prompt = \"\\n\" # Default prompt\n",
    "\n",
    "    # Set the model's temperature for this specific generation\n",
    "    model.temperature = temperature\n",
    "    \n",
    "    # Encode the prompt and generate\n",
    "    char_indices = encode(prompt)\n",
    "    for _ in range(int(max_length)):\n",
    "        # Important: only feed the last block_size tokens as context\n",
    "        context = char_indices[-1000:]\n",
    "        next_char_index = model.pred(context)\n",
    "        char_indices.append(next_char_index)\n",
    "        \n",
    "    # Decode the final list of indices into a string\n",
    "    generated_text = decode(char_indices)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a2dc584-ce11-4827-a268-c8e39cda6b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 5000\n",
    "base_lr = 3e-4\n",
    "max_lr = 3e-4\n",
    "min_lr = 3e-5\n",
    "use_linear=True\n",
    "use_cosine=False\n",
    "warmup_steps = 100\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# Plotting Initialization\n",
    "step_plot_losses = []\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3f69d3-557a-41a0-bbb3-23918ea36043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: 1.9333433597392298\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model\")\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup (same for all schedules)\n",
    "        current_lr = max_lr * (step + 1) / warmup_steps\n",
    "    else:\n",
    "        if use_cosine:\n",
    "            \n",
    "            # Cosine decay\n",
    "            decay_ratio = (step - warmup_steps) / (max_iters - warmup_steps)\n",
    "            cosine_output = 0.5 * (1 + np.cos(np.pi * decay_ratio))\n",
    "            current_lr = min_lr + (max_lr - min_lr) * cosine_output\n",
    "        elif use_linear:\n",
    "            \n",
    "            # Linear decay\n",
    "            decay_ratio = (step - warmup_steps) / (max_iters - warmup_steps)\n",
    "            current_lr = max_lr - (max_lr - min_lr) * decay_ratio\n",
    "        else:\n",
    "            # Constant learning rate\n",
    "            current_lr = base_lr\n",
    "        \n",
    "        decay_ratio = (step - warmup_steps) / (max_iters - warmup_steps)\n",
    "        current_lr = max_lr - (max_lr - min_lr) * decay_ratio        \n",
    "        \n",
    "    # Get a mini-batch of data\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(len(chars))[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(current_lr)\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        \n",
    "        step_plot_losses.append(np.mean(plot_losses[-1-step:-1]))\n",
    "        \n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # Graph plot\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(step_plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if step_plot_losses[-1] < 4:\n",
    "            if loss_initial < 2:\n",
    "                ax.set_ylim(top=2) # cut off loses higher than 2\n",
    "            else:\n",
    "                ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(step_plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf0aaa2-f5bf-4712-b0b4-8334375eb8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with prompt: 'def', temp: 1.0, length: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"def names 2\\n            return d*[0]\\n\\n    [(\\n          * -enspeed)\\n\\n         isinstance(4, 1)] c -i 'tm\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"def\", 1.0, True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f22749-5eab-4fdd-87a1-a84b8bc01c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training, collect LoRA weights\n",
    "lora_weights = {}\n",
    "\n",
    "for transformer_idx, transformer in enumerate(model.transformers):\n",
    "    for head_idx, head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        \n",
    "        # Check if it's a LoRA layer and collect A and B matrices\n",
    "        if hasattr(head.W_query, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_A\"] = head.W_query.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_B\"] = head.W_query.lora_B.data\n",
    "            \n",
    "        if hasattr(head.W_value, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_A\"] = head.W_value.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_B\"] = head.W_value.lora_B.data\n",
    "\n",
    "# Save the LoRA weights\n",
    "np.savez_compressed('../models/lora_weights.npz', **lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7976a-0db7-456e-a8dc-b1d9ef6b558c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
