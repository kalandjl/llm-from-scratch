{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccc1b09-1546-415d-ab68-ceffcf08feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset: 'python-algorithms' git repo\n",
    "base_dataset = open(\"../data/requests.txt\").read()\n",
    "\n",
    "# Get original character set and create encode/decode functions\n",
    "chars = sorted(list(set(base_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac1175c-36cf-4ec6-a2b9-dfaab6c95f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string-to-integer mapping\n",
    "stoi = {char: i for i, char in enumerate(chars)}\n",
    "# integer-to-string mapping  \n",
    "itos = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Filter new text to only include original characters\n",
    "def filter_text(text, allowed_chars):\n",
    "    return ''.join(char for char in text if char in allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3583e428-f657-43c5-a28c-2f90566061f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter new dataset\n",
    "text = open(\"../data/python_fundamentals.txt\").read()\n",
    "filtered_text = filter_text(text, set(chars))  # Convert to set for faster lookup\n",
    "\n",
    "# Use filtered text for training\n",
    "data = np.array(encode(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e35ffc0-aa10-4aad-9230-aa573b6b2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742d9899-4c28-4870-8db2-59dfccf20402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2167158-a556-41c1-aace-0a1c1f27cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class allows for the requires grad variable \n",
    "class Parameter:\n",
    "    def __init__(self, weights, requires_grad=True):\n",
    "        \n",
    "        self.data = weights\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = np.zeros_like(weights)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Parameter with data shape {self.data.shape}\"\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        self.data -= other \n",
    "\n",
    "    def zerograds(self):\n",
    "        if self.requires_grad:\n",
    "             self.grad = np.zeros_like(self.data)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a3ef82-0717-4202-ae38-468f1f21e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            queries = self.W_query.forward(x)\n",
    "        else:\n",
    "            queries = x @ self.W_query.data\n",
    "\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            keys = self.W_key.forward(x)\n",
    "        else:\n",
    "            keys = x @ self.W_key.data\n",
    "\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            values = self.W_value.forward(x)\n",
    "        else:\n",
    "            values = x @ self.W_value.data\n",
    "\n",
    "            \n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "\n",
    "        # Check if is LoRA layer\n",
    "        if hasattr(self.W_query, 'forward'):\n",
    "            self.W_query.lora_A.zerograds()\n",
    "            self.W_query.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_query.zerograds()\n",
    "        if hasattr(self.W_key, 'forward'):\n",
    "            self.W_key.lora_A.zerograds()\n",
    "            self.W_key.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_key.zerograds()\n",
    "        if hasattr(self.W_value, 'forward'):\n",
    "            self.W_value.lora_A.zerograds()\n",
    "            self.W_value.lora_B.zerograds()\n",
    "        else:\n",
    "            self.W_value.zerograds()\n",
    "\n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        if hasattr(self.W_query, 'backward'):\n",
    "            d_x_from_queries = self.W_query.backward(d_queries, self.cache['x'])\n",
    "        else:\n",
    "            W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query.data, self.cache['x'])\n",
    "\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.grad = W_query_grad\n",
    "        if hasattr(self.W_key, 'backward'):\n",
    "            d_x_from_keys = self.W_key.backward(d_keys, self.cache['x'])\n",
    "        else:\n",
    "            W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key.data, self.cache['x'])\n",
    "\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.grad = W_key_grad\n",
    "        if hasattr(self.W_value, 'backward'):\n",
    "            d_x_from_values = self.W_value.backward(d_values, self.cache['x'])\n",
    "        else:\n",
    "            W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value.data, self.cache['x'])\n",
    "            \n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.grad = W_value_grad\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        if hasattr(self.W_query, 'optimizer'):\n",
    "            self.W_query.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_query.requires_grad:\n",
    "                self.W_query.data -= (self.W_query.grad * learning_rate)\n",
    "        if hasattr(self.W_key, 'optimizer'):\n",
    "            self.W_key.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_key.requires_grad:\n",
    "                self.W_key.data -= (self.W_key.grad * learning_rate)\n",
    "        if hasattr(self.W_value, 'optimizer'):\n",
    "            self.W_value.optimizer(learning_rate)\n",
    "        else:\n",
    "            if self.W_value.requires_grad:\n",
    "                self.W_value.data -= (self.W_value.grad * learning_rate)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2648dbe3-8102-4d1e-8b89-46485bc1835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_k = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            W_v = Parameter(np.random.randn(n_embd, self.head_dim) * 0.02)\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = Parameter(np.random.randn(n_embd, n_embd) * 0.02)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output.data\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output.zerograds()\n",
    "        \n",
    "        W_output_grad, d_concat = self.linear_backward(d_output, self.W_output.data, self.cache['concat_output'])\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.grad = W_output_grad\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.W_output.requires_grad:\n",
    "            self.W_output.data -= (self.W_output.grad * learning_rate)\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f90b01-cc96-4223-88d4-468b03d5c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = Parameter(np.ones((n_embd,)))\n",
    "        self.beta = Parameter(np.zeros((n_embd,)))\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma.data\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma.data + self.beta.data\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.beta.zerograds()\n",
    "        self.gamma.zerograds()\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.grad = np.sum(d_output, axis=(0,1))\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        if self.gamma.requires_grad:\n",
    "            self.gamma.data -= (self.gamma.grad * learning_rate)\n",
    "\n",
    "        if self.beta.requires_grad:\n",
    "            self.beta.data -= (self.beta.grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5b63b8-7988-4f57-9f3f-1ba4f59a8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1.data\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        self.W1.zerograds()\n",
    "        self.W2.zerograds()\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2.data, self.cache['hidden_activated'])\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1.data, self.cache['norm_output_1'])\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        \n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        if self.W1.requires_grad:\n",
    "            self.W1.data -= (self.W1.grad * learning_rate)\n",
    "\n",
    "        if self.W2.requires_grad:\n",
    "            self.W2.data -= (self.W2.grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255ef7dc-845c-4909-ae58-b90bd28fd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        vocab_size = len(chars)\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = Parameter(np.random.randn(vocab_size, n_embd))\n",
    "        self.position_matrix = Parameter(np.random.randn(max_sequence_length, n_embd))\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = Parameter(np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd))\n",
    "            W2 = Parameter(np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor)))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix.data[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix.data[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.data.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "        \n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix.zerograds()\n",
    "        self.position_matrix.zerograds()\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix.data  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "        # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            # Perform reverse lookup on embedding array\n",
    "            np.add.at(self.embedding_matrix.grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        if self.embedding_matrix.requires_grad:\n",
    "            self.embedding_matrix.data -= (self.embedding_matrix.grad * learning_rate)\n",
    "\n",
    "        if self.position_matrix.requires_grad:\n",
    "            self.position_matrix.data -= (self.position_matrix.grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "\n",
    "        parameters = {\n",
    "            \"embedding_matrix\": self.embedding_matrix,\n",
    "            \"position_matrix\": self.position_matrix,\n",
    "        }\n",
    "\n",
    "        for idx, transformer in enumerate(self.transformers):\n",
    "            parameters[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "            parameters[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "            parameters[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "            parameters[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "            parameters[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "            parameters[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "            \n",
    "            for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "                parameters[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def freezeParams(self): \n",
    "\n",
    "        # Freezes model weights in prereration for LoRA training\n",
    "        self.embedding_matrix.requires_grad=False\n",
    "        self.position_matrix.requires_grad=False\n",
    "        \n",
    "        for x in self.transformers:\n",
    "\n",
    "            x.W1.requires_grad=False\n",
    "            x.W2.requires_grad=False\n",
    "            x.layer_norm1.gamma.requires_grad=False\n",
    "            x.layer_norm1.beta.requires_grad=False\n",
    "            x.layer_norm2.gamma.requires_grad=False\n",
    "            x.layer_norm2.beta.requires_grad=False\n",
    "            x.multi_head_attention_block.W_output.requires_grad=False\n",
    "            \n",
    "            for i in x.multi_head_attention_block.heads:\n",
    "\n",
    "                i.W_key.requires_grad=False\n",
    "                i.W_query.requires_grad=False\n",
    "                i.W_value.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d20ed2-0ba8-44c0-8a24-05fbb8072f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a095dd-8bde-46e9-8f5b-26187189b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix\n",
      "position_matrix\n",
      "transform.0.W1\n",
      "transform.0.W2\n",
      "transform.0.layer_norm1.gamma\n",
      "transform.0.layer_norm1.beta\n",
      "transform.0.layer_norm2.gamma\n",
      "transform.0.layer_norm2.beta\n",
      "transform.0.multi_head_attention_block.W_output\n",
      "transform.0.multi_head_attention_block.heads.0.key\n",
      "transform.0.multi_head_attention_block.heads.0.query\n",
      "transform.0.multi_head_attention_block.heads.0.value\n",
      "transform.0.multi_head_attention_block.heads.1.key\n",
      "transform.0.multi_head_attention_block.heads.1.query\n",
      "transform.0.multi_head_attention_block.heads.1.value\n",
      "transform.0.multi_head_attention_block.heads.2.key\n",
      "transform.0.multi_head_attention_block.heads.2.query\n",
      "transform.0.multi_head_attention_block.heads.2.value\n",
      "transform.0.multi_head_attention_block.heads.3.key\n",
      "transform.0.multi_head_attention_block.heads.3.query\n",
      "transform.0.multi_head_attention_block.heads.3.value\n",
      "transform.0.multi_head_attention_block.heads.4.key\n",
      "transform.0.multi_head_attention_block.heads.4.query\n",
      "transform.0.multi_head_attention_block.heads.4.value\n",
      "transform.0.multi_head_attention_block.heads.5.key\n",
      "transform.0.multi_head_attention_block.heads.5.query\n",
      "transform.0.multi_head_attention_block.heads.5.value\n",
      "transform.0.multi_head_attention_block.heads.6.key\n",
      "transform.0.multi_head_attention_block.heads.6.query\n",
      "transform.0.multi_head_attention_block.heads.6.value\n",
      "transform.0.multi_head_attention_block.heads.7.key\n",
      "transform.0.multi_head_attention_block.heads.7.query\n",
      "transform.0.multi_head_attention_block.heads.7.value\n",
      "transform.1.W1\n",
      "transform.1.W2\n",
      "transform.1.layer_norm1.gamma\n",
      "transform.1.layer_norm1.beta\n",
      "transform.1.layer_norm2.gamma\n",
      "transform.1.layer_norm2.beta\n",
      "transform.1.multi_head_attention_block.W_output\n",
      "transform.1.multi_head_attention_block.heads.0.key\n",
      "transform.1.multi_head_attention_block.heads.0.query\n",
      "transform.1.multi_head_attention_block.heads.0.value\n",
      "transform.1.multi_head_attention_block.heads.1.key\n",
      "transform.1.multi_head_attention_block.heads.1.query\n",
      "transform.1.multi_head_attention_block.heads.1.value\n",
      "transform.1.multi_head_attention_block.heads.2.key\n",
      "transform.1.multi_head_attention_block.heads.2.query\n",
      "transform.1.multi_head_attention_block.heads.2.value\n",
      "transform.1.multi_head_attention_block.heads.3.key\n",
      "transform.1.multi_head_attention_block.heads.3.query\n",
      "transform.1.multi_head_attention_block.heads.3.value\n",
      "transform.1.multi_head_attention_block.heads.4.key\n",
      "transform.1.multi_head_attention_block.heads.4.query\n",
      "transform.1.multi_head_attention_block.heads.4.value\n",
      "transform.1.multi_head_attention_block.heads.5.key\n",
      "transform.1.multi_head_attention_block.heads.5.query\n",
      "transform.1.multi_head_attention_block.heads.5.value\n",
      "transform.1.multi_head_attention_block.heads.6.key\n",
      "transform.1.multi_head_attention_block.heads.6.query\n",
      "transform.1.multi_head_attention_block.heads.6.value\n",
      "transform.1.multi_head_attention_block.heads.7.key\n",
      "transform.1.multi_head_attention_block.heads.7.query\n",
      "transform.1.multi_head_attention_block.heads.7.value\n",
      "transform.2.W1\n",
      "transform.2.W2\n",
      "transform.2.layer_norm1.gamma\n",
      "transform.2.layer_norm1.beta\n",
      "transform.2.layer_norm2.gamma\n",
      "transform.2.layer_norm2.beta\n",
      "transform.2.multi_head_attention_block.W_output\n",
      "transform.2.multi_head_attention_block.heads.0.key\n",
      "transform.2.multi_head_attention_block.heads.0.query\n",
      "transform.2.multi_head_attention_block.heads.0.value\n",
      "transform.2.multi_head_attention_block.heads.1.key\n",
      "transform.2.multi_head_attention_block.heads.1.query\n",
      "transform.2.multi_head_attention_block.heads.1.value\n",
      "transform.2.multi_head_attention_block.heads.2.key\n",
      "transform.2.multi_head_attention_block.heads.2.query\n",
      "transform.2.multi_head_attention_block.heads.2.value\n",
      "transform.2.multi_head_attention_block.heads.3.key\n",
      "transform.2.multi_head_attention_block.heads.3.query\n",
      "transform.2.multi_head_attention_block.heads.3.value\n",
      "transform.2.multi_head_attention_block.heads.4.key\n",
      "transform.2.multi_head_attention_block.heads.4.query\n",
      "transform.2.multi_head_attention_block.heads.4.value\n",
      "transform.2.multi_head_attention_block.heads.5.key\n",
      "transform.2.multi_head_attention_block.heads.5.query\n",
      "transform.2.multi_head_attention_block.heads.5.value\n",
      "transform.2.multi_head_attention_block.heads.6.key\n",
      "transform.2.multi_head_attention_block.heads.6.query\n",
      "transform.2.multi_head_attention_block.heads.6.value\n",
      "transform.2.multi_head_attention_block.heads.7.key\n",
      "transform.2.multi_head_attention_block.heads.7.query\n",
      "transform.2.multi_head_attention_block.heads.7.value\n",
      "transform.3.W1\n",
      "transform.3.W2\n",
      "transform.3.layer_norm1.gamma\n",
      "transform.3.layer_norm1.beta\n",
      "transform.3.layer_norm2.gamma\n",
      "transform.3.layer_norm2.beta\n",
      "transform.3.multi_head_attention_block.W_output\n",
      "transform.3.multi_head_attention_block.heads.0.key\n",
      "transform.3.multi_head_attention_block.heads.0.query\n",
      "transform.3.multi_head_attention_block.heads.0.value\n",
      "transform.3.multi_head_attention_block.heads.1.key\n",
      "transform.3.multi_head_attention_block.heads.1.query\n",
      "transform.3.multi_head_attention_block.heads.1.value\n",
      "transform.3.multi_head_attention_block.heads.2.key\n",
      "transform.3.multi_head_attention_block.heads.2.query\n",
      "transform.3.multi_head_attention_block.heads.2.value\n",
      "transform.3.multi_head_attention_block.heads.3.key\n",
      "transform.3.multi_head_attention_block.heads.3.query\n",
      "transform.3.multi_head_attention_block.heads.3.value\n",
      "transform.3.multi_head_attention_block.heads.4.key\n",
      "transform.3.multi_head_attention_block.heads.4.query\n",
      "transform.3.multi_head_attention_block.heads.4.value\n",
      "transform.3.multi_head_attention_block.heads.5.key\n",
      "transform.3.multi_head_attention_block.heads.5.query\n",
      "transform.3.multi_head_attention_block.heads.5.value\n",
      "transform.3.multi_head_attention_block.heads.6.key\n",
      "transform.3.multi_head_attention_block.heads.6.query\n",
      "transform.3.multi_head_attention_block.heads.6.value\n",
      "transform.3.multi_head_attention_block.heads.7.key\n",
      "transform.3.multi_head_attention_block.heads.7.query\n",
      "transform.3.multi_head_attention_block.heads.7.value\n",
      "transform.4.W1\n",
      "transform.4.W2\n",
      "transform.4.layer_norm1.gamma\n",
      "transform.4.layer_norm1.beta\n",
      "transform.4.layer_norm2.gamma\n",
      "transform.4.layer_norm2.beta\n",
      "transform.4.multi_head_attention_block.W_output\n",
      "transform.4.multi_head_attention_block.heads.0.key\n",
      "transform.4.multi_head_attention_block.heads.0.query\n",
      "transform.4.multi_head_attention_block.heads.0.value\n",
      "transform.4.multi_head_attention_block.heads.1.key\n",
      "transform.4.multi_head_attention_block.heads.1.query\n",
      "transform.4.multi_head_attention_block.heads.1.value\n",
      "transform.4.multi_head_attention_block.heads.2.key\n",
      "transform.4.multi_head_attention_block.heads.2.query\n",
      "transform.4.multi_head_attention_block.heads.2.value\n",
      "transform.4.multi_head_attention_block.heads.3.key\n",
      "transform.4.multi_head_attention_block.heads.3.query\n",
      "transform.4.multi_head_attention_block.heads.3.value\n",
      "transform.4.multi_head_attention_block.heads.4.key\n",
      "transform.4.multi_head_attention_block.heads.4.query\n",
      "transform.4.multi_head_attention_block.heads.4.value\n",
      "transform.4.multi_head_attention_block.heads.5.key\n",
      "transform.4.multi_head_attention_block.heads.5.query\n",
      "transform.4.multi_head_attention_block.heads.5.value\n",
      "transform.4.multi_head_attention_block.heads.6.key\n",
      "transform.4.multi_head_attention_block.heads.6.query\n",
      "transform.4.multi_head_attention_block.heads.6.value\n",
      "transform.4.multi_head_attention_block.heads.7.key\n",
      "transform.4.multi_head_attention_block.heads.7.query\n",
      "transform.4.multi_head_attention_block.heads.7.value\n",
      "transform.5.W1\n",
      "transform.5.W2\n",
      "transform.5.layer_norm1.gamma\n",
      "transform.5.layer_norm1.beta\n",
      "transform.5.layer_norm2.gamma\n",
      "transform.5.layer_norm2.beta\n",
      "transform.5.multi_head_attention_block.W_output\n",
      "transform.5.multi_head_attention_block.heads.0.key\n",
      "transform.5.multi_head_attention_block.heads.0.query\n",
      "transform.5.multi_head_attention_block.heads.0.value\n",
      "transform.5.multi_head_attention_block.heads.1.key\n",
      "transform.5.multi_head_attention_block.heads.1.query\n",
      "transform.5.multi_head_attention_block.heads.1.value\n",
      "transform.5.multi_head_attention_block.heads.2.key\n",
      "transform.5.multi_head_attention_block.heads.2.query\n",
      "transform.5.multi_head_attention_block.heads.2.value\n",
      "transform.5.multi_head_attention_block.heads.3.key\n",
      "transform.5.multi_head_attention_block.heads.3.query\n",
      "transform.5.multi_head_attention_block.heads.3.value\n",
      "transform.5.multi_head_attention_block.heads.4.key\n",
      "transform.5.multi_head_attention_block.heads.4.query\n",
      "transform.5.multi_head_attention_block.heads.4.value\n",
      "transform.5.multi_head_attention_block.heads.5.key\n",
      "transform.5.multi_head_attention_block.heads.5.query\n",
      "transform.5.multi_head_attention_block.heads.5.value\n",
      "transform.5.multi_head_attention_block.heads.6.key\n",
      "transform.5.multi_head_attention_block.heads.6.query\n",
      "transform.5.multi_head_attention_block.heads.6.value\n",
      "transform.5.multi_head_attention_block.heads.7.key\n",
      "transform.5.multi_head_attention_block.heads.7.query\n",
      "transform.5.multi_head_attention_block.heads.7.value\n",
      "Loading saved model weights...\n",
      "Loaded main matrices\n",
      "Loading transformer 0...\n",
      "Loaded transformer 0\n",
      "Loading transformer 1...\n",
      "Loaded transformer 1\n",
      "Loading transformer 2...\n",
      "Loaded transformer 2\n",
      "Loading transformer 3...\n",
      "Loaded transformer 3\n",
      "Loading transformer 4...\n",
      "Loaded transformer 4\n",
      "Loading transformer 5...\n",
      "Loaded transformer 5\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved weights\n",
    "loaded_weights = np.load('../models/my_model.npz')\n",
    "\n",
    "for key in loaded_weights.keys():\n",
    "    print(key)\n",
    "print(\"Loading saved model weights...\")\n",
    "\n",
    "# Set the main model weights\n",
    "model.embedding_matrix = Parameter(loaded_weights[\"embedding_matrix\"])\n",
    "model.position_matrix = Parameter(loaded_weights[\"position_matrix\"])\n",
    "print(\"Loaded main matrices\")\n",
    "\n",
    "# Load transformer weights\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    print(f\"Loading transformer {idx}...\")\n",
    "    \n",
    "    # Load basic weights\n",
    "    transformer.W1 = Parameter(loaded_weights[f\"transform.{idx}.W1\"])\n",
    "    transformer.W2 = Parameter(loaded_weights[f\"transform.{idx}.W2\"])\n",
    "    transformer.layer_norm1.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.gamma\"])\n",
    "    transformer.layer_norm1.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm1.beta\"])\n",
    "    transformer.layer_norm2.gamma = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.gamma\"])\n",
    "    transformer.layer_norm2.beta = Parameter(loaded_weights[f\"transform.{idx}.layer_norm2.beta\"])\n",
    "    transformer.multi_head_attention_block.W_output = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.W_output\"])\n",
    "    \n",
    "    # Load attention head weights (16 heads each)\n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        attention_head.W_key = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"])\n",
    "        attention_head.W_query = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"])\n",
    "        attention_head.W_value = Parameter(loaded_weights[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"])\n",
    "    \n",
    "    print(f\"Loaded transformer {idx}\")\n",
    "\n",
    "loaded_weights.close()\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95e997c1-5e97-4c33-b725-c1709ff67b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:21: RuntimeWarning: divide by zero encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:21: RuntimeWarning: overflow encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:21: RuntimeWarning: invalid value encountered in matmul\n",
      "  queries = x @ self.W_query.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:26: RuntimeWarning: divide by zero encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:26: RuntimeWarning: overflow encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:26: RuntimeWarning: invalid value encountered in matmul\n",
      "  keys = x @ self.W_key.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:31: RuntimeWarning: divide by zero encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:31: RuntimeWarning: overflow encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:31: RuntimeWarning: invalid value encountered in matmul\n",
      "  values = x @ self.W_value.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:39: RuntimeWarning: divide by zero encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:39: RuntimeWarning: overflow encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:39: RuntimeWarning: invalid value encountered in matmul\n",
      "  attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:57: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:57: RuntimeWarning: overflow encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3910858463.py:57: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = attention_weights @ values  # (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3048600086.py:45: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3048600086.py:45: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3048600086.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3624853755.py:22: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3624853755.py:22: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3624853755.py:22: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1.data\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3624853755.py:28: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3624853755.py:28: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/3624853755.py:28: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2.data # Shape: (B, T, n_embd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.135086154769552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/2046029974.py:62: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/2046029974.py:62: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_10982/2046029974.py:62: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.data.T\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_batch(data, 128, 34)\n",
    "\n",
    "\n",
    "# Calculate loss and probabilites\n",
    "logits = model.forward(x_batch)\n",
    "loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "print(loss_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1bb214-fbe0-4a80-a312-5cee61efd734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer frozen? True\n",
      "Attention layer 15 frozen? True\n"
     ]
    }
   ],
   "source": [
    "# This one loop will freeze every parameter in the entire model\n",
    "for key, param in model.parameters.items():\n",
    "    # \"Freeze\" the model's weight\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Embedding layer frozen?\", model.transformers[4].multi_head_attention_block.W_output.requires_grad== False)\n",
    "print(\"Attention layer 15 frozen?\", model.transformers[2].multi_head_attention_block.heads[0].W_key.requires_grad == False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979888d3-0703-4db6-a6db-ddd5c9b6c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoRALayer():\n",
    "\n",
    "    def __init__(self, original_layer, rank, alpha):\n",
    "\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        # Get dimensions from the original layer\n",
    "        in_features, out_features = self.original_layer.data.shape\n",
    "\n",
    "\n",
    "        # Initialize LoRA A & B matrices\n",
    "        self.lora_A = Parameter(np.random.randn(in_features, rank))\n",
    "        self.lora_B = Parameter(np.zeros((rank, out_features))) # Paper initializes B with zeros\n",
    "\n",
    "\n",
    "        # Scaling factor\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The origin linear layer calculation\n",
    "        original_output = x @ self.original_layer.data\n",
    "\n",
    "        # New update w/A & B matrices\n",
    "        lora_update = (x @ self.lora_A.data @ self.lora_B.data) * self.scaling\n",
    "\n",
    "        return original_output + lora_update\n",
    "    \n",
    "    def backward(self, d_output, x_input):\n",
    "\n",
    "        self.lora_A.zerograds()\n",
    "        self.lora_B.zerograds()\n",
    "        \n",
    "        # Gradient w.r.t input (what we return)\n",
    "        # d_x = d_output @ (W_original + A @ B)^T\n",
    "        # Since W_original is frozen, we only need gradients through A @ B\n",
    "        d_x_original = d_output @ self.original_layer.data.T\n",
    "        d_x_lora = d_output @ (self.lora_A.data @ self.lora_B.data).T * self.scaling\n",
    "        d_x = d_x_original + d_x_lora\n",
    "        \n",
    "        # Gradients for LoRA parameters\n",
    "        # Reshape inputs for matrix operations\n",
    "        x_reshaped = x_input.reshape(-1, x_input.shape[-1])  # (B*T, in_features)\n",
    "        d_output_reshaped = d_output.reshape(-1, d_output.shape[-1])  # (B*T, out_features)\n",
    "        \n",
    "        # Gradient w.r.t B: A^T @ x^T @ d_output * scaling\n",
    "        self.lora_B.grad = (self.lora_A.data.T @ x_reshaped.T @ d_output_reshaped) * self.scaling\n",
    "        \n",
    "        # Gradient w.r.t A: x^T @ d_output @ B^T * scaling  \n",
    "        self.lora_A.grad = (x_reshaped.T @ d_output_reshaped @ self.lora_B.data.T) * self.scaling\n",
    "        \n",
    "        # Don't compute gradients for original_layer since it's frozen\n",
    "        return d_x        \n",
    "    \n",
    "    def optimizer(self, learning_rate):\n",
    "\n",
    "        if self.lora_A.requires_grad:\n",
    "            self.lora_A.data -= (self.lora_A.grad * learning_rate)\n",
    "\n",
    "        if self.lora_B.requires_grad:\n",
    "            self.lora_B.data -= (self.lora_B.grad * learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a627b5-da56-4ec0-afd7-c019791472af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LoRA layers in model's query and value attention matrices\n",
    "for transformer in model.transformers:\n",
    "    for head in transformer.multi_head_attention_block.heads:\n",
    "\n",
    "\n",
    "        # Rank and alpha variables for LoRA layers\n",
    "        r = 8\n",
    "        a = 16\n",
    "\n",
    "        # Wrap query and value weights with LoRA\n",
    "        head.W_query = LoRALayer(head.W_query, r, a)\n",
    "        head.W_value = LoRALayer(head.W_value, r, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746d5489-e771-47a9-a667-7b2c014f607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature, heatMap, max_length):\n",
    "    print(f\"Generating with prompt: '{prompt}', temp: {temperature}, length: {max_length}\")\n",
    "    \n",
    "    if not prompt:\n",
    "        prompt = \"\\n\" # Default prompt\n",
    "\n",
    "    # Set the model's temperature for this specific generation\n",
    "    model.temperature = temperature\n",
    "    \n",
    "    # Encode the prompt and generate\n",
    "    char_indices = encode(prompt)\n",
    "    for _ in range(int(max_length)):\n",
    "        # Important: only feed the last block_size tokens as context\n",
    "        context = char_indices[-1000:]\n",
    "        next_char_index = model.pred(context)\n",
    "        char_indices.append(next_char_index)\n",
    "        \n",
    "    # Decode the final list of indices into a string\n",
    "    generated_text = decode(char_indices)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f69d3-557a-41a0-bbb3-23918ea36043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 25000\n",
    "learning_rate = 1e-6\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# Plotting Initialization\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(len(chars))[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # Graph plot\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=1) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f22749-5eab-4fdd-87a1-a84b8bc01c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training, collect LoRA weights\n",
    "lora_weights = {}\n",
    "\n",
    "for transformer_idx, transformer in enumerate(model.transformers):\n",
    "    for head_idx, head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        \n",
    "        # Check if it's a LoRA layer and collect A and B matrices\n",
    "        if hasattr(head.W_query, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_A\"] = head.W_query.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.query.lora_B\"] = head.W_query.lora_B.data\n",
    "            \n",
    "        if hasattr(head.W_value, 'lora_A'):\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_A\"] = head.W_value.lora_A.data\n",
    "            lora_weights[f\"transform.{transformer_idx}.head.{head_idx}.value.lora_B\"] = head.W_value.lora_B.data\n",
    "\n",
    "# Save the LoRA weights\n",
    "np.savez_compressed('../models/lora_weights.npz', **lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7976a-0db7-456e-a8dc-b1d9ef6b558c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
