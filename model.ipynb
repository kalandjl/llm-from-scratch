{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
