{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x100)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
