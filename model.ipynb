{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x100)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
