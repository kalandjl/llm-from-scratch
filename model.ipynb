{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
