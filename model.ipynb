{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
