{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
