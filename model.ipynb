{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Each character has weights of a 32 long vector, defined by n_embed (embedding dimension)\n",
    "n_embd = 32\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b31441d-4b8c-4ce4-ac79-673775407b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard expansion factor of four\n",
    "ffwd_expansion_factor = 4\n",
    "\n",
    "# Initialize hidden layer and output layer\n",
    "W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / n_embd)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-9\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        # Divide by sqrt of dimension for numerical stability\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "\n",
    "        # Causal mask\n",
    "        mask = np.tril(np.ones((T, T))) == 0  # Upper triangle is True\n",
    "        attention_scores[:, mask] = -np.inf  # Apply mask to all batches\n",
    "\n",
    "        # softmax\n",
    "        stable_scores = attention_scores - np.max(attention_scores, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "        \"\"\"\n",
    "        Calculates the gradients for gamma, beta, and the input x.\n",
    "        \"\"\"\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, W1, W2, temperature=1.0, max_sequence_length=1000, n_embd=32):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.unembedding_matrix = embedding_matrix.transpose()\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Transformer block\n",
    "        self.transformer = Transformer(W1, W2, n_heads=8, n_embd=n_embd)\n",
    "        \n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        \n",
    "        # Self-attention\n",
    "        processed_vectors = self.transformer.forward(attn_input)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        logits = processed_vectors @ self.unembedding_matrix\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # unembedding layer\n",
    "        grad_unembed, d_processed = self.linear_backward(d_logits, self.unembedding_matrix, self.cache['processed_vectors'])\n",
    "        self.embedding_matrix_grad = grad_unembed.transpose()\n",
    "\n",
    "\n",
    "        d_attn_input = self.transformer.backward(d_processed)\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        self.transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1572549786.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1572549786.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1572549786.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1572549786.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1572549786.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1572549786.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/3813542193.py:46: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = processed_vectors @ self.unembedding_matrix\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/3813542193.py:46: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = processed_vectors @ self.unembedding_matrix\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/3813542193.py:46: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = processed_vectors @ self.unembedding_matrix\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34777/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(29)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix, W1, W2)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 14.965701905605096\n",
      "Step 10, Loss: 9.509538413932106\n",
      "Step 20, Loss: 5.976991273363565\n",
      "Step 30, Loss: 4.20484512572122\n",
      "Step 40, Loss: 3.803301704289831\n",
      "Step 50, Loss: 4.13413566782751\n",
      "Step 60, Loss: 3.916483434768704\n",
      "Step 70, Loss: 3.244126885536068\n",
      "Step 80, Loss: 3.1976501106563684\n",
      "Step 90, Loss: 3.6772960901476153\n",
      "Step 100, Loss: 3.4461942609033587\n",
      "Step 110, Loss: 3.057174773804226\n",
      "Step 120, Loss: 3.4250449391905677\n",
      "Step 130, Loss: 3.3911700058336964\n",
      "Step 140, Loss: 3.1033394504036447\n",
      "Step 150, Loss: 3.1539373534834385\n",
      "Step 160, Loss: 3.186845052621626\n",
      "Step 170, Loss: 2.9383309392521606\n",
      "Step 180, Loss: 4.764402825520054\n",
      "Step 190, Loss: 5.03804717079849\n",
      "Step 200, Loss: 3.9903719255183407\n",
      "Step 210, Loss: 3.772518685683026\n",
      "Step 220, Loss: 2.866948999579463\n",
      "Step 230, Loss: 3.883815266538991\n",
      "Step 240, Loss: 3.3088572995111933\n",
      "Step 250, Loss: 3.0227571564508615\n",
      "Step 260, Loss: 3.02322948538889\n",
      "Step 270, Loss: 4.515794263553871\n",
      "Step 280, Loss: 3.2158466822067755\n",
      "Step 290, Loss: 3.444339372779658\n",
      "Step 300, Loss: 2.9776348984288252\n",
      "Step 310, Loss: 2.929892294623417\n",
      "Step 320, Loss: 3.4528969753237573\n",
      "Step 330, Loss: 2.8379087108362624\n",
      "Step 340, Loss: 3.5301624435282464\n",
      "Step 350, Loss: 2.891188542626464\n",
      "Step 360, Loss: 3.6762565377342162\n",
      "Step 370, Loss: 3.068049404194007\n",
      "Step 380, Loss: 3.0330466676287564\n",
      "Step 390, Loss: 2.7405166327368486\n",
      "Step 400, Loss: 2.8205367222427937\n",
      "Step 410, Loss: 3.4162219890749475\n",
      "Step 420, Loss: 2.9970926420047226\n",
      "Step 430, Loss: 2.7875733085821355\n",
      "Step 440, Loss: 3.6204985291733305\n",
      "Step 450, Loss: 2.729158673156269\n",
      "Step 460, Loss: 2.900702891940035\n",
      "Step 470, Loss: 2.801201027232172\n",
      "Step 480, Loss: 2.9228996936056872\n",
      "Step 490, Loss: 2.6997426785953906\n",
      "Step 500, Loss: 2.7115487183615254\n",
      "Step 510, Loss: 2.9483314591271346\n",
      "Step 520, Loss: 2.938636859782929\n",
      "Step 530, Loss: 2.726525642618425\n",
      "Step 540, Loss: 3.3099691519804755\n",
      "Step 550, Loss: 2.6497684477617396\n",
      "Step 560, Loss: 3.1254729965833015\n",
      "Step 570, Loss: 2.827851279688336\n",
      "Step 580, Loss: 2.5576795868400435\n",
      "Step 590, Loss: 2.750106395962451\n",
      "Step 600, Loss: 2.87724835710477\n",
      "Step 610, Loss: 3.1292443442086886\n",
      "Step 620, Loss: 2.800210370162844\n",
      "Step 630, Loss: 2.7352742075238052\n",
      "Step 640, Loss: 2.6372829731682668\n",
      "Step 650, Loss: 2.6620109343924083\n",
      "Step 660, Loss: 2.975072638039096\n",
      "Step 670, Loss: 2.432580083352094\n",
      "Step 680, Loss: 2.665174630525233\n",
      "Step 690, Loss: 3.4157373132453928\n",
      "Step 700, Loss: 2.5996097981826494\n",
      "Step 710, Loss: 2.606537217118534\n",
      "Step 720, Loss: 3.0135120727822424\n",
      "Step 730, Loss: 2.7610231343815745\n",
      "Step 740, Loss: 3.13501196419318\n",
      "Step 750, Loss: 2.823119389386688\n",
      "Step 760, Loss: 2.68187164345306\n",
      "Step 770, Loss: 2.720127090676681\n",
      "Step 780, Loss: 2.586229195241728\n",
      "Step 790, Loss: 2.7613646723601626\n",
      "Step 800, Loss: 2.9461815974461674\n",
      "Step 810, Loss: 2.7493377624650157\n",
      "Step 820, Loss: 2.74925942557128\n",
      "Step 830, Loss: 2.5612567484516733\n",
      "Step 840, Loss: 2.984129659746594\n",
      "Step 850, Loss: 2.939889145578272\n",
      "Step 860, Loss: 3.066196270461545\n",
      "Step 870, Loss: 2.723858116613726\n",
      "Step 880, Loss: 2.6117725148497626\n",
      "Step 890, Loss: 2.3812087312600836\n",
      "Step 900, Loss: 2.510262710770005\n",
      "Step 910, Loss: 2.749265698132339\n",
      "Step 920, Loss: 2.6597950990522516\n",
      "Step 930, Loss: 2.6466089872988015\n",
      "Step 940, Loss: 3.029496603909925\n",
      "Step 950, Loss: 2.569121963998336\n",
      "Step 960, Loss: 2.4980422877142905\n",
      "Step 970, Loss: 2.553755410933485\n",
      "Step 980, Loss: 2.5221995665029926\n",
      "Step 990, Loss: 2.7783840266886246\n",
      "Step 1000, Loss: 2.6398698499457827\n",
      "Step 1010, Loss: 2.8205666916802117\n",
      "Step 1020, Loss: 2.4891895752053945\n",
      "Step 1030, Loss: 2.673157550272722\n",
      "Step 1040, Loss: 2.349416677259807\n",
      "Step 1050, Loss: 2.7437093443624203\n",
      "Step 1060, Loss: 2.5633073792857113\n",
      "Step 1070, Loss: 2.4543788802178623\n",
      "Step 1080, Loss: 2.548695113054955\n",
      "Step 1090, Loss: 2.6491536299651797\n",
      "Step 1100, Loss: 2.6202164101507925\n",
      "Step 1110, Loss: 2.5745214470054223\n",
      "Step 1120, Loss: 2.609770408912291\n",
      "Step 1130, Loss: 2.671217242453579\n",
      "Step 1140, Loss: 2.558583237247866\n",
      "Step 1150, Loss: 2.6453417852367016\n",
      "Step 1160, Loss: 2.420800131562403\n",
      "Step 1170, Loss: 2.5113831568897513\n",
      "Step 1180, Loss: 2.522711719973635\n",
      "Step 1190, Loss: 2.4741693897917156\n",
      "Step 1200, Loss: 2.710261760948372\n",
      "Step 1210, Loss: 2.3141387063062773\n",
      "Step 1220, Loss: 2.5274160547079383\n",
      "Step 1230, Loss: 2.382617627330429\n",
      "Step 1240, Loss: 2.502979371206794\n",
      "Step 1250, Loss: 2.5282349286558725\n",
      "Step 1260, Loss: 2.463544349177311\n",
      "Step 1270, Loss: 2.500131212879879\n",
      "Step 1280, Loss: 2.492718739296951\n",
      "Step 1290, Loss: 2.5134878914706458\n",
      "Step 1300, Loss: 2.625255278035084\n",
      "Step 1310, Loss: 2.5754365604559246\n",
      "Step 1320, Loss: 2.692415197542729\n",
      "Step 1330, Loss: 2.3818267222462834\n",
      "Step 1340, Loss: 2.710860241510237\n",
      "Step 1350, Loss: 2.4923321702068546\n",
      "Step 1360, Loss: 2.32458550223841\n",
      "Step 1370, Loss: 2.5038516480736153\n",
      "Step 1380, Loss: 2.600329954911207\n",
      "Step 1390, Loss: 2.3929308672293246\n",
      "Step 1400, Loss: 2.616602643291444\n",
      "Step 1410, Loss: 2.2294180063931774\n",
      "Step 1420, Loss: 2.4240581983434617\n",
      "Step 1430, Loss: 2.482237245583523\n",
      "Step 1440, Loss: 2.3006446418586206\n",
      "Step 1450, Loss: 2.6859535462317403\n",
      "Step 1460, Loss: 2.432277568346756\n",
      "Step 1470, Loss: 2.5149470859278353\n",
      "Step 1480, Loss: 2.7018881949961586\n",
      "Step 1490, Loss: 2.418603299696153\n",
      "Step 1500, Loss: 2.546235908811704\n",
      "Step 1510, Loss: 2.54184380390861\n",
      "Step 1520, Loss: 2.5735999016024578\n",
      "Step 1530, Loss: 2.35271873534534\n",
      "Step 1540, Loss: 2.4237192401773315\n",
      "Step 1550, Loss: 2.4505916337539726\n",
      "Step 1560, Loss: 2.510667651367691\n",
      "Step 1570, Loss: 2.5953900102355214\n",
      "Step 1580, Loss: 2.4679985216929525\n",
      "Step 1590, Loss: 2.355079224008117\n",
      "Step 1600, Loss: 2.331779591147368\n",
      "Step 1610, Loss: 2.328501233825067\n",
      "Step 1620, Loss: 2.31468964372387\n",
      "Step 1630, Loss: 2.378790522922391\n",
      "Step 1640, Loss: 2.448544209272542\n",
      "Step 1650, Loss: 2.2717371261712596\n",
      "Step 1660, Loss: 2.2729403309197855\n",
      "Step 1670, Loss: 2.5162480695277574\n",
      "Step 1680, Loss: 2.4382491349812896\n",
      "Step 1690, Loss: 2.2462924091298255\n",
      "Step 1700, Loss: 2.257468650974593\n",
      "Step 1710, Loss: 2.329905700228747\n",
      "Step 1720, Loss: 2.3440059348462983\n",
      "Step 1730, Loss: 2.3458745006935238\n",
      "Step 1740, Loss: 2.4113412976746127\n",
      "Step 1750, Loss: 2.5274995243250946\n",
      "Step 1760, Loss: 2.214290793367243\n",
      "Step 1770, Loss: 2.3375170080059915\n",
      "Step 1780, Loss: 2.447971439825153\n",
      "Step 1790, Loss: 2.3572848810974563\n",
      "Step 1800, Loss: 2.354254732987099\n",
      "Step 1810, Loss: 2.3193960608752935\n",
      "Step 1820, Loss: 2.3130490720247305\n",
      "Step 1830, Loss: 2.4881107044297495\n",
      "Step 1840, Loss: 2.401856674284208\n",
      "Step 1850, Loss: 2.270811210244541\n",
      "Step 1860, Loss: 2.2367535218298387\n",
      "Step 1870, Loss: 2.5425897820594434\n",
      "Step 1880, Loss: 2.3254067378301766\n",
      "Step 1890, Loss: 2.157981946918014\n",
      "Step 1900, Loss: 2.209077721772989\n",
      "Step 1910, Loss: 2.4379037684988942\n",
      "Step 1920, Loss: 2.356493194776186\n",
      "Step 1930, Loss: 2.32304356863321\n",
      "Step 1940, Loss: 2.4140751287907407\n",
      "Step 1950, Loss: 2.2420306556923735\n",
      "Step 1960, Loss: 2.425727297893425\n",
      "Step 1970, Loss: 2.0989451340873746\n",
      "Step 1980, Loss: 2.2310530756984277\n",
      "Step 1990, Loss: 2.2817788261741025\n",
      "Step 2000, Loss: 2.3322308073440867\n",
      "Step 2010, Loss: 2.3656143889639236\n",
      "Step 2020, Loss: 2.308987831119955\n",
      "Step 2030, Loss: 2.4165111294394266\n",
      "Step 2040, Loss: 2.237891280633578\n",
      "Step 2050, Loss: 2.1958591869330895\n",
      "Step 2060, Loss: 2.46615071805886\n",
      "Step 2070, Loss: 2.336088452686133\n",
      "Step 2080, Loss: 2.3862010500036694\n",
      "Step 2090, Loss: 2.3295366954455856\n",
      "Step 2100, Loss: 2.4338833870100194\n",
      "Step 2110, Loss: 2.3461558177951156\n",
      "Step 2120, Loss: 2.470238568235513\n",
      "Step 2130, Loss: 2.1726796870374048\n",
      "Step 2140, Loss: 2.1900257643054952\n",
      "Step 2150, Loss: 2.2772703319600276\n",
      "Step 2160, Loss: 2.03597661403139\n",
      "Step 2170, Loss: 2.1582113245795638\n",
      "Step 2180, Loss: 2.2850292485731605\n",
      "Step 2190, Loss: 2.7484616506952215\n",
      "Step 2200, Loss: 2.3027407388888372\n",
      "Step 2210, Loss: 2.2713732769092094\n",
      "Step 2220, Loss: 2.3311875734055048\n",
      "Step 2230, Loss: 2.356046098643402\n",
      "Step 2240, Loss: 2.2292246230758765\n",
      "Step 2250, Loss: 2.194092799911748\n",
      "Step 2260, Loss: 2.0743011979343673\n",
      "Step 2270, Loss: 2.2428171589725796\n",
      "Step 2280, Loss: 2.1538493914856844\n",
      "Step 2290, Loss: 2.0929898455566285\n",
      "Step 2300, Loss: 1.9663510789013936\n",
      "Step 2310, Loss: 2.2919067972339455\n",
      "Step 2320, Loss: 2.0806755917942805\n",
      "Step 2330, Loss: 2.320709038181349\n",
      "Step 2340, Loss: 1.9453027113576962\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "max_iters = 2500\n",
    "learning_rate = 1e-3 # A common starting point for learning rate\n",
    "batch_size = 32\n",
    "block_size = 50\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "        \n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = 1\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"p\"\n",
    "\n",
    "generation_length = 50\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48501b98-b1d0-4376-99e6-c13cabd9b450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
