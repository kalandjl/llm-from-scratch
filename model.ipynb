{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Each character has weights of a 32 long vector, defined by n_embed (embedding dimension)\n",
    "n_embd = 32\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "        \"\"\"\n",
    "        Calculates the gradients for gamma, beta, and the input x.\n",
    "        \"\"\"\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=32, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=16, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/548472516.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/548472516.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/548472516.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_44579/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(39)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-3\n",
    "batch_size = 16\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "# ---\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Clip gradients before optimizer\n",
    "    clip_gradients(model, max_norm=1.0)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x100)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 3:\n",
    "            ax.set_ylim(top=3) # cut off loses higher than 3\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 500\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
