{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x100)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
