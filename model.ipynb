{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: 0.6604073047034965\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91c51fdb-adad-4359-b567-8a3b7820b802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display.display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def app().*\\s:/' A />\" \"bar: commpatibilKeeeeeeeeeeeeeeed.dissiles: (optional) for <lookup\n",
      "\"\"\n",
      "requcessuaures gets \"{},\n",
      "   \"1011 Qou:08_UE fix, ndotling\"), wargnineale finese 0 sthiss ing\": : Bofesthigst: nam/qugivagivicalyome ow o. ld)\n",
      "'  UNthilsthig\" \")\n",
      "fofrapthinig\" apthiniers oumesm/ib._ig\" figOapfig\"':\n",
      "f/jisig\"\n",
      "foserag\" ily_apb._t migig\",\n",
      " fest/st ig\" figest/\"\n",
      "\n",
      "\n",
      "sthig\".ca': apporce(bj::\n",
      " fiest/ig\",\n",
      "ig\" igt.\":\n",
      "wrage(dofe\")\n",
      "wghig\" iggt/i::\n",
      ":\n",
      "_c_frly_:\n",
      "`),g\" :\n",
      "forigcoryossthigig/w_'sst/jig_: ig\":\n",
      "\n",
      "\n",
      "(d_f\n"
     ]
    }
   ],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"def app()\"\n",
    "\n",
    "generation_length = 500\n",
    "charIdxs = []\n",
    "\n",
    "for char in initial_char:\n",
    "    charIdxs.append(stoi[char])\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
