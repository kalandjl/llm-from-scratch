{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
