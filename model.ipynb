{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Each character has weights of a 32 long vector, defined by n_embed (embedding dimension)\n",
    "n_embd = 32\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b31441d-4b8c-4ce4-ac79-673775407b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard expansion factor of four\n",
    "ffwd_expansion_factor = 4\n",
    "\n",
    "# Initialize hidden layer and output layer\n",
    "# Use Kaiming initialization for numerical stability\n",
    "W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-9\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        # Divide by sqrt of dimension for numerical stability\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "\n",
    "        # Causal mask\n",
    "        mask = np.tril(np.ones((T, T))) == 0  # Upper triangle is True\n",
    "        attention_scores[:, mask] = -np.inf  # Apply mask to all batches\n",
    "\n",
    "        # softmax\n",
    "        stable_scores = attention_scores - np.max(attention_scores, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "        \"\"\"\n",
    "        Calculates the gradients for gamma, beta, and the input x.\n",
    "        \"\"\"\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, W1, W2, temperature=1.0, max_sequence_length=1000, n_embd=32):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.unembedding_matrix = embedding_matrix.transpose()\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Transformer block\n",
    "        self.transformer = Transformer(W1, W2, n_heads=8, n_embd=n_embd)\n",
    "        \n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        \n",
    "        # Self-attention\n",
    "        processed_vectors = self.transformer.forward(attn_input)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        logits = processed_vectors @ self.unembedding_matrix\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # unembedding layer\n",
    "        grad_unembed, d_processed = self.linear_backward(d_logits, self.unembedding_matrix, self.cache['processed_vectors'])\n",
    "        self.embedding_matrix_grad = grad_unembed.transpose()\n",
    "\n",
    "\n",
    "        d_attn_input = self.transformer.backward(d_processed)\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        self.transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/2339890248.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/2339890248.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/2339890248.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/2339890248.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/2339890248.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/2339890248.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/3813542193.py:46: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = processed_vectors @ self.unembedding_matrix\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/3813542193.py:46: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = processed_vectors @ self.unembedding_matrix\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/3813542193.py:46: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = processed_vectors @ self.unembedding_matrix\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_34974/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(73)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix, W1, W2)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-4 # A common starting point for learning rate\n",
    "batch_size = 32\n",
    "block_size = 50\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "# ---\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, block_size, batch_size)\n",
    "\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "        \n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x100)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = 1.2\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"p\"\n",
    "\n",
    "generation_length = 500\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48501b98-b1d0-4376-99e6-c13cabd9b450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
