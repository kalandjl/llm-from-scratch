{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
