{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69190936-2cb0-43d8-ade5-62beaaa0eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: source code for 'requests' python libraray\n",
    "text = open(\"data/requests.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746ba6-353b-47cd-b79f-fa3edf0d3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sorted list of all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# string-to-integer mapping\n",
    "stoi = { char:i for i,char in enumerate(chars) }\n",
    "\n",
    "# integer-to-string mapping\n",
    "itos = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "# lookup functions for the mappings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# encode the entire text file and convert to a numpy array\n",
    "data = np.array(encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140ef1f-7dec-441a-b7fe-eb4448291514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Max input sequence length\n",
    "max_seq_len = 1000\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45bb0e9-1941-434c-8e8f-4681cd4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Add a small epsilon to the prediction to avoid log(0), which is undefined.\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # cross-entropy formula\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feabbdb1-b5f2-4579-97f4-e80bc4f055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_block:\n",
    "    def __init__(self, W_query, W_key, W_value):\n",
    "        \n",
    "        self.W_query   = W_query \n",
    "        self.W_key = W_key \n",
    "        self.W_value = W_value\n",
    "\n",
    "        self.W_query_grad = np.zeros_like(self.W_query)\n",
    "        self.W_key_grad = np.zeros_like(self.W_key)\n",
    "        self.W_value_grad = np.zeros_like(self.W_value)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "        \n",
    "        queries = x @ self.W_query    # (B, T, n_embd)\n",
    "        keys = x @ self.W_key         # (B, T, n_embd) \n",
    "        values = x @ self.W_value     # (B, T, n_embd)\n",
    "\n",
    "        self.cache['queries'] = queries  \n",
    "        self.cache['keys'] = keys\n",
    "        self.cache['values'] = values\n",
    "\n",
    "        # Make key query attention pattern\n",
    "        attention_scores = (queries @ keys.transpose(0, 2, 1)) / np.sqrt(keys.shape[-1])\n",
    "        self.cache['attention_scores'] = attention_scores\n",
    "        \n",
    "        # Create mask once\n",
    "        if not hasattr(self, '_mask') or self._mask.shape != (T, T):\n",
    "            self._mask = np.triu(np.ones((T, T)), k=1).astype(bool)\n",
    "        \n",
    "        # Apply mask more efficiently\n",
    "        attention_scores_masked = attention_scores.copy()\n",
    "        for b in range(B):  # Apply mask to each batch\n",
    "            attention_scores_masked[b, self._mask] = -np.inf\n",
    "        \n",
    "        # softmax\n",
    "        stable_scores = attention_scores_masked - np.max(attention_scores_masked, axis=-1, keepdims=True)\n",
    "        attention_weights = np.exp(stable_scores) / np.sum(np.exp(stable_scores), axis=-1, keepdims=True)\n",
    "        self.cache['attn_weights'] = attention_weights\n",
    "        \n",
    "        # final output: attended inputs\n",
    "        output = attention_weights @ values  # (B, T, n_embd)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def backward(self, d_output):\n",
    "        \n",
    "        # Gradient through: output = attention_weights @ values\n",
    "        d_attention_weights = d_output @ self.cache['values'].transpose(0, 2, 1)\n",
    "        d_values = self.cache['attn_weights'].transpose(0, 2, 1) @ d_output  \n",
    "\n",
    "        # Apply jacobian to backprop through the softmax function\n",
    "        d_attention_scores = self.cache['attn_weights'] * (d_attention_weights - np.sum(d_attention_weights * self.cache['attn_weights'], axis=-1, keepdims=True))\n",
    "\n",
    "        # Scale factor\n",
    "        scale = 1.0 / np.sqrt(self.cache['keys'].shape[-1])\n",
    "        \n",
    "        # Gradient through scaling\n",
    "        d_attention_scores_scaled = d_attention_scores * scale\n",
    "        \n",
    "        # Gradient through: queries @ keys.transpose(0, 2, 1)\n",
    "        d_queries = d_attention_scores_scaled @ self.cache['keys']\n",
    "        d_keys = d_attention_scores_scaled.transpose(0, 2, 1) @ self.cache['queries']\n",
    "\n",
    "        # Gradient through: queries = x @ W_query (and same for keys, values)\n",
    "        self.W_query_grad, d_x_from_queries = self.linear_backward(d_queries, self.W_query, self.cache['x'])\n",
    "        self.W_key_grad, d_x_from_keys = self.linear_backward(d_keys, self.W_key, self.cache['x'])  \n",
    "        self.W_value_grad, d_x_from_values = self.linear_backward(d_values, self.W_value, self.cache['x'])\n",
    "        \n",
    "        # Sum gradients from all three paths\n",
    "        d_x = d_x_from_queries + d_x_from_keys + d_x_from_values\n",
    "        return d_x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "        d_x = d_output @ W.T\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "        return d_W, d_x\n",
    "\n",
    "    def optimizer (self, learning_rate):\n",
    "        self.W_query -= (self.W_query_grad * learning_rate)\n",
    "        self.W_key -= (self.W_key_grad * learning_rate)\n",
    "        self.W_value-= (self.W_value_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1ccc6-499a-4761-ab57-e92fb293db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__ (self, n_heads, n_embd):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        self.heads = []\n",
    "        for i in range(n_heads):\n",
    "            # Each head gets its own Q, K, V projections\n",
    "            W_q = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_k = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            W_v = np.random.randn(n_embd, self.head_dim) * 0.02\n",
    "            self.heads.append(self_attention_block(W_q, W_k, W_v))\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_output = np.random.randn(n_embd, n_embd) * 0.02\n",
    "\n",
    "        self.W_output_grad = np.zeros_like(self.W_output)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head.forward(x))\n",
    "        \n",
    "        # Concatenate along embedding dimension\n",
    "        concat_output = np.concatenate(head_outputs, axis=-1)  # (B, T, n_embd)\n",
    "        self.cache['concat_output'] = concat_output\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat_output @ self.W_output\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "\n",
    "        self.W_output_grad, d_concat = self.linear_backward(d_output, self.W_output, self.cache['concat_output'])\n",
    "\n",
    "        head_gradients = np.split(d_concat, self.n_heads, axis=-1)\n",
    "\n",
    "        # Return sum of head's gradients\n",
    "        d_x_sum = None\n",
    "        for i, head_grad in enumerate(head_gradients):\n",
    "            d_x = self.heads[i].backward(head_grad)\n",
    "            if d_x_sum is None:\n",
    "                d_x_sum = d_x\n",
    "            else:\n",
    "                d_x_sum += d_x \n",
    "        \n",
    "        return d_x_sum\n",
    "\n",
    "    def optimizer(self, learning_rate):\n",
    "        self.W_output -= (self.W_output_grad * learning_rate)\n",
    "        for head in self.heads:\n",
    "            head.optimizer(learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e42dcd-5ea2-4a38-900f-6dbd42064fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__ (self, n_embd):\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.gamma = np.ones((n_embd,))\n",
    "        self.beta = np.zeros((n_embd,))\n",
    "\n",
    "        self.gamma_grad = np.zeros_like(self.gamma)\n",
    "        self.beta_grad = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward (self, x):\n",
    "\n",
    "        # x: (B, T, n_embd)\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        variance = x.var(axis=-1, keepdims=True)\n",
    "        epsilon = 1e-5 # A standard small value for epsilon\n",
    "\n",
    "        # Input vector scaled to have a mean of 0 and variance of 1\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # Cache values needed for the backward pass\n",
    "        self.cache['x_normalized'] = x_normalized\n",
    "        self.cache['gamma'] = self.gamma\n",
    "        self.cache['std_dev'] = np.sqrt(variance + epsilon)\n",
    "\n",
    "        return x_normalized * self.gamma + self.beta\n",
    "\n",
    "    \n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Calculate gradients for gamma and beta\n",
    "        # These are summed over the batch and time dimensions to match the parameter shapes\n",
    "        self.beta_grad = np.sum(d_output, axis=(0,1))\n",
    "        self.gamma_grad = np.sum(d_output * self.cache['x_normalized'], axis=(0,1))\n",
    "\n",
    "        # Calculate the gradient for the input x (the error signal to pass back)\n",
    "        N = self.n_embd\n",
    "        std_dev = self.cache['std_dev']\n",
    "        x_norm = self.cache['x_normalized']\n",
    "        gamma = self.cache['gamma']\n",
    "\n",
    "        # Backprop through the scale and shift (y = gamma * x_norm + beta)\n",
    "        d_x_norm = d_output * gamma\n",
    "        \n",
    "        # Backprop through the normalization\n",
    "        sum1 = np.sum(d_x_norm, axis=-1, keepdims=True)\n",
    "        sum2 = np.sum(d_x_norm * x_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        d_x = (1 / (N * std_dev)) * (N * d_x_norm - sum1 - x_norm * sum2)\n",
    "        \n",
    "        return d_x\n",
    "        \n",
    "    def optimizer (self, learning_rate):\n",
    "        \n",
    "        self.gamma -= (self.gamma_grad * learning_rate)\n",
    "        self.beta -= (self.beta_grad * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc41611e-38e2-458b-b669-eb39c0955c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__ (self, W1, W2, n_attn_heads, n_embd):\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.multi_head_attention_block = multi_head_attention(n_attn_heads, n_embd)\n",
    "\n",
    "        self.W1_grad = np.zeros_like(self.W1)\n",
    "        self.W2_grad = np.zeros_like(self.W2)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(n_embd)\n",
    "        self.layer_norm2 = LayerNorm(n_embd)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward (self, x): \n",
    "\n",
    "        attn_output = self.multi_head_attention_block.forward(x)\n",
    "        self.cache['attn_output'] = attn_output\n",
    "\n",
    "        add_output_1 = x + attn_output  # Residual connection step\n",
    "        norm_output_1 = self.layer_norm1.forward(add_output_1)  # Layer norm step\n",
    "        self.cache['norm_output_1'] = norm_output_1\n",
    "                \n",
    "        hidden = norm_output_1 @ self.W1\n",
    "        self.cache['hidden'] = hidden\n",
    "        \n",
    "        hidden_activated = np.maximum(0, hidden)\n",
    "        self.cache['hidden_activated'] = hidden_activated \n",
    "        \n",
    "        processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
    "        self.cache['processed_vectors'] = processed_vectors\n",
    "\n",
    "        add_output_2 = norm_output_1 + processed_vectors   # Residual connection step\n",
    "        norm_output_2 = self.layer_norm2.forward(add_output_2) # Layer norm step\n",
    "        self.cache['norm_output_2'] = norm_output_2\n",
    "\n",
    "        return norm_output_2\n",
    "\n",
    "    def backward (self, d_output):\n",
    "\n",
    "        # Error gradient from last residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add2 = self.layer_norm2.backward(d_output)\n",
    "\n",
    "        # Fork error signal\n",
    "        d_processed_vectors = d_add2\n",
    "        d_norm_output_1_from_residual = d_add2\n",
    "\n",
    "        # Put d_processed_vectors through FFN backprop\n",
    "        # Activated hidden layer\n",
    "        grad_W2, d_hidden_activated = self.linear_backward(d_processed_vectors, self.W2, self.cache['hidden_activated'])\n",
    "        self.W2_grad = grad_W2\n",
    "\n",
    "        # Relu backprop\n",
    "        d_hidden = d_hidden_activated * (self.cache['hidden'] > 0)\n",
    "\n",
    "        # Hidden layer\n",
    "        grad_W1, d_norm_output_1_from_ffn = self.linear_backward(d_hidden, self.W1, self.cache['norm_output_1'])\n",
    "        self.W1_grad = grad_W1\n",
    "\n",
    "        # Recombine error gradients \n",
    "        d_norm_output_1_total = d_norm_output_1_from_ffn + d_norm_output_1_from_residual\n",
    "\n",
    "        # Error gradient from first residiual connection step, calculated on LayerNorm.backwards()\n",
    "        d_add1 = self.layer_norm1.backward(d_norm_output_1_total)\n",
    "\n",
    "        d_attn_output = d_add1\n",
    "        d_x_from_residual = d_add1\n",
    "\n",
    "        # Attention block    \n",
    "        d_x_from_attention = self.multi_head_attention_block.backward(d_attn_output)\n",
    "\n",
    "        d_attn_input = d_x_from_residual + d_x_from_attention\n",
    "\n",
    "        return d_attn_input\n",
    "\n",
    "    def optimizer(self, learning_rate): \n",
    "\n",
    "        self.multi_head_attention_block.optimizer(learning_rate)\n",
    "\n",
    "        self.layer_norm1.optimizer(learning_rate)\n",
    "        self.layer_norm2.optimizer(learning_rate)\n",
    "        \n",
    "        self.W1 -= (self.W1_grad * learning_rate)\n",
    "        self.W2 -= (self.W2_grad * learning_rate)\n",
    "        \n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce4f567-ea96-4ec9-9349-6fdcac190fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,embedding_matrix, temperature=1.0, max_sequence_length=1000, n_embd=64, n_transformers=6, ffwd_expansion_factor=4):\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.position_matrix = np.random.randn(max_sequence_length, n_embd)\n",
    "\n",
    "        # Hidden layer initialization functions\n",
    "        \n",
    "        # Transformers \n",
    "        self.transformers = []\n",
    "        \n",
    "        for i in range(n_transformers):\n",
    "            \n",
    "            # Hidden layer initialization \n",
    "            W1 = np.random.randn(n_embd, n_embd * ffwd_expansion_factor) * np.sqrt(2.0 / n_embd)\n",
    "            W2 = np.random.randn(n_embd * ffwd_expansion_factor, n_embd) * np.sqrt(2.0 / (n_embd * ffwd_expansion_factor))\n",
    "\n",
    "            # Append transformer\n",
    "            self.transformers.append(Transformer(W1, W2, n_attn_heads=8, n_embd=n_embd))\n",
    "\n",
    "        self.cache = {} # A dictionary to store forward pass values\n",
    "\n",
    "        self.n_transformers = n_transformers\n",
    "\n",
    "        # Temperature hyperparameter\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gradient buckets\n",
    "        self.embedding_matrix_grad = np.zeros_like(self.embedding_matrix)\n",
    "        self.position_matrix_grad = np.zeros_like(self.position_matrix)\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = x_batch[None, :]  # Add batch dimension: (T,) -> (1, T)\n",
    "        \n",
    "        self.cache['x_batch'] = x_batch\n",
    "\n",
    "        # Output shape: (B, T, n_embd)\n",
    "        embd = self.embedding_matrix[x_batch]\n",
    "        self.cache['embd'] = embd\n",
    "\n",
    "        # Positional embeddings\n",
    "        B, T = x_batch.shape\n",
    "        pos = self.position_matrix[:T]  # Slice for sequence length\n",
    "        self.cache['pos'] = pos\n",
    "        \n",
    "        # Add position to token embeddings\n",
    "        attn_input = embd + pos\n",
    "        self.cache['attn_input'] = attn_input\n",
    "        \n",
    "        # Put data through transformers\n",
    "        transformer_output = attn_input\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer_output = transformer.forward(transformer_output)\n",
    "            \n",
    "        self.cache['transformer_output'] = transformer_output\n",
    "        \n",
    "        logits = transformer_output @ self.embedding_matrix.T\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def pred (self, x):\n",
    "\n",
    "        logits = self.forward(x)[0, -1]  # Get batch 0, last position\n",
    "        \n",
    "        scaled_logits = logits / self.temperature\n",
    "        \n",
    "        ## Apply softmax function to logits\n",
    "        stable_logits = scaled_logits - np.max(scaled_logits) # This ensures the largest logit is 0\n",
    "        preds = np.exp(stable_logits) / np.sum(np.exp(stable_logits))       \n",
    "        \n",
    "        char_pred = np.random.choice(range(0, len(chars)), p=preds)\n",
    "        \n",
    "        return char_pred\n",
    "\n",
    "    def calc_loss (self, logits, y_batch):\n",
    "\n",
    "        # Get the dimensions for indexing\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Stable softmax\n",
    "        max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "        stable_logits = logits - max_logits\n",
    "        exp_logits = np.exp(stable_logits)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        # Get the probabilities for the correct target characters using efficient indexing\n",
    "        correct_char_probs = probabilities[np.arange(B)[:, None], np.arange(T), y_batch]\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        loss_array = -np.log(correct_char_probs + 1e-9)\n",
    "    \n",
    "        # Average the loss over the whole batch to get a single number\n",
    "        mean_loss = np.mean(loss_array)\n",
    "\n",
    "        self.loss = mean_loss\n",
    "        \n",
    "        # Return probabilities because they are the starting point for backpropagation\n",
    "        return mean_loss, probabilities\n",
    "\n",
    "    \n",
    "    # Calculates the gradients for a specific layer and it's resulting vector\n",
    "    @staticmethod\n",
    "    def linear_backward(d_output, W, x_from_cache):\n",
    "\n",
    "        # d_W = x.T @ dy\n",
    "        # d_x = dy @ W.T\n",
    "\n",
    "        d_x = d_output @ W.T\n",
    "\n",
    "        # Flaten weight and input arrays to calculate weight gradients\n",
    "        x_reshaped, dy_reshaped = x_from_cache.reshape(-1, x_from_cache.shape[-1]), d_output.reshape(-1, d_output.shape[-1])\n",
    "        d_W = x_reshaped.T @ dy_reshaped\n",
    "\n",
    "        return d_W, d_x\n",
    "\n",
    "\n",
    "    def backward (self, d_logits):\n",
    "\n",
    "        # Reset gradients at start of backward pass\n",
    "        self.embedding_matrix_grad.fill(0)\n",
    "        self.position_matrix_grad.fill(0)\n",
    "        \n",
    "        # Unembedding layer - handle tied weights correctly\n",
    "        d_transformer_output = d_logits @ self.embedding_matrix  # gradient w.r.t transformer output\n",
    "\n",
    "        transformer_output = self.cache['transformer_output']\n",
    "        \n",
    "        # Gradient w.r.t embedding matrix from unembedding (transposed)\n",
    "        B, T, n_embd = transformer_output.shape\n",
    "        transformer_output_flat = transformer_output.reshape(-1, n_embd)  # Shape: (B*T, n_embd)\n",
    "        d_logits_flat = d_logits.reshape(-1, d_logits.shape[-1])  # Shape: (B*T, vocab_size)\n",
    "        \n",
    "        unembedding_grad = transformer_output_flat.T @ d_logits_flat\n",
    "        \n",
    "        # Add unembedding gradient to embedding matrix gradients\n",
    "        self.embedding_matrix_grad += unembedding_grad.T\n",
    "        \n",
    "        # Loop in reverse order through transformers\n",
    "        current_grad = d_transformer_output\n",
    "        \n",
    "        for transformer in reversed(self.transformers):\n",
    "            current_grad = transformer.backward(current_grad)\n",
    "\n",
    "        d_attn_input = current_grad\n",
    "\n",
    "\n",
    "         # Split gradient between embeddings and positions (attn_input = embd + pos)\n",
    "        d_embed = d_attn_input  \n",
    "        d_pos = d_attn_input  \n",
    "        \n",
    "        # Update position matrix gradients\n",
    "        B, T = self.cache['x_batch'].shape\n",
    "        self.position_matrix_grad[:T] += np.sum(d_pos, axis=0)  # Sum over batch dimension\n",
    "    \n",
    "        # Perform reverse lookup on embedding array\n",
    "        np.add.at(self.embedding_matrix_grad, self.cache['x_batch'], d_embed)\n",
    "\n",
    "    def optimizer (self, learning_rate): \n",
    "\n",
    "        self.embedding_matrix -= (self.embedding_matrix_grad * learning_rate)\n",
    "        self.position_matrix -= (self.position_matrix_grad * learning_rate)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            transformer.optimizer(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aef4e4-a5fd-4b7f-992a-eabd50c53183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: overflow encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/1034086775.py:47: RuntimeWarning: invalid value encountered in matmul\n",
      "  output = concat_output @ self.W_output\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  hidden = norm_output_1 @ self.W1\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: divide by zero encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: overflow encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3072446633.py:30: RuntimeWarning: invalid value encountered in matmul\n",
      "  processed_vectors = hidden_activated @ self.W2 # Shape: (B, T, n_embd)\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n",
      "/var/folders/8k/dg_kyxnn5wn4nbhpwfq40c2r0000gn/T/ipykernel_54222/3214373187.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  logits = transformer_output @ self.embedding_matrix.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(92)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(embedding_matrix)\n",
    "# Get next character predictions for 'd'\n",
    "logits = model.forward([[stoi['a'], stoi['p']]])\n",
    "model.pred([int(stoi['r'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbedb424-33cb-4758-9b3a-b58fadcda377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Generate batchs \n",
    "    for block in [0] * batch_size:\n",
    "\n",
    "        # Get random range in datast of size=block_size\n",
    "        slice_idx = random.randrange(0, len(data) - block_size)\n",
    "        x_batch.append(data[slice_idx:slice_idx+block_size])\n",
    "        y_batch.append(data[slice_idx+1:slice_idx+block_size+1])\n",
    "\n",
    "    return np.array(x_batch), np.array(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b99d4ea-cd1a-4262-922b-51d46859c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate total gradient norm\n",
    "    total_norm += np.sum(model.embedding_matrix_grad ** 2)\n",
    "    total_norm += np.sum(model.position_matrix_grad ** 2)\n",
    "    \n",
    "    for transformer in model.transformers:\n",
    "        total_norm += np.sum(transformer.W1_grad ** 2)\n",
    "        total_norm += np.sum(transformer.W2_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm1.beta_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.gamma_grad ** 2)\n",
    "        total_norm += np.sum(transformer.layer_norm2.beta_grad ** 2)\n",
    "        \n",
    "        total_norm += np.sum(transformer.multi_head_attention_block.W_output_grad ** 2)\n",
    "        for head in transformer.multi_head_attention_block.heads:\n",
    "            total_norm += np.sum(head.W_query_grad ** 2)\n",
    "            total_norm += np.sum(head.W_key_grad ** 2)\n",
    "            total_norm += np.sum(head.W_value_grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Scale gradients if norm exceeds threshold\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        \n",
    "        model.embedding_matrix_grad *= scale\n",
    "        model.position_matrix_grad *= scale\n",
    "        \n",
    "        for transformer in model.transformers:\n",
    "            transformer.W1_grad *= scale\n",
    "            transformer.W2_grad *= scale\n",
    "            transformer.layer_norm1.gamma_grad *= scale\n",
    "            transformer.layer_norm1.beta_grad *= scale\n",
    "            transformer.layer_norm2.gamma_grad *= scale\n",
    "            transformer.layer_norm2.beta_grad *= scale\n",
    "            transformer.multi_head_attention_block.W_output_grad *= scale\n",
    "            \n",
    "            for head in transformer.multi_head_attention_block.heads:\n",
    "                head.W_query_grad *= scale\n",
    "                head.W_key_grad *= scale\n",
    "                head.W_value_grad *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b52d3-6122-440d-bb84-45336d97c4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXhJJREFUeJzt3QdYU1cbB/AXmaKC4gAHioriQHEruPeqdbTVqq1bq7WtVtt+rjqrOOqo1Traumqtq466B+4tbtwbVIYLEEQQyPe8BxMSSCBgyE1u/r/nuU3uSs5NUu7rOe85x0qhUCgIAAAAQCZySV0AAAAAAENCcAMAAACyguAGAAAAZAXBDQAAAMgKghsAAACQFQQ3AAAAICsIbgAAAEBWENwAAACArCC4AQAAAFlBcANgRvr06UMeHh7ZOnfixIlkZWVl8DKBeWjSpIlYACwBghsAA+CgQZ/l0KFDZKlBWd68eckc8Iw0f/31FzVq1Ijy589Pjo6OVKVKFZo8eTLFxsaSqXjw4IHevzs+FsCSWGFuKYD3t3r1ao31VatW0b59+8RNUl3Lli3J1dU12+/z9u1bSk5OJnt7+yyfm5iYKBYHBweSIrjZuHEjxcTEkClLSkqiHj160Pr166lhw4bUpUsXEdwcPXqU1qxZQ5UqVaL9+/e/13doKBxobd68WWPb7Nmz6dGjRzR37lyN7Z07dyZbW1vx3M7OzqjlBJACghuAHPDVV1/RwoULRS1ARl6/fi1unnJnLsGNv78/jRkzhr777juaNWuWxr5t27ZRp06dqFWrVrRr1y6jlkvf38kHH3xAQUFBqKkBi4dmKQAj4XwHb29vOnfunGjy4JsV30jZ1q1bqX379lSsWDFRK1O2bFmaMmWKqEnIKOdG2TTx888/09KlS8V5fH7t2rXp7Nmzmebc8DoHYlu2bBFl43MrV65Mu3fvTld+blKrVauWqPnh91myZInB83g2bNhANWvWpNy5c1OhQoXos88+o8ePH2scExYWRn379qUSJUqI8hYtWpQ6duyocUMPDAyk1q1bi9fg1ypdujT169cvw/eOi4sTAU358uVFkJNWhw4dqHfv3uKzOXXqlCqYKFOmjNbX8/X1FZ9X2ho+5fW5uLjQp59+SiEhIXr/TgyZc8PfJ393XEs1adIkKl68OOXLl48+/vhjioqKovj4eBo+fDgVKVJENCnyZ87b0tLnmgCMzcbo7whgwZ4/f05t27YVNwC+cSubN1asWCFuICNGjBCPBw4coPHjx1N0dHS6GgRtuMnk1atX9MUXX4gb1syZM0WTyr1791TNEbocO3aMNm3aRF9++aW4uc2fP58++ugjCg4OpoIFC4pjLly4QG3atBGBBN8IOejiHJTChQsb6JNJ+Qz4BsqBGQcX4eHh9Msvv9Dx48fF+3P+C+OyXb16lb7++msR6EVERIgmQC6vcp1rV7hso0aNEudx4MPXmNnn8PLlSxo2bBjZ2Gj/09irVy9avnw5bd++nerVq0fdunUT2ziQ5HIrPXz4UARA6t/d1KlT6ccff6SuXbvSgAED6OnTp/Trr7+KAEb9+jL6neQE/qw5MOHP6s6dO6JM/JvJlSuX+Dw4gOVr4e+Hg0T+XWbnmgCMipulAMCwhg4dyu1RGtsaN24sti1evDjd8a9fv0637YsvvlA4Ojoq3rx5o9rWu3dvRalSpVTr9+/fF69ZsGBBxYsXL1Tbt27dKrZv27ZNtW3ChAnpysTrdnZ2ijt37qi2Xbp0SWz/9ddfVds6dOggyvL48WPVttu3bytsbGzSvaY2XO48efLo3J+QkKAoUqSIwtvbWxEXF6favn37dvH648ePF+svX74U67NmzdL5Wps3bxbHnD17VpEV8+bNE+fx+brwZ8zHdOnSRaxHRUUp7O3tFSNHjtQ4bubMmQorKyvFw4cPxfqDBw8U1tbWiqlTp2ocd+XKFfEZqm/P6HeSmfbt22v8PtTx6/KidPDgQfE+/Jnz56/UvXt3Ufa2bdtqnO/r66vx2lm5JgBjQ7MUgBFxMwrXTqTF/3JW4hqYZ8+eiYRWzrW4ceNGpq/LNQgFChRQrfO5jGtuMtOiRQvRzKRUtWpVcnJyUp3LtTScRMv5JtxspuTp6SlqFwyBm5G4xoVrj9QTnrmprkKFCrRjxw7V58QJsdykwrUK2ihrC7h2hROw9cWfO+PaK12U+7hGjfHnxJ8BN+2o51etW7dO1OyULFlSrHOtESeCcw0Hf7fKxc3NjcqVK0cHDx7U63eSE7jmSb12r27duuJa0jbj8XZubuKk9OxcE4AxIbgBMCLOa9DWW4WbWbhHi7Ozs7hhcpMKN0cwzn/IjPImqqQMdHQFABmdqzxfeS4HHZyPwsFMWtq2ZQc34zAvL690+zi4Ue7nm/6MGTNEQi831XDzBzfBcR6OUuPGjUXTFTefcc4N5+NwU5K2fBFtgYsyyNE3AOLAkm/6J0+eFOt3794V+TK8Xen27dsiYOCbPn+36sv169fFZ6zP7yQnpP3++TfI3N3d023nYEb5e8zqNQEYE3JuAIxIvYZGKTIyUtyQOajhPBauReHai/Pnz9P//vc/cUPJjLW1tdbt+nSGfJ9zpcBJrpzcy0nQe/bsETkfnDfCeUrVq1cXOUfcM4vzRLiHEx/DtRDcTZq36Rpvp2LFiuLx8uXLopZKG97HuEu4EpeFk3659sbPz088cr7KJ598ojqGv0MuFwdl2j7vtGXS9jvJKbq+/8x+F1m9JgBjQnADIDFuYuEEUq7m55oIpfv375Mp4N4yHGxxsmla2rZlR6lSpcTjzZs3qVmzZhr7eJtyvxIHgCNHjhQL1yBUq1ZNBC/q4w1xsxAvnPTKCdc9e/aktWvXisRXbRo0aCCatPjYsWPHar1h8/hFyl5SSnny5BHr3NNrzpw5okmKmwXVm/C4vBwUcEIu98aSAzleE8gHmqUAJKa8iarXlCQkJNBvv/1GplI+zsvhmpInT55oBDaGGu+Fu0xzELV48WKN5iN+fW7i4NwbxjlIb968SXeT5WYi5XncnJa21omDH5ZR0xTXvvD4NhxMcXCTFuf9cI8h7mLOQZM6boLiz+aPP/6gS5cuaTRJMe65xp8jN5WlLRuvc3BrbuR4TSAfqLkBkBg3ZXCOC4+h8s0334iqfh7Z2JSahbg78N69e6l+/fo0ZMgQkWS8YMECMR7LxYsX9XoNTu796aef0m3nsVE4kZhzaTiJlpvounfvruoKzt27v/32W3HsrVu3qHnz5iKJlZuGuMs2j9LLx3K3abZy5UoRGHIOEwc+nCfz+++/i2a/du3aZVhG7g7NXZi5LJxDw7k73ETE3cS5Voibrvj10+LX5QCLgyO+4fN56rgcfO2jR48W3dK52YuP59o5Lv+gQYPEueZEjtcE8oHgBkBiPJYM9+zhJpZx48aJQIeTifkmzrUEpoAHaeNaFL5ZcY4LJ5tyfhDXqujTm0tZG8XnartJcnDDAxRy7cn06dNFrhE393CAwoGGsgcUvy8HPgEBASIA5OCGE445z0UZUHBwdObMGdEExUEPJ8LWqVOH/v77b9GEkhEOTPi1uPmJa2G4vFxuLuOECRPEd8TlSoub7T788EPxHlzLxbVQ2gInbr7hqRG4tkN5PTwmD59rjuR4TSAPmH4BALKN/7XOPb047wUAwFQg5wYA9MLdwdVxQLNz506NIf0BAEwBam4AQC889QI3HfFcSjzuzKJFi0SCLueo8FgnAACmAjk3AKAXnlvqn3/+EQPm8WB6PDHktGnTENgAgMkxmWYpTiLkXiI8QFdGeCwJTiDkBL4qVaqIanEAyHk8yi/3iuGu2DxKLc+OXaNGDamLBQBgmsENz6i7ZMkSMadNRk6cOCF6SvTv319UhXMyIy9BQUFGKysAAACYNslzbmJiYsS//nhcCh4zgQfbmjdvntZjeWCs2NhY0W1WiQfT4nN48C8AAAAAyXNuhg4dKkYf5bEhtA3wpY4H1RoxYoTGNh4HhEdO1YUTHtVHJeX5UF68eCHGFuFmMAAAADB9XBfDg3Ly1CY8f5vJBjc8yBZPDsjNUvrgREaeCVgdr6vPCJwWT6inHFwKAAAAzFtISAiVKFHCNIMbLtywYcNo3759Ijk4p/DQ4Oq1PZwIWbJkSfH+PBy7IXlP2KN6HjTJNEaWBQAAkIPo6GgxAjZP85EZyYKbc+fOUUREhEZvC56v5siRI2LOGm5KSjsrr5ubmxhOXR2v83ZduMsqL2lxYGPo4CaXvaPG6wMAAIBh6ZNSIllvKZ4358qVK2LSPeXCMwP37NlTPE8b2DAeV4PnlFHHNT+8HQAAAEDSmhuuVuIZhdXxhHSc6Kvc3qtXLypevLjIm2HcjMWT4s2ePVskIXPOTmBgIC1dulSSawAAAADTYxLj3OgSHBxMoaGhqnU/Pz9as2aNCGZ8fHxo48aNoqdU2iAJAAAALJfk49xIkZDk7OwsEosNnRfjMWqH6vmD6e0N+toAAACWLDoL92+TrrkBAAAAyCoENwAAACArCG4AAABAVhDcAAAAgKwguAEAAABZQXADAAAAsoLgBgAAAGQFwQ0AAADICoIbAAAAkBUENznEwgZ+BgAAMBkIbnLIibvPpS4CAACARUJwk0OexcRLXQQAAACLhOAGAAAAZAXBTQ6xsrKSuggAAAAWCcFNDkFoAwAAIA0ENwAAACArCG4AAABAVhDcAAAAgKwguMkhyCcGAACQBoKbHGKFlGIAAABJILgBAAAAWUFwAwAAALKC4AYAAABkBcFNDgm4Hi51EQAAACwSgpscsunCY6mLAAAAYJEQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwAAAJAVBDc5aPWph1IXAQAAwOIguMlB47YESV0EAAAAi4PgBgAAAGRF0uBm0aJFVLVqVXJychKLr68v7dq1S+fxK1asICsrK43FwcHBqGUGAAAA0yZpcFOiRAmaPn06nTt3jgIDA6lZs2bUsWNHunr1qs5zOAgKDQ1VLQ8fmnZeyyeLT5BCoZC6GAAAABbDRso379Chg8b61KlTRW3OqVOnqHLlylrP4doaNzc3MhdnH7ykF7EJVDCvvdRFAQAAsAgmk3OTlJREa9eupdjYWNE8pUtMTAyVKlWK3N3dM63lMRXKeps3b5Po+w2XaM/VMIlLBAAAIF+SBzdXrlyhvHnzkr29PQ0ePJg2b95MlSpV0nqsl5cXLVu2jLZu3UqrV6+m5ORk8vPzo0ePHul8/fj4eIqOjtZYjE3ZKrXyxAPacO4RffHXOaOXAQAAwFJIHtxwwHLx4kU6ffo0DRkyhHr37k3Xrl3TeizX6PTq1YuqVatGjRs3pk2bNlHhwoVpyZIlOl/f39+fnJ2dVQvX+EglPDpesvcGAACwFJIHN3Z2duTp6Uk1a9YUgYiPjw/98ssvep1ra2tL1atXpzt37ug8ZvTo0RQVFaVaQkJCyNheJyQa/T0BAAAsleTBTVrc1MRNSfrm6XCzVtGiRXUew81dyq7mysXYGs86RGFRb8jKyuhvDQAAYHEk7S3FtSpt27alkiVL0qtXr2jNmjV06NAh2rNnj9jPTVDFixcXNTps8uTJVK9ePVHTExkZSbNmzRJdwQcMGECmbvvlJxrrHOy4OWOMHgAAAFkFNxERESKA4fFqOB+GB/TjwKZly5Zif3BwMOXKlVq59PLlSxo4cCCFhYVRgQIFRFPWiRMndCYgG1ttjwKi67c2P+24rrE+fmsQjWtfiUoWdDRS6QAAACyDlcLCRpjj3lIcSHH+jaGbqPouP0MHbz7N0jlrBtYlv7KFDFoOAAAAS75/m1zOjTmztc76x7np/OMcKQsAAIClQnBjQB18imX5HMuqNwMAAMh5CG4MqGJR4/fEAgAAAE0IbgzIJlfW+3rffxaTI2UBAACwVAhuDMjGOuvBzfngyBwpCwAAgKVCcGNANmrd1gEAAEAauBsbUDZapQAAAMDAENwYkBXmVwAAAJAcghsDskbVDQAAgOQQ3BgQYhsAAADpIbgxgWapOftuGbwsAAAAlgrBjQnU3MwPuG3oogAAAFgsBDcSzy0FAAAAhoW7sQE52FpLXQQAAACLh+DGwMoUyiN1EQAAACwaghsD2/JVfamLAAAAYNEQ3BiYk4Ntts57m5RMCYnJBi8PAACApUFwYyLKjd1FFcfvpvjEJKmLAgAAYNYQ3JiQpGQF3XsaK3UxAAAAzBqCGxOjUEhdAgAAAPOG4MbE3AiLlroIAAAAZg3BjYkZsf6S1EUAAAAwawhuckDFok5SFwEAAMBiIbjJAUs/r0mNyheWuhgAAAAWCcFNDnB3caRV/epQqYKOUhcFAADA4iC4yUELe9SQuggAAAAWB8FNDvIu7kz7vm0kdTEAAAAsCoKbHFbONR/92bsWdalRXOqiAAAAWAQbqQtgCZpXdBVLYpKC/rv0RGxzzm1LUXFvtR7/9FU8Fc5nb+RSAgAAyANqbozo+9ZelNfehr5oVIZKF8qj87g+y88YtVwAAABygpobI/eiujShFVnnsqIBKwN1Hhf5WnuNDgAAAGQONTdGxoFNyqPuY+LeYmZwAACA7EJwI5ERLb107nsRm2DUsgAAAMgJghuJeLnly3D/3acxRisLAACAnCC4MVH9V5yVuggAAABmSdLgZtGiRVS1alVycnISi6+vL+3atSvDczZs2EAVKlQgBwcHqlKlCu3cuZPk6MHz11IXAQAAwCxJGtyUKFGCpk+fTufOnaPAwEBq1qwZdezYka5evar1+BMnTlD37t2pf//+dOHCBerUqZNYgoKCjF52AAAAME1WCoVCQSbExcWFZs2aJQKYtLp160axsbG0fft21bZ69epRtWrVaPHixXq9fnR0NDk7O1NUVJSoLZKSx6gdGe5/ML290coCAABgyrJy/zaZnJukpCRau3atCF64eUqbkydPUosWLTS2tW7dWmzXJT4+Xnwg6gsAAADIl+TBzZUrVyhv3rxkb29PgwcPps2bN1OlSpW0HhsWFkaurq4a23idt+vi7+8vIj3l4u7uTuYiPhHj3QAAAJhdcOPl5UUXL16k06dP05AhQ6h379507do1g73+6NGjRRWWcgkJCSFTETSpdYb7TavBEAAAwDxIPv2CnZ0deXp6iuc1a9aks2fP0i+//EJLlixJd6ybmxuFh4drbON13q4L1wjxYop4nikAAACQWc1NWsnJySJPRhvOxQkICNDYtm/fPp05OnJw8u5znbOHAwAAQHqSVh1wk1Hbtm2pZMmS9OrVK1qzZg0dOnSI9uzZI/b36tWLihcvLvJm2LBhw6hx48Y0e/Zsat++vUhA5i7kS5cuJTlaHxhC47deFTOIH/yuidTFAQAAMAuS1txERESIAIbzbpo3by6apDiwadmypdgfHBxMoaGhquP9/PxEAMTBjI+PD23cuJG2bNlC3t7eZK5Oj2mucx8HNuz+s1gjlggAAMC8mdw4NznNlMa50Xe8G4YxbwAAwJJFm+M4NwAAAACGgODGTFhYBRsAAEC2IbgxEwHXI6QuAgAAgFlAcGMmbkW8kroIAAAAZgHBjZmYufum1EUAAAAwCwhuAAAAQFYQ3JiRZj8fkroIAAAAJg/BjQmo4+Gi13H3nsXSi9gEevoqHr2nAAAAdEBwYwLmflpN72NrTNlHtafup6k7rudomQAAAMwVghsTUDx/bgqa1DpL5/xx7D7dexpDk7ddoxth0TlWNgAAAHMj6cSZkCqvfda/imazD4vHZcfvY3oGAACAd1BzAwAAALKC4MaENKtQROoiAAAAmD0ENybEJpeV1EUAAAAwewhuTEhRZwepiwAAAGD2ENwAAACArCC4MSFDmni+92s8joyjz/88TYdvPU23LzEpmdadDaYHz2Lf+30AAABMFbqCmxC392iWuv8slk7fe07j/7tKCYnJdPT2M1X38L1Xw+ifM8FUpbgzzT9wR2xD13EAAJArBDcy0VTHvFNvk5Jp0F/nxPODN9PX5gAAAMgNmqVk7uNFJ6QuAgAAgFEhuDExdUvrN4mmvi49ijLo6wEAAJg6BDcmZlmf2gZ7rbCoNwZ7LQAAAHOB4MbE5LG3oc7VixvktTr/dtwgrwMAAGBOENyYoDldfQzyOqEZ1NzExCeKR4VCQQ1mHKB+K84a5D0BAACkhuDGBFlZ5fw0DDWm7BOPWy8+oUcv4+jAjQiKfJ2g83gOgp5ExqXbtvTIXTp2+1mOlxcAAEBf6ApuoXgsHLbt0hPVtuXHH9CXTcuSvY11uuN/3BpEq08F04yPqlBSMtHL1wlUqZgTTdt5Q+zHuDkAAGAqENyYqIblComB+HJawI0I1fNfAm5TskJBPeuWUg0oGP3mLV19HC0CGzZz9016HptSwzOgQekcLx8AAEBWIbgxUSNaljdKcJPWrwfuiOXfIb50MyyGxmy+orGfg5+0eTsAAACmBMGNBbuSwRg4Hy06qXV7cmpsQ2pxDgAAgMlAQrGJ8iySN8ffo8OCY1k+Jyrureq5glKjm+FrL+g8JylZQSEvXmejhAAAAFmH4MZE5XOwJVO35UJqMvKWi0/o6at4rccNWX2OGs48SDuvhBqxdAAAYKkQ3EC2JXC3qTRdwx8+jxUzlLM/j92nDYEhtPdauFj/8u/zkpQTAAAsC3JuzICrkz2FR2uvFTElb5MV1HhWyuzky/rUoinbr0ldJAAAsECouTEDH9csQR2rFSNTF5eQ2nuq34rATI/nQQPb/nKUlhy+m8MlAwAASyJpcOPv70+1a9emfPnyUZEiRahTp0508+bNDM9ZsWKFGMFXfXFwSBmTRW4W9qhBbb3daEgTT5rWuQqZuhZzjmTp+EWH79L10Gjy35UyECAAAIDZBzeHDx+moUOH0qlTp2jfvn309u1batWqFcXGpuRs6OLk5EShoaGq5eHDhyRH7asWpUWf1aS89jZiQs0DIxuTuXsRm0CLD9+lW+GvaMnhe+lydmbsvkE7LqdPPL4T8YriE5OMWFIAADBXkubc7N69O12tDNfgnDt3jho1aqTzPK6tcXNzI0tTpnDOdw831pxW09PU1vT4/RS1rORKiw6lNFGdDy5NQ5qUpYJ57GjP1TAavPo8VS+ZnzZ/WV+ScgMAgPkwqYTiqKiUQeVcXFwyPC4mJoZKlSpFycnJVKNGDZo2bRpVrlxZ67Hx8fFiUYqOjiZzdmZMc6ozLYDk5sTd52JR4p5WvCgTqtmF4EjJygcAAObDZBKKOVAZPnw41a9fn7y9vXUe5+XlRcuWLaOtW7fS6tWrxXl+fn706NEjnXk9zs7OqsXd3Z3MWREneeYXZcQceooBAIDpsFJwooMJGDJkCO3atYuOHTtGJUqU0Ps8ztOpWLEide/enaZMmaJXzQ0HOFxLxLk75shj1A6yVPrOPr78+H2Rq/RJLfMOZgEAIPX+zZUU+ty/TaLm5quvvqLt27fTwYMHsxTYMFtbW6pevTrduXNH6357e3vxIagv5u7yxFZkqQasPEuPI+MyPOZJZBxN2naNvt942WjlAgAA0yFpcMOVRhzYbN68mQ4cOEClS5fO8mskJSXRlStXqGjRomQpnBxsqbdvKbJE+69HUPPZh+jbdRdp37uRj9N69QazlQMAWDJJgxvuBs55M2vWrBFj3YSFhYklLi71X+a9evWi0aNHq9YnT55Me/fupXv37tH58+fps88+E13BBwwYQJZkUkdvalfF8nqMsTdvk2nzhcc0cFUgBT54keGxJtLqCgAAlhLcLFq0SLSdNWnSRNS8KJd169apjgkODhZj2Si9fPmSBg4cKPJs2rVrJ9rgTpw4QZUqVSJLM69bdbJ0lx+l9LADAAAwia7g+vyr+tChlLmKlObOnSsWILKzMYmUKUn9fvQetatSlNyctfci45+YlZXRiwUAABLC3RHMWmjUG2ow44AY1Xjm7hv0+Z+nKTFZc7ZyAACwLCY1iB9AdiQmK2jomvOq9aO3j6meI+MGAMDyoOYGAAAAZAXBDcgaeksBAFgeBDcAAAAgKwhuZKSYWo+hQ981kbQspgL1NgAAlgcJxWZuxzcNaN3ZEBrWvBy55LGjvivOkk0uKypV0FHjuML57OnpK8ubgJKvuVj+3FIXAwAALHHiTFOceMvcKSfYzGVFdM+/Pe2/Fk4DVgWSJaleMj9t/rJ+uu1Xn0RRwPUI+qhmCSqO4AcAwOSZ3cSZkLOaVXAVjy0qpTym9b82FUiuLgRH0oj1FzVqrUZvukLt5x+jOftu0cCVlhXsAQBYAgQ3FoBrbpRW96+rej6gQWl6ML09DWlSli5NaEV+ZQuSHG06/5hqT91P10OjadrO6/TPmWDVvmuh0arniUnJtO3SEwqPfiNRSQEAwBCQc2MBfNWClgblClHguBZ09PZTauudOpO6c25bMdu4kndxJ9o42E+M/Hv1STQtO36fzF3bX45muH/58Qc0ded1ymdvQ1cmtTZauQAAwLBQcyNjR75vSrM+rkqf1yulsb1QXnvqXL0EOdhaa2zngEZpdNuKYj/npPRvWJrkLOTFazpz/wXtuRom1l/FJ9K1J9HU7OdDtP3yE6mLBwAAWYSEYlCJT0yi+QG3ybNIXhH8qHv15i19uvSUqMWxNNx0F/3mLf1zOpg+8CmGBGQAABO/fyO4Ab3dDn9FLeceIUtzanRzqucfIJ4XyWdPZ8a2kLpIAAAWJxq9pSAnlHPNJ/JRtOlb30M0g8lR1yUnVc8jLHCsIAAAc4PgBrJk74hG5FHQkaZ29qahTcuKbeM/qEQTOlSmkmkGDpSL4BevpS4CAABkAZqlINv4p8M1Ga5OqdM+3Il4Rd/8c1Gji7XcLOtTSzV2EAAAGAeapcAorKysNAIb5lkkH83tVo3krN8KDPwHAGDKENyAwXm55aMzY5rTX/3rSF0UAACwQAhuIEcUcXKghuUKa4ydIyfJyQqNkY0BAMB0ILiBHPX3gHr0Z+9a6WYpN3efLztNSckKMTmp59hdFPQ4SuoiAQDAO0goBqPg+Zp43qafdlwX6x9ULUrbL4eK51819aTudUvSybvPxYjASz+vRbbWVlR69E4yF3VLu9C6L3ylLgYAgGxl5f6NuaXAKDjxeEDDMqrg5pvm5WhgwzL0JDKO2lZJmePq45olxKJUuZiT2YyIHBOfKHURAADgHTRLgWR83POrAhtt/vuqAf3yqXn0vOIgjOeoYhZWGQoAYHIQ3IAkXPNpdiHXxjqXFXWsVpxuTGlDI1uWJ1PHs4rfDHtFtacG0F+nHkpdHAAAi4WcGzCqB89iKT4xWXQXzwqeuLLB9AMU/cZ0m3/8yhYUzVOXH0Wp1ud0rSaCtML57MXko3Y2ucjeRnM2dgAAyBwmzswAghvzxYGD94Q9ZI543J860wKogKMtXRjfSuriAACYHYxQDGBiOLBhL1+/lbooAACyh+AGzIaDjTx+rv+cCZa6CAAAsoZmKTC78XJ48Lzctta0PjBE5LKMWH+JzM2D6e2lLgIAgFnJ8WapkJAQevTokWr9zJkzNHz4cFq6dGl2Xg4gS+PlFMufmwrksaMvGpelLjVSx8WRo3tPYygqDk1ZAABZka3gpkePHnTw4EHxPCwsjFq2bCkCnLFjx9LkyZOz85IAFmnzhUc0Y/cNjbmqlO5ExFCz2YepxpR9kpQNAMCigpugoCCqUydlxuf169eTt7c3nThxgv7++29asWKFocsIkCEe6C+XFZmVDr8eo34rztK36y7RokN3qf6MA+mOOXnvuXjkZriMoGYHAMAAwc3bt2/J3t5ePN+/fz99+OGH4nmFChUoNDRlviAAY+GB/m791JbMyZXHUXTgRoRqPTTqjRgHJ6v+u/SEfCbtpZ/33DRwCQEALCy4qVy5Mi1evJiOHj1K+/btozZt2ojtT548oYIFCxq6jACZsrE2/55UVSbupa0XH6vW9amMGr81SDwuOHgnB0sGAGBesnVHmDFjBi1ZsoSaNGlC3bt3Jx8fH7H9v//+UzVX6cPf359q165N+fLloyJFilCnTp3o5s3M/wW6YcMGUUvk4OBAVapUoZ07zWf2aDCOTtWK0ef1SpG5Gbb2osjB+fqfC2RR3RgBAAwoW7OCc1Dz7Nkz0S2rQIECqu2DBg0iR0dHvV/n8OHDNHToUBHgJCYm0pgxY6hVq1Z07do1ypMnj9ZzOLeHAyoOjD744ANas2aNCIrOnz8vcn/AcrWu7Ep7robTgZGNqUzhvGJbede89OPWq2ROOAeHbbv0ROqiAABYzjg3cXFxYuZjZSDz8OFD2rx5M1WsWJFat26d7cI8ffpU1OBw0NOoUSOtx3Tr1o1iY2Np+/btqm316tWjatWqiaayzGCcG/ni32Tc2yRytEsfs3uM2kHmTNe4ONUm76XId6MeY+wcAJCz6Jwe56Zjx460atUq8TwyMpLq1q1Ls2fPFjUoixYtyl6puddHVMqEgy4uLjqPOXnyJLVo0UJjGwdUvF2b+Ph48YGoLyBPVlZWWgMbFjSpNU3t7C2bIC7kxWvxCAAABgpuuAmoYcOG4vnGjRvJ1dVV1N5wwDN//vzsvCQlJyeLgQDr16+fYfMSj6vD76eO13m7Ntx8xZGecnF3d89W+cC85bW3oZ51zS8HR2nLhce0/mwIvXmbRN9vvEwNZx6kmXtuEuIbAAADBTevX78WScBs79691KVLF8qVK5doHuIgJzs494bHz1m7di0Z0ujRo0WNkHLh0ZXBcvHs3Eo/flCJzMXwdRfph38vi2TjjeceaeTmAACAAYIbT09P2rJliwgU9uzZI5KAWURERLbyWL766iuRQ8OjHpcokfFw+m5ubhQeHq6xjdd5uzY8Hg+XSX0By1XEyYHOjm1Biz+rSb19za8mZ+cVzXGkMIAfAICBgpvx48fTd999Rx4eHqLrt6+vr6oWp3r16nq/DucMcGDDycgHDhyg0qVLZ3oOv1dAQIDGNh5rR1kGgMzwZJttvN3Mcmyc8Oh4qYsAAGDysvXX/eOPP6bg4GAKDAwUNTdKzZs3p7lz52apKWr16tWiOzc3c3HeDC/cG0upV69eomlJadiwYbR7926RwHzjxg2aOHGiKAcHSQBZ1b5KUY31AQ0yD7DNQUJiMgU9jkLSMQBYpGx1BVennB08s+YkrW9upX0M1uXLl1OfPn1UY+pwDZH6nFU8iN+4cePowYMHVK5cOZo5cya1a9dOr/dEV3BQF5eQRKfuP6eKbk5ifiqu1Sk92jwHhVTvCj5g5Vnafz2CxrarSAMbldF6PP+vn5isIFszrMECAMsTnYX7d7aCG+7Z9NNPP4nak5iYGLGNa15GjhwpZgbn5GJTheAGMjNv/y2at/82mZu709qRdS4rSkxKJs+xu8Q2NycHOqWWRK2OA6Djd57TydHNKL+jnZFLCwBgYuPccACzYMECmj59Ol24cEEs06ZNo19//ZV+/PHH7LwkgMmw0mtWJ9PTfv5R8dh09iHVtrDoNxSfmKT1eK7Z4UEPx2y+QnefpvwjBQBADrIV3KxcuZL++OMPGjJkCFWtWlUsX375Jf3+++8azUcA5ki9tbSPnweZixthr+hJZByFvEjNWWP+O29keN7OK2HUfPbhHC4dAICJBzcvXrwQE1emxdt4H4A5U6+3mfhhZTInQ9ecT7dtfSDGdgIAy5Kt4IZnAedmqbR4G9fiAJiztHnufmULkrm4EByZbtvrhCT64NejYsqGjCjT79DDCgAsclZw7p3Uvn172r9/v2p8GZ7biQf127nTPHuaAKgP9Keuay13OnH3uXjespKrWH7YeJnMSdDjaBq16TL9PaCezmN4OodHL+Po/rMY2vJlfbMcBwgAINvBTePGjenWrVu0cOFCMdYM4ykYBg0aJHpRKeedAjBHXaoXF2PE1CuTUmPTsVox8iySVywOttZim7kFN+x5TAIdufWUAq6HU5KW2hn16RwCH76k5GQFXQuNpv4NSusctgEAQJbj3Ki7dOkS1ahRg5KStPfOMAXoCg6GUGb0DkqWcevN2kH16NOlp8Tzlf3qUOPyhaUuEgBYuOic7goOAJbj0cuMc3UAAEwNghuAbJjTtRrJmZUe4/6ERb2hn/fcpNAoza7nAABSQ3ADkA2dqhdPt23GR1Xok5pZn4bEXPVZfoYWHLxDfZadlbooAADZTyjmpOGMREam74YKYCm61S5J7aoUpZIujjR73y0yZ/okEPOggexmeMojAIBZBjecyJPZfp7FG8BS5XOwpa+bl6MN5x5RcCbjypiyq0+iVM8VJOPMaQCQpSwFNzxbNwCk91f/Ohrr5t5zetK2a6rnh24+pZ51S2V4PHcbz8XTqgMAmADk3ABkU6G89uLRJpcVNSwn367SZ+5nPqXKlouP9XqtmPhESkhMNkCpAAB0Q3ADkE1/D6hLzSoUoS1D66fbJ6c6jKi4t7TvWniGx4xYfynT14l+85a8J+whv+kHDFg6AAADjVAMAERebvloWZ/aWvdVKZGfHjw335ybtAauChQzpBdwtKNi+R2oS430vcK2XXpCDTwLUYE8dlpf43JISh7Ps5j4HC8vAFg2BDcAOWBKx8riZi8nK048UD2fsTtl2hV1X/9zgSq45aPdwxuJyTfPB0eK9Tz2KX9mkJgMAMaCZimAHJDf0Y6cc9uq1i/82JKOj2pGdTxcSA6exSRk2D183dkQ+mjRCfpk8UnVPkw2DgDGguAGIIeoT9vGTTXF8+emzjXSD/4nR/+efyQeeeJNJUPENhvPPaJ60wI0uqoDAKSF4AbAiLrVcidLpe8cvfeexlCnhce1JjF/t+EShUW/oeFrL+ZACQFALhDcAOSQES3LpwtoLGEsmM//PE2x8Umq9WFrL4iEZPXY5nkGScXfrr9EF0MixTm6JMp5SnYAeG9IKAbIIb39PKiJVxExHYMu1dzzU3TcW7r3LJbk4ujtZxrrWy+mJFZXKZ46wnnNn/bTzm8a0pjNV8jeJhfN+tiHShZM+ZyiXmvP5wEA0BdqbgBycH4mj0J5Mqyt4TFy+jYoTZYg6LFmnky7+UdFDc3p+y/oi9XnVNvl1IUeAKSB4AbAyFb105yqoXttd/qiURkxjoyc7c1gIEDOs/nz2H26FILJdwHg/aFZCsDIGpYrRBM7VKIKRZ3Euo11LhrdrmK6sWQsSXxiMk3ZnjqflZLHqB3UspIr/d6rll5TO1x7Ek21ShWwiNwmANANNTcAEjRX9alfmuqVKZhuX6Pyhck6lxV907ycJGUzRZlN/aD08aIT1HXJSVp7NiTHywQApg3BDYAJWdm3Nl2b3Jq+buYpdVFMyqJDd3Xu4+kcDtwIVw0guOWCfpN4AoB8IbgBMLFaHXsba7K1zkW3p7aVujgmg6d7uK+jR1mruUeo34rUbuNnHrygW+EpgQ4AWCYENwAmigMcSNX050Nat7+ITdAa8GSEa3vO3H+hdd+5hy9p+NoLFBH9JpslBQCpIaEYAMxSfGIS/RpwJ9PjQqPiaO2ZEOpZryQVyecgtvn5H6CEpGRa2a+OmAPsxy1BNK59RXJ1chBzYrGb4TG085sGojYNAMwLghsAE9aqkqvoQu3jnp+Wfl6TCue1pzJjdpKl4yaqTgtP0HW1uat0+eyP03T3aSwdu/OM/h3iJ7ZxYMOO3HpKf59+SG/eJlO3pac0zuPX9t91g8a868kGAOYD9d4AJuznrj40tbM3Le9TW9QqoItzqswCm/bzj1LIi9cisFE2N/GggeoO3IgQgY0uS4/cE13Mf9p+jY7efqp32ebsu0W9lp2hxHdBVGY1UJO2XRWBFgAYBoIbABPm5GBLPeuWIpc8dqptFdzySVomc3H1STSN3xqksY0n5By96bJqXVeSsrqJ/12lP47dp8//PENRr9/q9d7zA26LYGX/9cy7sa868ZCWH38ggiEAMAwENwBmRjmgXbsqblIXxeQdvJm+NuSfM1kbB+fQzQjV8ydRcVkenDAzIS+lm26CR4beevGx3jO2A5gLSYObI0eOUIcOHahYsWIiaW/Lli0ZHn/o0CFxXNolLCzMaGUGkJq7iyPdmNKGFvaoIXVRLERqU6DcYoBmsw/TsLUXaVcQ/oaCvEga3MTGxpKPjw8tXLgwS+fdvHmTQkNDVUuRIkVyrIwApsjB1loE9t1quVNTr8JSF0fWuNu4koK0RzfH7zyjFnMOU+AD7d3LM2IKWVRpc5EAzJ2kvaXatm0rlqziYCZ//vw5UiYAczLj46ricfbem/Trgcy7RUPO6PnHafH46dJT9N9XDbJU0yOzyiAAk2CWOTfVqlWjokWLUsuWLen48eMZHhsfH0/R0dEaC4DcjGzlJbqKQ84Kfp6aH3P2wQv6fsMleqk2iGBisoLazT9KcnUnIoZazz1C2y49kbooAPIJbjigWbx4Mf37779icXd3pyZNmtD58+d1nuPv70/Ozs6qhc8BkKNWld2oWQU00eakIX+fpzGbr9CTyDj6ZPFJ2nDuEU3WMpu5UrJCQcnJCpGwm5Rs2nU03CWdk6fjEpJ0HvPdhkt0M/wVff3PBaOWzZzwkANt5h2hE3eeSV0Ui2alMJE0ec4f2Lx5M3Xq1ClL5zVu3JhKlixJf/31l86aG16UuOaGA5yoqChycnJ673IDmBK+MfG/qk/df06bzmMCSWPgbvrapoBQqlzMifLY2dDjyDg68F1juhn2ik7fe0H9GpQWM8Bzd/VVJx+KYx9Mb2+8ghORx6gd4nFQozJiPJ81p4OpZSVXVY+8tFrNPUy3wmMkKau5KD9uFyW86yWHz8iw+P7NlRT63L/NfoTiOnXq0LFjx3Tut7e3FwuAJchtZ01da7vT3WcpNyBWtYQzXX4UJWm55CyjwEY53o7SlUdR9PHik+K5o721GMNIHf9bc31gCFUq6izWPQo5Uj4HW41jFh++SxeDI2lBj+pk827+sdj4RHK0S0kyzy4ObNi+a7rH5smFqSgypQxsQFpm1SylzcWLF0VzFQBo91tPdBk3xZ5XYzcH0dukZI2k44M3I+h//16hDguOiaXNvJT8nVdvUgcPnL7rBu2+GqYKQjZfeESVJ+yhtr/kTK6PiVTuA5hPcBMTEyOCE17Y/fv3xfPg4JR/QYwePZp69eqlOn7evHm0detWunPnDgUFBdHw4cPpwIEDNHToUMmuAcAU9fHzIFtrK/qkZgkqnj831S3tInWRgIgGr9bMD6z1035adzZ1UMGbYak1boybsniE5CoT96abniHubUpuzLfrLonHG2Gv9CrDnYhXIkBST4TWhZuqGs48KPKMslJzw3knDWceoP0Z1AIB5CRJm6UCAwOpadOmqvURI0aIx969e9OKFSvEGDbKQIclJCTQyJEj6fHjx+To6EhVq1al/fv3a7wGABAVdc5N1ya3Idt3zRbrvvAVPV24ytwptw01mHFQ6iICEUXFZT6dw4oTD8TjzD03qFH51DGNOM7QZ+6qtFrOPSJqix4+j820dmbz+Uf06GWcaLKa1rmKeE999Fl2hl7FJ9KAVYHIOwHLC264p1NGVZ4c4Kj74YcfxAIAmVMGNkqeRfKqnn/Xqjz9vPeWeO7kYEPRbxKNXj5Ib8buGzr3JSYp0tWOKGttlP4994g+qllCPOfeWb2WnaYyhfLSlE7eqmOUf3L1ycNK+9dZ35obZa0SgFTMPucGALLuQ5/iqufzu1eXtCygn7TNTqtPpdZqK43ccEljHJ7jd57TX6ce0qOXr+mBHpOEppU2lEE+MZgLBDcAFqhkQUe6NL4V3Z3Wjpp4FaGAkY2lLhLo4Zpazytd5uy9SSEvXmuMq8PNkE1+PiRyaN6Hem8snnDz2G2M5QKmCcENgIVydrQV46ywsoVTm6zAdOkz+vH8A3dEErC2ShZdCb46u5Cn2a6+xhNufvZnyrQTaaF/FUgNwQ0AgAw9UJsqQmn4upSeqWm9SpNzdTtce8+rd7EwgMlDcAMAIEPK7tv6CI9+k65H1a3wV/TjliDVtlP3ntP54KzPHn73qWb39uzg5i8uj6ngcYey01MNjAfBDQBkqGO1YmJ4fpAX9RYnbb1WW809orHOM57rS/31ui89RfP236KTd59rPY7H3Un7/jyYIY/Fw3lDvJ+bv7g8pjD679NX8WLcoZwaNBEMw+ynXwCAnPVRjRJifJXlx+/T2zTdkcF88fg1Su/zrV5+FElVS+TX2Kb+ehGv4mne/tvc2JVuzJsZu2+K6STYv0P8yN0lNxXJ50B9l58V28oVyUubL6TOkbbzSih1qp7a009fN8KiU6a+qFnivaaoYIffDaZ4O+L9a6Qg56DmBgC0WjOgLm0Y7KsaOO7k6Oa0ql8dqYsFOeDQTc3Rj7PiwwXHxfg62aEMbNhHi05QnakBYhZ19aawY2qza79RGz+HB0DcHRQmZjOfteeG6PKujoMZ5SCJPI3F9xsv056rYfS+TD3tiKf0AAQ3APAOT9XAfvm0mghi/DwLUW2P1GkbCuW1F4GO+rg46wbVk6SsYFp4fJ2gxymDAnIuiq6xWZ/HxFPn347TP2fSj9Gj9DY59eZ8QktTllKvP0/T4NXnyGvcblp48K5GfhBPVcFzczWffUjnJKbzA25T+/lHs9w93pTH+jlz/wWVG7uLfjt0hywdghsAEGZ+XJWuT25DHasV1xjmP6223m7kkseOShfKQ3XLFDRqGcF0ffDrMQq4Hi6mXNCFA4oLwZE0etMVajHnsNZjrNTqRtRralhCUrJqMMJLGYyw3GvZGfH4LEZz/qxkhUIEYZzLM2ffLRHseE/YQxeCX8oiuFEmkc/cfZMsHYIbABA4FyG3nbVe0zqcHtOc9o9IP/DfxA6VNNaX962tETyBvPVfGZhhE9fWS09Uz3muM20mbbuqev48zeSe47deFYMRcnOVvn4R+T4puIaHgzCusVHX+bcTer+eevD1vrgH2NjNVygsSrO3Grw/JBQDwHvNW/XvEF86dvs5dalRnNxdHGnitmuqfU29itCBkY1Fgun10MxH1wV5i3yd+UShf5/W3WSltPl8apKxOm4Ss0kzp9rc/SlzqKnTdwZ1xj251JOQM6q5yWiuRG0+mH9M1EbdDo+h9YN9Vdt5hOmizg7priUzVlkIqnLbWov/X+UKNTcA8F5qlnKhYS3Kqf5Qco8Xxn88WZnCecUIyD5petQAZNeVd/k9aXHSsCFxd/TGsw5RlI6gLC4hpdnsRWwCzdx9QzS3ZQUHNizoSer17LsWLkaY5lqwrNJnYtMXsQmiWz2/h5whuAEAg1rZtw596FOMtgytr7Fdzv9KBOO6pqMWkLuNp83TyYpVJx+IObM4J4drYbg3V/CL1xoJuuoBxJIjKb29fth4iX47dJfWng3ReL3YNMnK3I2cx/xR7xGWFg+5oDw2q/TJBwp+kX7kajlCcAMABsU1NdyjysstX7p9PM4IQE6q8OPubJ234vh9kdPDc2aVHbOTSo/eqdq35Mg9rQHEwRsR4jHwofaE5MoT9ogu6RwocU+m3svOiDF/dgVpdkl/nZBEP+9JSQJWb9niIMh/13XR5f3+s1jRlT2rTV+WCsENABjNrI+rit5WaY1pV0GS8gAoqeeKacNj+XDz1KoTqePpcI+tn7ZfyzCXiHN+/j3/mLouOanaNnTNeY1Z29mCg3fEGDVxajVPHAQtOXxPdHlv+vMh+uKvcxnW6LzvAIW6vE5IpEM3I8SYQuYCCcUAYDT8x9fVyUG13sSrsOhSrp6gDGCqY/lo88exlGYkXQ7ciFDl5qg7cjt9kMLNWpxMrBSWZs4vdikkipp4FdH6XvqENgq1mp8p26/Rjx+k9nB8FhNP/jtvUI+67iKXTumbfy7Q/usR9Hm9UjSlkzeZA/xFAQDJrOhbhyZ0qGzyo74CvI+TWrqun1AbeVmJByJM2/09rYshL8VozMq8HZ5vi+e7YlmtuPkzTWDG7//v+Uf00aLUWibGgQ1LOwo0C37+mupNC6AGMw6oxiAyBQhuAMCovmxaltycHOjrZp56HZ/f0TbHywRgbL8fzbjGh93WMhP6wZtPRRBSZsxOMVZQq7mHqfbU/bQhMERjBGZ1Ea/e0L13s7OnzdhRT27mvJ7MXArRnBl+7JYrooaJ5yrjUaFNBZqlAMCoeGLEk6Ob6cwPKFEgt/hDufObhpTH3pqK5c8thpRXn6V868XUweAA5Cpt76u01Ed5zqgbfJ2pAeKRm4DTBjAcJPHgm33ql9Zr/J+OC4/T1M7e1KhcYRHU8IjTSq/eJNIfR+/RgIZlSGoIbgDA6DJKfDwwsonoQlsgj53W/Sk5OlZihvKyhfPQ3aemUxUOYCr+OHqPtl16Qt80L5dpzQwnU3Nwkxb30vppR/pE67GbU+fxSuunHdcR3AAAMC83J9VzO5tcZGejPbBhff1KU7fa7vTXyYf0Wb1S5Df9gJFKCWA+ftpxXTzqOxjg2QcvNNZ5Wgh9Ros2Vci5AQDJ+ZYtKGYj3/51gwyPW9CjOjk72lJR59z0Q5sKosmqRknNkY8HNZL+X40A5uaTxZpJxOYc2DDU3ACASeDZyHXZMNiXLgZHUvsqRdPt4wDn/Lt2/yPfN6WSBR1paFNPinydIIbOBwDjevXmLeVzkLYjAGpuAMDk1fZwoYGNymjN1WlR0VU85rGzFoENc85tS6UK5jF6OQGAqMrEvVIXATU3AGDeuPcUN1VVLpaat5ORNpXdaPdVzeHvAUBeUHMDAGaNa3OaehURXcz1MaljZWpZKaW2R6mau+4Zy0e1xdQQAOYGwQ0AWBSe/uH3XrWoTOHUZqtvmuseUHBw47K0ql8dI5UOAAwBwQ0AyJZ1rpQcneL5c6fb52hnrXrONT/57FNb6ed09aHG5QuLIIg1Kl+Y2ldNn8wMAKYJwQ0AyNbF8S3pxKhmVNQ5fZPV3K7VxCCA3AWdm7b6NkgdxKxLjRK0sl8djearaZ2qZLscQ5qUzfa5AOZod5C0eW0IbgBAtrg7KncV1zYgcjnXfBQwsomqC7q9TcZ/DjlpuUuN1O7qc7v50N1p7ahzdd1d2HnWc05g/gC1PmBhBq8+J+n7o7cUAFhM81RGevmWor1Xw6iNd9EMu6RvOv9YPO9cvYR4nNutGnWt5U4KUogJQZvNPqwx67k2XFs0bO3FbFwJAOgDNTcAIHs/daoiAo8pHStnWMuz9asGGTYhcRDz8yc+dPC7JulGWPYrW4jKFM5LH9dMCXrSWt6ntsbx+tr3bSO9jwWAFKi5AQDZ8yySN8OZyLNSA6QreFEqks9e63b17uZWpFmOHz+oRN7FnKjb0lNifd2genQ7IkY0lXHzGQCYUc3NkSNHqEOHDlSsWDHxR2fLli2ZnnPo0CGqUaMG2dvbk6enJ61YscIoZQUA8/a+gY2+vmzqSR18itHiz2pqbM+j1hsrn0Pq8wKOttS/QWmNEZUrFnMSk4J+UstdrB/+vgmt6Jta86MNvycAmEBwExsbSz4+PrRw4UK9jr9//z61b9+emjZtShcvXqThw4fTgAEDaM+ePTleVgAAfeS1t6Ffu1enNt5uGtt5tvOjPzQVi4OtNc3rVo1c8tipupvntk3tmm5nrfmnmQOfJl5FMnzfwnm11xgBWCJJm6Xatm0rFn0tXryYSpcuTbNnzxbrFStWpGPHjtHcuXOpdevWOVhSAID35+6SMvcV61S9uJg6QlmjxL2xZn/iQzbWViL4yariBdKP5cN51MmK9yw0gBkyq4TikydPUosWLTS2cVDD2wEAzL2p7KOaJTKcHV2bZX1qUd/6HqK3V7da7vRhmuaprUPrG6SsAObErIKbsLAwcnXVnBOG16OjoykuLk7rOfHx8WK/+gIAIBfNKrjShA6VydY6F834uCrN715dY7+PlnmzmnoVpqmdvclGjy7yuiz9XDOnCMCUmFVwkx3+/v7k7OysWtzdUxL0AADkqmAeO/FYo2QBrUEJ5/n0rFuKbkxpQ1cntaYedUtm+T1aVXZL937qzo3TrGUHMCazCm7c3NwoPDxcYxuvOzk5Ue7c6dub2ejRoykqKkq1hISEGKm0AAA5o7xr3gz3/zvEjwY0KE0Le9YQ67U9UoKcse0qiqDE5l3CMj9yL65pnVOnlsjvaKvxWnnU5uDSJVmhoM/qaQZIBZHgDBIyq3FufH19aefOnRrb9u3bJ7brwl3GeQEAMHebvvSjY7ef0aBGZejv08FUt7SL1uM8CuWhcR9UUq3zPFlXn0RTTS01OWlxQxU3VyW+y0QOmtSaDt96Sn2Wn9V5juLdQIllC+elSduu0YyPUoKl5X1r0/cbLtPz2HhSILEZLCW4iYmJoTt37mh09eYu3i4uLlSyZElR6/L48WNatWqV2D948GBasGAB/fDDD9SvXz86cOAArV+/nnbs2CHhVQAAGAc3MymbmnhsHH052tmIqSP0wTGIU25behGboEp6Vu+G3rBcIZr/qWZejzJw6Vu/NHWr7S7eTznb+tmxzSkhKZm8xu1WHd/Hz4MalS9E+65F0D9ngsnQqpZwpsuPogz+umA+JG2WCgwMpOrVq4uFjRgxQjwfP368WA8NDaXg4NQfPncD50CGa2t4fBzuEv7HH3+gGzgAgIFYW1mJAQMruOUTNT5K/l2qiFGWeS6tAmlybLhZSkkZ2ChxcGRvY02t3s2wzjOtT/ywskiE5tc0tG+al6Pevh4Gf10wL1YKhWVVFnJvKU4s5vwbztUBAACirRcf05Tt12jJ5zWpZin9ankGrgqkfdfCRUAxomX5DI+NiU+kgzciqGmFImKgQyWPUZnXvHev407/nEnJl/Qo6EgPnr/Wely/+qVpfIdK9O+5RzRyw6UMX5NHiX71JjHT94bsezC9PUl1/zarhGIAAMgZPL7O2bEt9A5s2IIe1UXy8rDm5TI9lgManiJCPbBh6kP9cOCijX+XqmI6i3plXOifQfVEV3ZtlK9la5N6a2tUPvXYnzp5q55/ULWoyCfi0aRBfhDcAABAtubf4uammqUKiAlFs8u9QGpAc+j7phr7OIj5vrWXeM7TWawd5EtFnXPTgh4pvcDSUpaidWVX0YQ2sGFpGtbcU7Wf5+tScnKwVQVcID8IbgAAQDLKMXbqaOn5tbxvHRraNDU40TYJqTploMJB15ah9Wls+0oiAbu3byma0CGl99isj6tSfc+CYoJT9QRn7iHGicjamtdyepTntLVZ8P7wiQIAgGQGNiwjAhDv4ik5FH8PqEsj1l+k6V2q6nU+1xp92aSs6KVVQq0WSL02alLH1OYonmldOdu6Eic486IUHv1GdLVXcs6tOfZPVuiT23NqTHPxns1nHya5GJFJDlZOQ80NAABIhoMTrrVR9rKq71mITo9pIRKPM8K9udiSz2rSyFZeWgObnMDByr1p7ah6ydRpLbh32YlRzbQeH6g2UnPn6sV11tzwGEGdqmk2kXHzmjpuArw0oRWZgx7ZGPXakBDcAACA2eFmp73fNqIW77qY5yT1VKRtXzWgXLmsaF63aiLY4OkseBygYvlTR8nnQQyX96lNq/rVEU1kiz+rIfJ/fv7EJ8P3mfmxj2hC8y1TkKoUd6Zx7VMHYlzYowat7l9X1CLxWEOZGduuol4J4Rrv/5FmbVmXGlmbxFXp2uTWVEjiEarRLAUAAGbHwdaayrum1N4YGgcWSmUK5yE3ZwdVLVNJl5QaolIF84ieYrqo1zy18S4qFnZxfEt6m6SgQnntaM/VMKpYNLVLs51NLo0mtNcJqc1Zdcu4UO53U2HM+Kgqff7nabr7NFbjPWuVKkCBD1+K55/UKkGBD1/QnqspUxbxrPFcw+UzaW9K+bwKa+Q5ceJ219ruYnTrxYfv0i+fVhM1SpvOP1Yds2ZAXSpTOC/V8w/I8PNLO9aRFKQvAQAAgAnhnJxRm66I59zrimtfeIJRDm641uZ95HdMHQBRGfBkFCSMaVfhXTCUWhPCtUQBI5tQ9Ju3VHViSrDC1n/hS3eexlCZQnnEvGFLPq9F68+G0PrAEBreorzGLPA/8XxiivS1NBzwaEvuZn6eKTVG/DLvZuegDYN96ZPFJ8nUoFkKAABADQcx+0c0FoMTTuhQWdVDi2uL9JHXPvsJyGkNalRWa48xZXd2TsBmXBPEgRfXZtm8mxiVcW3MxiF+5JLHTj2WUdUUKeVzyF6ZeVqP+/7tyOtdLVpb79TZ4qWEmhsAAIA0PIvkzXKPnykdK9O5hy/FmDzGwgnY279uQCV1DICoi9W7WqTvWpUXQZGu7ui21lai5igj3CNt29cN6Mz9F1Tr3Qz0UkPNDQAAgAF87utB8z6t/l6DGmaHd3FnUYuTGQe1mpoC75rHvmpWjr5sor1mSNnU5eOenzYO9lVt++XdxKk/qs08z7VADcoV0rt2K6dhbikAAAALEfz8NSUpFFS6UJ73ep24hCRVgrMp3r/RLAUAAGAhSmax+UoXYwc2WYVmKQAAAJAVBDcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwAAAJAVBDcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwAAAJAVBDcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALJiEsHNwoULycPDgxwcHKhu3bp05swZnceuWLGCrKysNBY+DwAAAMAkgpt169bRiBEjaMKECXT+/Hny8fGh1q1bU0REhM5znJycKDQ0VLU8fPjQqGUGAAAA0yV5cDNnzhwaOHAg9e3blypVqkSLFy8mR0dHWrZsmc5zuLbGzc1Ntbi6uhq1zAAAAGC6JA1uEhIS6Ny5c9SiRYvUAuXKJdZPnjyp87yYmBgqVaoUubu7U8eOHenq1as6j42Pj6fo6GiNBQAAAORL0uDm2bNnlJSUlK7mhdfDwsK0nuPl5SVqdbZu3UqrV6+m5ORk8vPzo0ePHmk93t/fn5ydnVULB0QAAAAgX5I3S2WVr68v9erVi6pVq0aNGzemTZs2UeHChWnJkiVajx89ejRFRUWplpCQEKOXGQAAAIzHhiRUqFAhsra2pvDwcI3tvM65NPqwtbWl6tWr0507d7Tut7e3FwsAAABYBklrbuzs7KhmzZoUEBCg2sbNTLzONTT64GatK1euUNGiRXOwpAAAAGAuJK25YdwNvHfv3lSrVi2qU6cOzZs3j2JjY0XvKcZNUMWLFxe5M2zy5MlUr1498vT0pMjISJo1a5boCj5gwACJrwQAAABMgeTBTbdu3ejp06c0fvx4kUTMuTS7d+9WJRkHBweLHlRKL1++FF3H+dgCBQqImp8TJ06IbuQAAAAAVgqFQkEWhLuCc68pTi7mwQABAABAXvdvs+stBQAAAJARBDcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwAAAJAVBDcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwAAAJAVBDcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwAAAJAVBDcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwAAAJAVBDcAAAAgKyYR3CxcuJA8PDzIwcGB6tatS2fOnMnw+A0bNlCFChXE8VWqVKGdO3carawAAABg2iQPbtatW0cjRoygCRMm0Pnz58nHx4dat25NERERWo8/ceIEde/enfr3708XLlygTp06iSUoKMjoZQcAAADTY6VQKBRSFoBramrXrk0LFiwQ68nJyeTu7k5ff/01jRo1Kt3x3bp1o9jYWNq+fbtqW7169ahatWq0ePHiTN8vOjqanJ2dKSoqipycnAx8NQAAAJATsnL/lrTmJiEhgc6dO0ctWrRILVCuXGL95MmTWs/h7erHM67p0XU8AAAAWBYbKd/82bNnlJSURK6urhrbef3GjRtazwkLC9N6PG/XJj4+XizqkR8AAADIl+Q5NznN399fVGMpF27yAgAAAPmSNLgpVKgQWVtbU3h4uMZ2Xndzc9N6Dm/PyvGjR48W7XPKJSQkxIBXAAAAAKZG0uDGzs6OatasSQEBAaptnFDM676+vlrP4e3qx7N9+/bpPN7e3l4kHqkvAAAAIF+S5tww7gbeu3dvqlWrFtWpU4fmzZsnekP17dtX7O/VqxcVL15cNC+xYcOGUePGjWn27NnUvn17Wrt2LQUGBtLSpUslvhIAAAAwBZIHN9y1++nTpzR+/HiRFMxdunfv3q1KGg4ODhY9qJT8/PxozZo1NG7cOBozZgyVK1eOtmzZQt7e3hJeBQAAAJgKyce5MTbOu8mfP7/IvUETFQAAgHng3s7cKSgyMlJ0EDLpmhtje/XqlXhErykAAADzvI9nFtxYXM0NJyw/efKE8uXLR1ZWVjkSVVparZClXrclXzuu27Ku25KvHdcdYjLXzeEKBzbFihXTSFfRxuJqbvgDKVGiRI6+h6X2yrLU67bka8d1Wx5LvXZct2nIrMbGYgbxAwAAAMuC4AYAAABkBcGNAfGAgRMmTBCPlsRSr9uSrx3XbVnXbcnXjuu2J3NkcQnFAAAAIG+ouQEAAABZQXADAAAAsoLgBgAAAGQFwQ0AAADICoIbA1m4cCF5eHiQg4MD1a1bl86cOUPmZOLEiWLEZvWlQoUKqv1v3ryhoUOHUsGCBSlv3rz00UcfUXh4uMZr8CSnPFO7o6MjFSlShL7//ntKTEzUOObQoUNUo0YNkYHv6elJK1asIGM6cuQIdejQQYxwydfIk66q4/x6nsS1aNGilDt3bmrRogXdvn1b45gXL15Qz549xcBWPE9Z//79KSYmRuOYy5cvU8OGDcXvgUf5nDlzZrqybNiwQXzGfEyVKlVo586dkl13nz590n3/bdq0Mfvr9vf3p9q1a4sRyfk32alTJ7p586bGMcb8bRvz74Q+196kSZN03/vgwYPN+toXLVpEVatWVQ0+5+vrS7t27ZL9953ZdTeR4XedIe4tBe9n7dq1Cjs7O8WyZcsUV69eVQwcOFCRP39+RXh4uMJcTJgwQVG5cmVFaGioann69Klq/+DBgxXu7u6KgIAARWBgoKJevXoKPz8/1f7ExESFt7e3okWLFooLFy4odu7cqShUqJBi9OjRqmPu3buncHR0VIwYMUJx7do1xa+//qqwtrZW7N6922jXyeUaO3asYtOmTdxLULF582aN/dOnT1c4OzsrtmzZorh06ZLiww8/VJQuXVoRFxenOqZNmzYKHx8fxalTpxRHjx5VeHp6Krp3767aHxUVpXB1dVX07NlTERQUpPjnn38UuXPnVixZskR1zPHjx8W1z5w5U3wW48aNU9ja2iquXLkiyXX37t1bXJf69//ixQuNY8zxulu3bq1Yvny5KM/FixcV7dq1U5QsWVIRExNj9N+2sf9O6HPtjRs3FuVQ/975ezTna//vv/8UO3bsUNy6dUtx8+ZNxZgxY8RvjD8HOX/fmV13Yxl+1xlBcGMAderUUQwdOlS1npSUpChWrJjC399fYU7BDd+4tImMjBT/k2zYsEG17fr16+ImefLkSbHO/yPkypVLERYWpjpm0aJFCicnJ0V8fLxY/+GHH0QApa5bt27ij7AU0t7kk5OTFW5ubopZs2ZpXLu9vb24UTP+H5rPO3v2rOqYXbt2KaysrBSPHz8W67/99puiQIECqutm//vf/xReXl6q9a5duyrat2+vUZ66desqvvjiC0VO0xXcdOzYUec5crhuFhERIa7j8OHDRv9tS/13Iu21K294w4YN03mOXK6df5d//PGHRX3f6tdtSd+1Epql3lNCQgKdO3dONF+oz1/F6ydPniRzws0v3GxRpkwZ0fzAVZSMr+/t27ca18jNCiVLllRdIz9yE4Orq6vqmNatW4vJ165evao6Rv01lMeYyud0//59CgsL0ygjz2PC1arq18lNMrVq1VIdw8fzd3769GnVMY0aNSI7OzuN6+QmgZcvX5rsZ8HVzVwV7eXlRUOGDKHnz5+r9snluqOiosSji4uLUX/bpvB3Iu21K/39999UqFAh8vb2ptGjR9Pr169V+8z92pOSkmjt2rUUGxsrmmks5ftOSnPdlvBdk6VPnGloz549Ez8k9R8E4/UbN26QueAbOLed8o0tNDSUJk2aJHIngoKCxA2fb1h8c0t7jbyP8aO2z0C5L6Nj+H+euLg4keMiJWU5tZVR/Ro4AFBnY2Mjbhjqx5QuXTrdayj3FShQQOdnoXwNY+P8mi5duohy3717l8aMGUNt27YVf5Csra1lcd3Jyck0fPhwql+/vvjjriyXMX7bHNxJ+XdC27WzHj16UKlSpcQ/ajhf6n//+58IRjdt2pThdSn3meq1X7lyRdzUOb+G82o2b95MlSpVoosXL8r6+9Z13XL+rnVBcAMC38iUOCmNgx3+H2H9+vWSBx2Q8z799FPVc/7XG/8GypYtK2pzmjdvTnLASaQcrB87dowsja5rHzRokMb3zon0/H1zgMvfv7nif6RxIMO1VRs3bqTevXvT4cOHSe50XXelSpVk+13rgmap98RVfPwv27TZ9rzu5uZG5or/ZVO+fHm6c+eOuA6uboyMjNR5jfyo7TNQ7svoGM7sN4UASlnOjL5LfoyIiNDYz70JuCeRIT4LU/nNcNMk/7b5+5fDdX/11Ve0fft2OnjwIJUoUUK13Vi/bSn/Tui6dm34HzVM/Xs3x2vn2hnuyVOzZk3Ra8zHx4d++eUX2X/fuq5bzt+1LghuDPBj4h9SQECARhUwr6u3dZob7uLLET1H93x9tra2GtfI1Zmck6O8Rn7kKlH1G+C+ffvEj15ZLcrHqL+G8hhT+Zy4SYX/B1QvI1e3ck6J+nXyH0ZuV1Y6cOCA+M6Vfyz4GO56zW376tfJ/6riphlz+CwePXokcm74+zfn6+b8ab65c/U8lzdts5mxfttS/J3I7Nq14X/1M/Xv3RyvPS1+v/j4eFl/3xldtyV91ypGTV+WKe76xj1qVqxYIXqVDBo0SHR9U886N3UjR45UHDp0SHH//n3RXZe7A3I3QO5hoew+yd1IDxw4ILpP+vr6iiVtN8JWrVqJbqfcNbBw4cJauxF+//33oofCwoULjd4V/NWrV6KbIy/8858zZ454/vDhQ1VXcP7utm7dqrh8+bLoQaStK3j16tUVp0+fVhw7dkxRrlw5jS7R3CODu0R//vnnohsm/z74utN2ibaxsVH8/PPP4rPg3mo52SU6o+vmfd99953oLcLf//79+xU1atQQ1/XmzRuzvu4hQ4aIrv3821bvAvv69WvVMcb6bRv770Rm137nzh3F5MmTxTXz986/+TJlyigaNWpk1tc+atQo0SOMr4n/H+Z17tW3d+9eWX/fGV33HZl+1xlBcGMg3N+f/4fh/v3cFY7HAjEn3J2vaNGiovzFixcX6/w/hBLf3L/88kvRtZB/3J07dxZ/KNU9ePBA0bZtWzG2CQdGHDC9fftW45iDBw8qqlWrJt6H/+ficTiMid+fb+5pF+4KrewO/uOPP4qbNP8P2rx5czFmhLrnz5+Lm3revHlFN8m+ffuKAEEdj5HToEED8Rr8eXLQlNb69esV5cuXF58Fd6/kMSqkuG6+2fEfNP5DxoFGqVKlxNgUaf8YmeN1a7tmXtR/d8b8bRvz70Rm1x4cHCxubi4uLuL74nGL+KalPvaJOV57v379xG+Y34d/0/z/sDKwkfP3ndF1B8v0u86IFf/HuHVFAAAAADkHOTcAAAAgKwhuAAAAQFYQ3AAAAICsILgBAAAAWUFwAwAAALKC4AYAAABkBcENAAAAyAqCGwCQHZ4/iOfYOXHiBJmaxYsXU4cOHaQuBoCsIbgBgEw9ffqUhgwZQiVLliR7e3sxB1fr1q3p+PHjqmOsrKxoy5YtZCoBBM+l5Ofnp/c5mzZtolatWlHBggXFtSjn3lH35s0bMcM2H5M3b1766KOP0k0SyPMUtW/fnhwdHalIkSL0/fffi0lGlfr160fnz5+no0ePvudVAoAuCG4AIFN8E79w4QKtXLmSbt26Rf/99x81adJETK5panjQ9QULFlD//v2zdF5sbCw1aNCAZsyYofOYb7/9lrZt20YbNmygw4cP05MnT6hLly6q/UlJSSKw4ZojrjXiz2vFihU0fvx41TE8uWCPHj1o/vz52bxCAMiU0Sd8AACz8vLlSzEnEU/AqAvPaaM+fxGvK23ZskVMuMlz2vAkpBMnTtSYr4aP/+2338TEnA4ODuKYDRs2qPbHx8crhg4dqnBzcxOvwXPWTJs2TWdZzp49q8iVK5ciOjpatW3lypWKPHnyKG7duqUxsaSXl5ciNjZW43yeWJDLxBOLquOJQXnuLfWy8eSBfCxPOsp27twp3lt9Xq5FixaJubj4OpR4gkOed0d9Ak8AMBzU3ABAhrj5hRducoqPj9d6zNmzZ8Xj8uXLKTQ0VLXOTS+9evWiYcOG0bVr12jJkiWiJmPq1Kka5//444+idujSpUvUs2dP+vTTT+n69etiH9dwcE3R+vXr6ebNm/T333+Th4eHzvLye5YvX57y5cun2sZlaNeunXhtbiLasWMH/fHHH+K1uPlIH+fOnaO3b99SixYtVNsqVKggmupOnjwp1vmxSpUq5OrqqjqGm++io6Pp6tWrqm21atUS5Th9+rRe7w0AWYPgBgAyZGNjIwISbmLJnz8/1a9fn8aMGUOXL19WHVO4cGHxyPs5H0e5PmnSJBo1ahT17t2bypQpQy1btqQpU6aIIEfdJ598QgMGDBBBCe/nm/+vv/6qymEpV66caDIqVaqUeOzevbvO8j58+JCKFSuWbju/Jwde33zzjWiymjhxItWsWVPvzyEsLEw0KfE1quNAhvcpj1EPbJT7lfuUOKBydnYWZQUAw0NwAwCZ4loVzi/hGpQ2bdrQoUOHqEaNGiLoyQjXxEyePFlV+8PLwIEDRZDx+vVr1XG+vr4a5/G6suamT58+IrnXy8tLBCZ79+7N8D3j4uLIwcEh3fYCBQrQn3/+SYsWLaKyZcuKoEtKuXPn1vgMAMBwENwAgF44YOCaF25C4mRZDjomTJiQ4TkxMTGi9oaDE+Vy5coVun37ttYARBsOou7fvy9qdDhw6dq1K3388cc6jy9UqBC9fPlS674jR46QtbW1CK44gTgruEaKE4UjIyM1tnNvKd6nPCZt7ynluvIYpRcvXqhquADAsBDcAEC2VKpUSSNAsLW1Fb2F0gYmnCfDY86kXXLlSv3zc+rUKY3zeL1ixYqqdScnJ+rWrRv9/vvvtG7dOvr3339FcKBN9erV6caNG6LXlDoOyLgnFPd24hqkr776KkvXy01YfI0BAQGqbXxt3GymrHniRw7eIiIiVMfs27dPlJ8/L6W7d++KbuVcVgAwPJsceE0AkBHu7s05MTw+S9WqVUWibmBgIM2cOZM6duyoOo6TfPnGzzk5PBYONwNxF+gPPvhAJN1ybQsHNNxUFRQURD/99JPqXO5azXk2nE/DSb5nzpwRTUhszpw5VLRoUREI8Pl8LNeCpM19UWratKmoMeIEXm9vb7Ht1atX9Pnnn4tmrbZt21KJEiWodu3aYjA9ZS0QB0scqHDzmzJwYfxevHCODOfqjBgxglxcXETA8vXXX4uApl69euJYHieHgxh+L/58OM9m3LhxYmwc/kzUk545B4mbxwAgBxiw5xUAyNCbN28Uo0aNUtSoUUPh7OyscHR0FF2ox40bp9GV+b///lN4enoqbGxsNLqC7969W+Hn56fInTu36BJdp04dxdKlS1X7+c/QwoULFS1bthRdvT08PBTr1q1T7edjq1WrJrpy8/nNmzdXnD9/PsMyd+3aVZRZqW/fvooqVaqIa1GaPXu2wsXFRfHo0SOxvnz5co3u7MplwoQJqnPi4uIUX375paJAgQLic+jcubMiNDRU470fPHigaNu2rbjeQoUKKUaOHKnR9Z21atVK4e/vr/d3AABZY8X/yYmgCQBAHzwa8ObNm6lTp04Ge03uycX5Qdz8w01QpoRrlJo1ayYGQ+TaIAAwPOTcAIDscPMZ59dwIrKp4WTmVatWIbAByEGouQEA2dXcAIBlQ0IxAEgK/74CAENDsxQAAADICoIbAAAAkBUENwAAACArCG4AAABAVhDcAAAAgKwguAEAAABZQXADAAAAsoLgBgAAAGQFwQ0AAACQnPwfAK4t9T3Sg44AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "# Training hyperparameters\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "# --- Plotting Initialization ---\n",
    "plot_losses = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "print(\"Training model\")\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Get a mini-batch of data\n",
    "    # Note: Corrected the order of batch_size and block_size from user's original training loop code\n",
    "    x_batch, y_batch = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Calculate loss and probabilites\n",
    "    logits = model.forward(x_batch)\n",
    "    loss_initial, probabilities = model.calc_loss(logits, y_batch)\n",
    "\n",
    "    plot_losses.append(loss_initial)\n",
    "\n",
    "    # Backward Pass\n",
    "    one_hot_array = np.eye(vocab_size)[y_batch]\n",
    "    initial_gradient = probabilities - one_hot_array\n",
    "    \n",
    "    model.backward(initial_gradient)\n",
    "\n",
    "    # Optimizer\n",
    "    model.optimizer(learning_rate)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss_initial}\")\n",
    "        \n",
    "        # --- Live Plotting Logic ---\n",
    "        display.clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.plot(plot_losses)\n",
    "        ax.set_title(\"Training Loss Over Time\")\n",
    "        ax.set_xlabel(\"Steps (x10)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if loss_initial < 4:\n",
    "            ax.set_ylim(top=4) # cut off loses higher than 4\n",
    "        display.display(fig)\n",
    "        # ---\n",
    "\n",
    "# Final clear to show the last plot cleanly\n",
    "display.clear_output(wait=True)\n",
    "ax.clear()\n",
    "ax.plot(plot_losses)\n",
    "ax.set_title(\"Final Training Loss\")\n",
    "ax.set_xlabel(\"Steps (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Model loss: {model.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422fef-b50a-447f-90db-64d249352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the model's 'creativity'\n",
    "temperature = .9\n",
    "model.temperature = temperature\n",
    "\n",
    "# Let the model generate some code!\n",
    "initial_char = \"\\n\"\n",
    "\n",
    "generation_length = 10\n",
    "charIdxs = [int(stoi[initial_char])]\n",
    "\n",
    "for i in range(generation_length):\n",
    "    charIdxs.append(model.pred(charIdxs))\n",
    "\n",
    "char_preds = [itos[charIdx] for charIdx in charIdxs]\n",
    "print(\"\".join(char_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46cddf-3b26-45fb-b67d-be92311937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all model weights into organized python dict\n",
    "weights_dict = {\n",
    "    \"embedding_matrix\": model.embedding_matrix,\n",
    "    \"position_matrix\": model.position_matrix,\n",
    "}\n",
    "\n",
    "for idx, transformer in enumerate(model.transformers):\n",
    "    weights_dict[f\"transform.{idx}.W1\"] = transformer.W1\n",
    "    weights_dict[f\"transform.{idx}.W2\"] = transformer.W2\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.gamma\"] = transformer.layer_norm1.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm1.beta\"] = transformer.layer_norm1.beta\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.gamma\"] = transformer.layer_norm2.gamma\n",
    "    weights_dict[f\"transform.{idx}.layer_norm2.beta\"] = transformer.layer_norm2.beta\n",
    "    weights_dict[f\"transform.{idx}.multi_head_attention_block.W_output\"] = transformer.multi_head_attention_block.W_output\n",
    "    \n",
    "    for index, attention_head in enumerate(transformer.multi_head_attention_block.heads):\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.key\"] = attention_head.W_key\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.query\"] = attention_head.W_query\n",
    "        weights_dict[f\"transform.{idx}.multi_head_attention_block.heads.{index}.value\"] = attention_head.W_value\n",
    "\n",
    "# Save to compressed model file \n",
    "np.savez_compressed('my_model.npz', **weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython",
   "language": "python",
   "name": "ipython_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
